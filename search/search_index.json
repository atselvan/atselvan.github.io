{
    "docs": [
        {
            "location": "/", 
            "text": "Welcome to the documentation of Private Square", 
            "title": "Welcome"
        }, 
        {
            "location": "/#welcome-to-the-documentation-of-private-square", 
            "text": "", 
            "title": "Welcome to the documentation of Private Square"
        }, 
        {
            "location": "/blog/cicd-fundamentals/", 
            "text": "CI/CD Fundamentals\n\n\nWhat is Continuous Intergration CI?\n\n\nContinuous integration is a software development practice where members of a team integrate their work frequently\n\n\n\n\nAutomate build and test executions of components\n\n\nBuild very often (eg. per commit) to detectregressions quickly\n\n\nTest often (unit and integration tests)\n\n\nAutomate regular code quality analysis exections\n\n\n\n\n\n\nWhat is Continuous Delivery?\n\n\n\n\nWhile CI lets you auomate the software build, scan and test process, CD automates the full application delivery pipeline taking new features and code from development to staging to production.\n\n\nCD is the ability to get new features, configuration changes, bug fixes into production or into the hands of the users safely and quickly in a sustainable way\n\n\n\n\n\n\nContinuous delivery vs continuous deployment\n\n\nContinuous delivery is a series of practices designed to ensure that code can be rapidly and safely deployed to production by delivering every change to a production-like environment and ensuring business applications and services function as expected through rigorous automated testing. Since every change is delivered to a staging environment using complete automation, you can have confidence the application can be deployed to production with a push of a button when the business is ready.\n\n\nContinuous deployment is the next step of continuous delivery: Every change that passes the automated tests is deployed to production automatically. Continuous deployment should be the goal of most companies that are not constrained by regulatory or other requirements.\n\n\n\n\nWhile continuous deployment may not be right for every company, continuous delivery is an absolute requirement of DevOps practices. Only when you continuously deliver your code can you have true confidence that your changes will be serving value to your customers within minutes of pushing the \"go\" button, and that you can actually push that button any time the business is ready for it.\n\n\nCICD, Why do it?\n\n\n\n\nMake releases painless\n\n\nReduce time to market\n\n\nIncrease software quality and stability\n\n\nReduce cost of ongoing software development\n\n\nSpeed up the feedback loop\n\n\n\n\nHow long would it take to your organization to deploy a changes that involves just one line of code?\n\n\nReferences\n\n\n\n\nCloudbees trainings\n\n\nContinuous delivery Vs continuous deployment", 
            "title": "CICD"
        }, 
        {
            "location": "/blog/cicd-fundamentals/#cicd-fundamentals", 
            "text": "", 
            "title": "CI/CD Fundamentals"
        }, 
        {
            "location": "/blog/cicd-fundamentals/#what-is-continuous-intergration-ci", 
            "text": "Continuous integration is a software development practice where members of a team integrate their work frequently   Automate build and test executions of components  Build very often (eg. per commit) to detectregressions quickly  Test often (unit and integration tests)  Automate regular code quality analysis exections", 
            "title": "What is Continuous Intergration CI?"
        }, 
        {
            "location": "/blog/cicd-fundamentals/#what-is-continuous-delivery", 
            "text": "While CI lets you auomate the software build, scan and test process, CD automates the full application delivery pipeline taking new features and code from development to staging to production.  CD is the ability to get new features, configuration changes, bug fixes into production or into the hands of the users safely and quickly in a sustainable way", 
            "title": "What is Continuous Delivery?"
        }, 
        {
            "location": "/blog/cicd-fundamentals/#continuous-delivery-vs-continuous-deployment", 
            "text": "Continuous delivery is a series of practices designed to ensure that code can be rapidly and safely deployed to production by delivering every change to a production-like environment and ensuring business applications and services function as expected through rigorous automated testing. Since every change is delivered to a staging environment using complete automation, you can have confidence the application can be deployed to production with a push of a button when the business is ready.  Continuous deployment is the next step of continuous delivery: Every change that passes the automated tests is deployed to production automatically. Continuous deployment should be the goal of most companies that are not constrained by regulatory or other requirements.   While continuous deployment may not be right for every company, continuous delivery is an absolute requirement of DevOps practices. Only when you continuously deliver your code can you have true confidence that your changes will be serving value to your customers within minutes of pushing the \"go\" button, and that you can actually push that button any time the business is ready for it.", 
            "title": "Continuous delivery vs continuous deployment"
        }, 
        {
            "location": "/blog/cicd-fundamentals/#cicd-why-do-it", 
            "text": "Make releases painless  Reduce time to market  Increase software quality and stability  Reduce cost of ongoing software development  Speed up the feedback loop   How long would it take to your organization to deploy a changes that involves just one line of code?", 
            "title": "CICD, Why do it?"
        }, 
        {
            "location": "/blog/cicd-fundamentals/#references", 
            "text": "Cloudbees trainings  Continuous delivery Vs continuous deployment", 
            "title": "References"
        }, 
        {
            "location": "/blog/networking-fundamentals/", 
            "text": "Networking Fundamentals\n\n\nWhat is the internet?\n\n\nThe internet is a huge network of computing devices communicating with each other based on a pre-agreed set of rules called protocols.\n\n\n\n\nThe internet is a global computer network providing a variety of information and communication facilities, consisting of interconnected networks using standardized communication protocols. (google definition)\n\n\n\n\n\n\nHow is this network formed and how does it work?\n\n\nThe edge devices that are part of this network are called hosts or end systems. For enample: Laptops, mobiles are the end systems. The end systems are connected by a network of communication links and packet switches. The communication links are made up of physical connections made of copper wires, fiber optics, radio spectrum etc. One end system sends data to another by segmenting the data into small segments with header bytes on them. These packages of information are called packets and are sent through the network to the destination end system where they are reassembled into the original data. A packet switch is responsible for routing the packets to its destination. The packet switch takes a packet arriving on one of its incoming communication links and forwards that packet on one of its outgoing communication links. Our router is a packet switch. The sequence of communication links and packet switches traversed by a packet in known as the route through the network.\n\n\n\n\nThere is beautiful analogy presented in the book Computer Networking by Kurose and Ross. It says, if a factory needs to move a large amount of cargo to some destination warehouse located thousands of kilometers away, then, the cargo is first segmented and loaded into a fleet of trucks. Each of the trucks then independently travels through the network of highways, roads, and intersections to the destination warehouse. At the destination warehouse, the cargo is unloaded and grouped with the rest of the cargo arriving from the same shipment. Thus, in many ways, packets are analogous to trucks, communication links analogous to highways and roads. Packet switches are analogous to intersections, and end systems are analogous to destination buildings.\n\n\n\n\nWhat are ISPs, TCP/IP and RFCs?\n\n\nISPs\n\n\nEnd systems access the internet through \nInternet Service Provider(ISPs)\n. ISPs include local cable or telephone companies. Each ISPs is in itself a network of packet switches and communication links. These ISPs are also interconnected. Lower-tier ISPs are interconnected through national and international upper-tier ISPs such as AT\nT and Sprint.\n\n\nTCP/IP\n\n\nAll the component of this network run protocols that control the sending and receiving of information. A protocol defines the format and the order of the messages exchanged between two or more communicating entities, as well as the actions taken on the transmission and/or receipt of a message or other events. The \nTransmission Control Protocol(TCP)\n and the \nInternet Protocol(IP)\n are the two of the most important protocols that are sent and received among the routers and end systems. The internet\u2019s principle protocols are collectively known as TCP/IP.\n\n\nIEFT and RFCs\n\n\nThese protocols are most important for the unanimous functioning and thus important that everyone agrees on what each and every protocol does. These Internet standards are developed by the \nInternet Engineering Task Force(IETF)\n. The IETF standard documents are called \nrequest for comments(RFCs)\n.\n\n\nHow does one application running on one end system instructs the Internet to deliver data to another software running on another end system?\n\n\nEnd systems attached to the Internet, provide an Application Programming Interface(API) that specifies the rules for this process. \n\n\n\n\nTaking an analogy from the same book: Suppose Alice wants to send a letter to Bob using the postal service. Alice, of course, can\u2019t just write the letter(the data) and drop the letter out her window. Instead, the postal service requires that Alice put the letter in an envelope; write Bob\u2019s full name, address, and zip code in the center of the envelope; seal the envelope; put a stamp in the upper-right-hand corner of the envelope; and finally, drop the envelope into an official postal service mailbox. Thus, the postal service has its own \u201cpostal service API\u201d, or set of rules, that Alice must follow to have the postal service deliver her letter to Bob.\n\n\n\n\nIn a similar manner, the internet has an API that the software sending data must follow to have the internet deliver the data to the software that will receive the data.\n\n\nWhat are client and server program and P2P architecture?\n\n\nA client program is a program running on one end system that requests and receives a service from a server program running on another end system. The web browser is an example of a client program and Nginx or NodeJS or Tomcat is an example of a server program.\n\n\nNot all Internet application today consists of pure client programs interacting with pure server programs. Increasingly, many application is peer-to-peer(P2P) applications, in which end systems interact and run programs that perform both client and server functions. For example, in P2P file-sharing applications(such as BitTorrent or eMule), the program in the user\u2019s end system act as a client when it requests a file from another peer; and the program acts as a server when it sends a file to another peer.\n\n\nWhat are LAN and WAN?\n\n\nLAN\n\n\nLocal Area Network (LAN)\n is a computer network, which is limited to a small office, a single building, multiple buildings inside a campus etc. Typically a LAN is a private network owned and maintained by a single organization.\n\n\nWAN\n\n\nA \nWide Area Network (WAN)\n spans over multiple geographic locations, which is composed of multiple LANs. ISPs provide the connectivity solutions for WAN.\n\n\nWhat is an IP address?\n\n\nFor one device to communicate with another, it needs an IP address, and it must be unique. If there is another device on the same network with the same IP there will be an IP address conflict and both devices will lose network capability until this is resolved.\n\n\nThe IP address consists of 4 numbers separated by decimals. The IP address itself is separated into a network address and a host address. This means that one part of the IP address identifies the computer network ID and the other part identifies the host ID.\nAs an example, an IP address of 192.168.0.11 with subnet mask 255.255.255.0 uses the first 3 numbers to identify the network and the last number to identify the host. So, the network id would be 192.168.0 and the host id would be 11. Devices can only communicate with other devices on the same network id. In other words, communication will work between 2 devices with IPs 192.168.0.221 and 192.168.0.11 respectively but neither can communicate with 192.168.1.31 because it is part of the 192.168.1 network.\n\n\nSubnet Mask\n : As a general rule wherever there is a 255 in the subnet mask then the corresponding number of the IP address is part of the network id; where there is 0 in the subnet mask the corresponding number in the IP address is part of the host id. For an IP address of 192.168.0.1 with a subnet mask of 255.0.0.0. This tells the device that the first number of the IP address is to be used as the network address and the last 3 are to be used as the host id. In this example, the computer network would be 192.x.x.x. As long as another computer has the same subnet mask and an IP address starting with 192 they can communicate with each other. If the subnet mask was 255.255.0.0 then this means that the first 2 numbers identify the network instead (192.168.x.x). Therefore to be on the same network both devices must have IP addresses starting with 192.168.\n\n\nHow do devices on different networks communicate?\n\n\nCommunication across different network IDs take place with the help of a router. A router is a network device with 2 \nnetwork interfaces (NICs)\n, each being on separate network ids. So, we may have 2 networks; 192.168.1.x and 192.168.2.x. On one NIC the router would have the IP address 192.168.1.1 and on the other, it would have an IP address of 192.168.2.1. Devices on the 192.168.1.x network can now communicate with devices on the 192.168.2.x network via the router.\n\n\nHow does the data travel from source end system to destination end system?\n\n\nThe default gateway is where a network device sends traffic to if it doesn\u2019t know the destination IP address. The default gateway is always a router.\nWhen a network device tries to communicate with another on the same network it sends the data directly to it. If it is on a separate network it forwards the data to whatever IP address is specified in the default gateway. This is because it doesn\u2019t know of this other network and it needs to send the data to a gateway out of its own network. This is why we always put the IP address of the router in the default gateway field. Because a router will be attached to multiple networks, so it knows where these other networks are and it can route traffic to them. Routers also have default gateways so that if they don\u2019t know where the destination is then they can also send the data to its own default gateway. This continues up the IP network hierarchy until it eventually finds a router that is part of the destination network. This last router knows where the destination is and sends it on its way.\n\n\nWhat is DHCP?\n\n\nNetwork devices need to be configured with an IP address, subnet mask and default gateway that will be unique to that network. Generally, we don\u2019t manually configure them but are configured automatically using DHCP servers. DHCP stands for \nDynamic Host Configuration Protocol\n. Servers and some routers can be configured to act as a DHCP server. It allots the IP addresses to the connecting devices so to prevent IP address conflicts.\n\n\nHow does a router function?\n\n\nA router should have at least two \nnetwork cards (NICs)\n, one physically connected to one network and the other physically connected to another network. A router can connect any number of networks together providing it has a dedicated NIC for each network.\n\n\nRouters also learn which are the fastest routes and use them first. Each route the router knows of has a metric value assigned to it. A metric value is basically a preference number. If there are two routes to the same destination then the one with the lowest metric is assumed to be the most efficient. Routers will always use this route first until it fails, in which case it will then try the route with the next lowest metric and so on.\n\n\nAll network devices that use the TCP/IP protocol have a routing table. On Linux based system run netstat -rn command to view this table. All devices use their routing table to determine where to send packets. When a device sends packets to another device, it looks at its routing table to determine the best route possible. If it finds the destination address is \u201con-link\u201d it knows that it is a part of the same subnet as the destination and sends the packets directly to the device. If not it forwards the packet onto whatever is in the gateway field of the matching route entry. This same process is repeated at every router/hop along the way until it eventually arrives at a router that is part of the destination network.\n\n\nWhat is a port?\n\n\nIn the internet protocol suite, a port is an endpoint of communication in an operating system, in software, it is a logical construct that identifies a specific process or a type of network service.\n\n\nA port is always associated with an IP address of a host and the protocol of the communication. A port is identified for each address and protocol by a 16-bit number, commonly known as the port number. For example, an address may be \u201cprotocol: TCP, IP address: 1.2.3.4, port number: 80\u201d, which is written as 1.2.3.4:80 when the protocol is known from context.\n\n\nThe port, which is the number after \u201c:\u201d in the IP address defines the port on which the data has to be sent.By default, HTTP uses port 80 and HTTPS uses port 443, and we don\u2019t add them in the browser but are implicitly handled by the browsers. But a URL like \nhttp://www.example.com:8080/path/\n specifies that the web browser connects instead to port 8080 of the HTTP server.\n\n\nWhat is port forwarding?\n\n\nPort forwarding or port mapping is an application of \nnetwork address translation (NAT)\n that redirects a communication request from one address and port number combination to another while the packets are traversing a network gateway, such as a router or firewall. This technique is most commonly used to make services on a host residing on a protected or masqueraded (internal) network available to hosts on the opposite side of the gateway (external network), by remapping the destination IP address and port number of the communication to an internal host.\n\n\nWhat this mean is that when a request arrives at a router at a specific port then it reroutes this request to a server in the local network of the router so to be able to process it (if configured on NAT settings). So, it in a way works as a gatekeeper.\n\n\nWhat is DNS?\n\n\nDomain names are the human-friendly forms of Internet addresses and are commonly used to find websites. The domain name system(DNS) is essentially a global addressing system. It is the way that domain names are located and translated into Internet Protocol (IP) addresses, and vice versa. A domain name such as example.com is a unique alias for an IP address 123.123.123.123, which is an actual physical point on the Internet. The \nInternet Corporation for Assigned Names and Numbers (ICANN)\n is a non-profit organization responsible for coordinating the maintenance and procedures of several databases related to the namespaces of the Internet. We can buy a domain name from a vendor like GoDaddy and others for a period and then tell that domain to point to an actual server address using the websites of those vendors.\n\n\nWhat is a public IP address?\n\n\nA public IP address is an IP address that our home or business router receives from your ISP. Public IP addresses are required for any publicly accessible network devices, like for our home router as well as for the servers that host websites.\n\n\nPublic IP addresses are what differentiate all devices that are plugged into the public internet. Each and every device that\u2019s accessing the internet is using a unique IP address. It\u2019s this address that each Internet Service Provider uses to forward internet requests to a specific home or business.", 
            "title": "Networking"
        }, 
        {
            "location": "/blog/networking-fundamentals/#networking-fundamentals", 
            "text": "", 
            "title": "Networking Fundamentals"
        }, 
        {
            "location": "/blog/networking-fundamentals/#what-is-the-internet", 
            "text": "The internet is a huge network of computing devices communicating with each other based on a pre-agreed set of rules called protocols.   The internet is a global computer network providing a variety of information and communication facilities, consisting of interconnected networks using standardized communication protocols. (google definition)", 
            "title": "What is the internet?"
        }, 
        {
            "location": "/blog/networking-fundamentals/#how-is-this-network-formed-and-how-does-it-work", 
            "text": "The edge devices that are part of this network are called hosts or end systems. For enample: Laptops, mobiles are the end systems. The end systems are connected by a network of communication links and packet switches. The communication links are made up of physical connections made of copper wires, fiber optics, radio spectrum etc. One end system sends data to another by segmenting the data into small segments with header bytes on them. These packages of information are called packets and are sent through the network to the destination end system where they are reassembled into the original data. A packet switch is responsible for routing the packets to its destination. The packet switch takes a packet arriving on one of its incoming communication links and forwards that packet on one of its outgoing communication links. Our router is a packet switch. The sequence of communication links and packet switches traversed by a packet in known as the route through the network.   There is beautiful analogy presented in the book Computer Networking by Kurose and Ross. It says, if a factory needs to move a large amount of cargo to some destination warehouse located thousands of kilometers away, then, the cargo is first segmented and loaded into a fleet of trucks. Each of the trucks then independently travels through the network of highways, roads, and intersections to the destination warehouse. At the destination warehouse, the cargo is unloaded and grouped with the rest of the cargo arriving from the same shipment. Thus, in many ways, packets are analogous to trucks, communication links analogous to highways and roads. Packet switches are analogous to intersections, and end systems are analogous to destination buildings.", 
            "title": "How is this network formed and how does it work?"
        }, 
        {
            "location": "/blog/networking-fundamentals/#what-are-isps-tcpip-and-rfcs", 
            "text": "", 
            "title": "What are ISPs, TCP/IP and RFCs?"
        }, 
        {
            "location": "/blog/networking-fundamentals/#isps", 
            "text": "End systems access the internet through  Internet Service Provider(ISPs) . ISPs include local cable or telephone companies. Each ISPs is in itself a network of packet switches and communication links. These ISPs are also interconnected. Lower-tier ISPs are interconnected through national and international upper-tier ISPs such as AT T and Sprint.", 
            "title": "ISPs"
        }, 
        {
            "location": "/blog/networking-fundamentals/#tcpip", 
            "text": "All the component of this network run protocols that control the sending and receiving of information. A protocol defines the format and the order of the messages exchanged between two or more communicating entities, as well as the actions taken on the transmission and/or receipt of a message or other events. The  Transmission Control Protocol(TCP)  and the  Internet Protocol(IP)  are the two of the most important protocols that are sent and received among the routers and end systems. The internet\u2019s principle protocols are collectively known as TCP/IP.", 
            "title": "TCP/IP"
        }, 
        {
            "location": "/blog/networking-fundamentals/#ieft-and-rfcs", 
            "text": "These protocols are most important for the unanimous functioning and thus important that everyone agrees on what each and every protocol does. These Internet standards are developed by the  Internet Engineering Task Force(IETF) . The IETF standard documents are called  request for comments(RFCs) .", 
            "title": "IEFT and RFCs"
        }, 
        {
            "location": "/blog/networking-fundamentals/#how-does-one-application-running-on-one-end-system-instructs-the-internet-to-deliver-data-to-another-software-running-on-another-end-system", 
            "text": "End systems attached to the Internet, provide an Application Programming Interface(API) that specifies the rules for this process.    Taking an analogy from the same book: Suppose Alice wants to send a letter to Bob using the postal service. Alice, of course, can\u2019t just write the letter(the data) and drop the letter out her window. Instead, the postal service requires that Alice put the letter in an envelope; write Bob\u2019s full name, address, and zip code in the center of the envelope; seal the envelope; put a stamp in the upper-right-hand corner of the envelope; and finally, drop the envelope into an official postal service mailbox. Thus, the postal service has its own \u201cpostal service API\u201d, or set of rules, that Alice must follow to have the postal service deliver her letter to Bob.   In a similar manner, the internet has an API that the software sending data must follow to have the internet deliver the data to the software that will receive the data.", 
            "title": "How does one application running on one end system instructs the Internet to deliver data to another software running on another end system?"
        }, 
        {
            "location": "/blog/networking-fundamentals/#what-are-client-and-server-program-and-p2p-architecture", 
            "text": "A client program is a program running on one end system that requests and receives a service from a server program running on another end system. The web browser is an example of a client program and Nginx or NodeJS or Tomcat is an example of a server program.  Not all Internet application today consists of pure client programs interacting with pure server programs. Increasingly, many application is peer-to-peer(P2P) applications, in which end systems interact and run programs that perform both client and server functions. For example, in P2P file-sharing applications(such as BitTorrent or eMule), the program in the user\u2019s end system act as a client when it requests a file from another peer; and the program acts as a server when it sends a file to another peer.", 
            "title": "What are client and server program and P2P architecture?"
        }, 
        {
            "location": "/blog/networking-fundamentals/#what-are-lan-and-wan", 
            "text": "", 
            "title": "What are LAN and WAN?"
        }, 
        {
            "location": "/blog/networking-fundamentals/#lan", 
            "text": "Local Area Network (LAN)  is a computer network, which is limited to a small office, a single building, multiple buildings inside a campus etc. Typically a LAN is a private network owned and maintained by a single organization.", 
            "title": "LAN"
        }, 
        {
            "location": "/blog/networking-fundamentals/#wan", 
            "text": "A  Wide Area Network (WAN)  spans over multiple geographic locations, which is composed of multiple LANs. ISPs provide the connectivity solutions for WAN.", 
            "title": "WAN"
        }, 
        {
            "location": "/blog/networking-fundamentals/#what-is-an-ip-address", 
            "text": "For one device to communicate with another, it needs an IP address, and it must be unique. If there is another device on the same network with the same IP there will be an IP address conflict and both devices will lose network capability until this is resolved.  The IP address consists of 4 numbers separated by decimals. The IP address itself is separated into a network address and a host address. This means that one part of the IP address identifies the computer network ID and the other part identifies the host ID.\nAs an example, an IP address of 192.168.0.11 with subnet mask 255.255.255.0 uses the first 3 numbers to identify the network and the last number to identify the host. So, the network id would be 192.168.0 and the host id would be 11. Devices can only communicate with other devices on the same network id. In other words, communication will work between 2 devices with IPs 192.168.0.221 and 192.168.0.11 respectively but neither can communicate with 192.168.1.31 because it is part of the 192.168.1 network.  Subnet Mask  : As a general rule wherever there is a 255 in the subnet mask then the corresponding number of the IP address is part of the network id; where there is 0 in the subnet mask the corresponding number in the IP address is part of the host id. For an IP address of 192.168.0.1 with a subnet mask of 255.0.0.0. This tells the device that the first number of the IP address is to be used as the network address and the last 3 are to be used as the host id. In this example, the computer network would be 192.x.x.x. As long as another computer has the same subnet mask and an IP address starting with 192 they can communicate with each other. If the subnet mask was 255.255.0.0 then this means that the first 2 numbers identify the network instead (192.168.x.x). Therefore to be on the same network both devices must have IP addresses starting with 192.168.", 
            "title": "What is an IP address?"
        }, 
        {
            "location": "/blog/networking-fundamentals/#how-do-devices-on-different-networks-communicate", 
            "text": "Communication across different network IDs take place with the help of a router. A router is a network device with 2  network interfaces (NICs) , each being on separate network ids. So, we may have 2 networks; 192.168.1.x and 192.168.2.x. On one NIC the router would have the IP address 192.168.1.1 and on the other, it would have an IP address of 192.168.2.1. Devices on the 192.168.1.x network can now communicate with devices on the 192.168.2.x network via the router.", 
            "title": "How do devices on different networks communicate?"
        }, 
        {
            "location": "/blog/networking-fundamentals/#how-does-the-data-travel-from-source-end-system-to-destination-end-system", 
            "text": "The default gateway is where a network device sends traffic to if it doesn\u2019t know the destination IP address. The default gateway is always a router.\nWhen a network device tries to communicate with another on the same network it sends the data directly to it. If it is on a separate network it forwards the data to whatever IP address is specified in the default gateway. This is because it doesn\u2019t know of this other network and it needs to send the data to a gateway out of its own network. This is why we always put the IP address of the router in the default gateway field. Because a router will be attached to multiple networks, so it knows where these other networks are and it can route traffic to them. Routers also have default gateways so that if they don\u2019t know where the destination is then they can also send the data to its own default gateway. This continues up the IP network hierarchy until it eventually finds a router that is part of the destination network. This last router knows where the destination is and sends it on its way.", 
            "title": "How does the data travel from source end system to destination end system?"
        }, 
        {
            "location": "/blog/networking-fundamentals/#what-is-dhcp", 
            "text": "Network devices need to be configured with an IP address, subnet mask and default gateway that will be unique to that network. Generally, we don\u2019t manually configure them but are configured automatically using DHCP servers. DHCP stands for  Dynamic Host Configuration Protocol . Servers and some routers can be configured to act as a DHCP server. It allots the IP addresses to the connecting devices so to prevent IP address conflicts.", 
            "title": "What is DHCP?"
        }, 
        {
            "location": "/blog/networking-fundamentals/#how-does-a-router-function", 
            "text": "A router should have at least two  network cards (NICs) , one physically connected to one network and the other physically connected to another network. A router can connect any number of networks together providing it has a dedicated NIC for each network.  Routers also learn which are the fastest routes and use them first. Each route the router knows of has a metric value assigned to it. A metric value is basically a preference number. If there are two routes to the same destination then the one with the lowest metric is assumed to be the most efficient. Routers will always use this route first until it fails, in which case it will then try the route with the next lowest metric and so on.  All network devices that use the TCP/IP protocol have a routing table. On Linux based system run netstat -rn command to view this table. All devices use their routing table to determine where to send packets. When a device sends packets to another device, it looks at its routing table to determine the best route possible. If it finds the destination address is \u201con-link\u201d it knows that it is a part of the same subnet as the destination and sends the packets directly to the device. If not it forwards the packet onto whatever is in the gateway field of the matching route entry. This same process is repeated at every router/hop along the way until it eventually arrives at a router that is part of the destination network.", 
            "title": "How does a router function?"
        }, 
        {
            "location": "/blog/networking-fundamentals/#what-is-a-port", 
            "text": "In the internet protocol suite, a port is an endpoint of communication in an operating system, in software, it is a logical construct that identifies a specific process or a type of network service.  A port is always associated with an IP address of a host and the protocol of the communication. A port is identified for each address and protocol by a 16-bit number, commonly known as the port number. For example, an address may be \u201cprotocol: TCP, IP address: 1.2.3.4, port number: 80\u201d, which is written as 1.2.3.4:80 when the protocol is known from context.  The port, which is the number after \u201c:\u201d in the IP address defines the port on which the data has to be sent.By default, HTTP uses port 80 and HTTPS uses port 443, and we don\u2019t add them in the browser but are implicitly handled by the browsers. But a URL like  http://www.example.com:8080/path/  specifies that the web browser connects instead to port 8080 of the HTTP server.", 
            "title": "What is a port?"
        }, 
        {
            "location": "/blog/networking-fundamentals/#what-is-port-forwarding", 
            "text": "Port forwarding or port mapping is an application of  network address translation (NAT)  that redirects a communication request from one address and port number combination to another while the packets are traversing a network gateway, such as a router or firewall. This technique is most commonly used to make services on a host residing on a protected or masqueraded (internal) network available to hosts on the opposite side of the gateway (external network), by remapping the destination IP address and port number of the communication to an internal host.  What this mean is that when a request arrives at a router at a specific port then it reroutes this request to a server in the local network of the router so to be able to process it (if configured on NAT settings). So, it in a way works as a gatekeeper.", 
            "title": "What is port forwarding?"
        }, 
        {
            "location": "/blog/networking-fundamentals/#what-is-dns", 
            "text": "Domain names are the human-friendly forms of Internet addresses and are commonly used to find websites. The domain name system(DNS) is essentially a global addressing system. It is the way that domain names are located and translated into Internet Protocol (IP) addresses, and vice versa. A domain name such as example.com is a unique alias for an IP address 123.123.123.123, which is an actual physical point on the Internet. The  Internet Corporation for Assigned Names and Numbers (ICANN)  is a non-profit organization responsible for coordinating the maintenance and procedures of several databases related to the namespaces of the Internet. We can buy a domain name from a vendor like GoDaddy and others for a period and then tell that domain to point to an actual server address using the websites of those vendors.", 
            "title": "What is DNS?"
        }, 
        {
            "location": "/blog/networking-fundamentals/#what-is-a-public-ip-address", 
            "text": "A public IP address is an IP address that our home or business router receives from your ISP. Public IP addresses are required for any publicly accessible network devices, like for our home router as well as for the servers that host websites.  Public IP addresses are what differentiate all devices that are plugged into the public internet. Each and every device that\u2019s accessing the internet is using a unique IP address. It\u2019s this address that each Internet Service Provider uses to forward internet requests to a specific home or business.", 
            "title": "What is a public IP address?"
        }, 
        {
            "location": "/blog/host-internet-accessible-website-on-laptop/", 
            "text": "How to host internet accessible website on your laptop?\n\n\nThis document explains how to setup the router (ZTE H369A) for receiving the request on its public IP address and then forward the request to a computer (macbook) in the local network.\n\n\nAccess router's software\n\n\nAccess your router software using a browser by entering its local IP address. You can find the local IP address using network details option in the connectivity of your laptop. The routers IP is the Default gateway address. (Mine is 192.168.2.254)\n\n\n\n\nOnce you access the routers IP you will be directed to the routers appliction login screen.\n\n\n\n\nProvide the user name and password which generally is written at the bottom of the router or is \u201cadmin\u201d for both entries. The steps you will take will vary according to the brand and model of your router but the process will be more or less the same. My router is the ZTE H369A, on which the default password is null. Thue the first login should be done leaving the password field empty. Make sure to set a strong password for security.\n\n\n\n\n\n\nThis site provides steps for port forwarding for most of the Routers: \nhttps://portforward.com/router.htm\n\n\n\n\nBelow steps will explain port forwarding settings for the ZTE H369A router:\n\n\nStep 1: Find the public IP of the router\n\n\nClick on the Status tab on the top navigation bar.\n(If you will enter the WAN IP Address then you will see your router\u2019s login page.)\n\n\n\n\nStep 2: Setup Port forwarding\n\n\nFind the port forwarding settings under the Settings tab in the top navigation bar\n\n\n\n\nForward the traffic comming to the router on the desired IP and ports\n\n\n\n\n\n\nStep 3: Run a server program on your laptop\n\n\nRun any application on your laptop on the port 8888 and 8080 and you will see the application is accessible via the public IP.\n\n\nStep 4: Register a domain and configure DNS for your public IP (Optional)\n\n\nYou can also register a domain on sites like goDaddy or others and point the domain to the public IP of your router (Which is your WAN IP).\n\n\nI have registers a domain privatesquare.in and created a A record cicd.privatesquare.in on the domain settings to point to my WAN IP. Thus my webapp can be accessed on cicd.privatesquare.in:8080 from the public internet.\n\n\n\n\nPS: The webapp will only be accessible when my laptop is powered on!", 
            "title": "Host internet accessible website on your laptop"
        }, 
        {
            "location": "/blog/host-internet-accessible-website-on-laptop/#how-to-host-internet-accessible-website-on-your-laptop", 
            "text": "This document explains how to setup the router (ZTE H369A) for receiving the request on its public IP address and then forward the request to a computer (macbook) in the local network.", 
            "title": "How to host internet accessible website on your laptop?"
        }, 
        {
            "location": "/blog/host-internet-accessible-website-on-laptop/#access-routers-software", 
            "text": "Access your router software using a browser by entering its local IP address. You can find the local IP address using network details option in the connectivity of your laptop. The routers IP is the Default gateway address. (Mine is 192.168.2.254)   Once you access the routers IP you will be directed to the routers appliction login screen.   Provide the user name and password which generally is written at the bottom of the router or is \u201cadmin\u201d for both entries. The steps you will take will vary according to the brand and model of your router but the process will be more or less the same. My router is the ZTE H369A, on which the default password is null. Thue the first login should be done leaving the password field empty. Make sure to set a strong password for security.    This site provides steps for port forwarding for most of the Routers:  https://portforward.com/router.htm   Below steps will explain port forwarding settings for the ZTE H369A router:", 
            "title": "Access router's software"
        }, 
        {
            "location": "/blog/host-internet-accessible-website-on-laptop/#step-1-find-the-public-ip-of-the-router", 
            "text": "Click on the Status tab on the top navigation bar.\n(If you will enter the WAN IP Address then you will see your router\u2019s login page.)", 
            "title": "Step 1: Find the public IP of the router"
        }, 
        {
            "location": "/blog/host-internet-accessible-website-on-laptop/#step-2-setup-port-forwarding", 
            "text": "Find the port forwarding settings under the Settings tab in the top navigation bar   Forward the traffic comming to the router on the desired IP and ports", 
            "title": "Step 2: Setup Port forwarding"
        }, 
        {
            "location": "/blog/host-internet-accessible-website-on-laptop/#step-3-run-a-server-program-on-your-laptop", 
            "text": "Run any application on your laptop on the port 8888 and 8080 and you will see the application is accessible via the public IP.", 
            "title": "Step 3: Run a server program on your laptop"
        }, 
        {
            "location": "/blog/host-internet-accessible-website-on-laptop/#step-4-register-a-domain-and-configure-dns-for-your-public-ip-optional", 
            "text": "You can also register a domain on sites like goDaddy or others and point the domain to the public IP of your router (Which is your WAN IP).  I have registers a domain privatesquare.in and created a A record cicd.privatesquare.in on the domain settings to point to my WAN IP. Thus my webapp can be accessed on cicd.privatesquare.in:8080 from the public internet.   PS: The webapp will only be accessible when my laptop is powered on!", 
            "title": "Step 4: Register a domain and configure DNS for your public IP (Optional)"
        }, 
        {
            "location": "/blog/cloud-computing/", 
            "text": "What is Cloud Computing?\n\n\nCloud computing is the on-demand delivery of compute power, database storage, applications, and other IT resources through a cloud services platform via the internet with pay-as-you-go pricing.\n\n\nCloud Computing Basics\n\n\nWhether you are running applications that share photos to millions of mobile users or you\u2019re supporting the critical operations of your business, a cloud services platform provides rapid access to flexible and low cost IT resources. With cloud computing, you don\u2019t need to make large upfront investments in hardware and spend a lot of time on the heavy lifting of managing that hardware. Instead, you can provision exactly the right type and size of computing resources you need to power your newest bright idea or operate your IT department. You can access as many resources as you need, almost instantly, and only pay for what you use.\n\n\nHow Does Cloud Computing Work?\n\n\nCloud computing provides a simple way to access servers, storage, databases and a broad set of application services over the Internet. A Cloud services platform such as Amazon Web Services owns and maintains the network-connected hardware required for these application services, while you provision and use what you need via a web application.\n\n\nAdvantages of cloud computing\n\n\nTrade capital expense for variable expense\n\n\nInstead of having to invest heavily in data centers and servers before you know how you\u2019re going to use them, you can only pay when you consume computing resources, and only pay for how much you consume.\n\n\nBenefit from massive economies of scale\n\n\nBy using cloud computing, you can achieve a lower variable cost than you can get on your own. Because usage from hundreds of thousands of customers are aggregated in the cloud, providers such as Amazon Web Services can achieve higher economies of scale which translates into lower pay as you go prices.\n\n\nStop guessing capacity\n\n\nEliminate guessing on your infrastructure capacity needs. When you make a capacity decision prior to deploying an application, you often either end up sitting on expensive idle resources or dealing with limited capacity. With cloud computing, these problems go away. You can access as much or as little as you need, and scale up and down as required with only a few minutes notice.\n\n\nIncrease speed and agility\n\n\nIn a cloud computing environment, new IT resources are only ever a click away, which means you reduce the time it takes to make those resources available to your developers from weeks to just minutes. This results in a dramatic increase in agility for the organization, since the cost and time it takes to experiment and develop is significantly lower.\n\n\nStop spending money on running and maintaining data centers\n\n\nFocus on projects that differentiate your business, not the infrastructure. Cloud computing lets you focus on your own customers, rather than on the heavy lifting of racking, stacking and powering servers.\n\n\nGo global in minutes\n\n\nEasily deploy your application in multiple regions around the world with just a few clicks. This means you can provide a lower latency and better experience for your customers simply and at minimal cost.", 
            "title": "Cloud Computing"
        }, 
        {
            "location": "/blog/cloud-computing/#what-is-cloud-computing", 
            "text": "Cloud computing is the on-demand delivery of compute power, database storage, applications, and other IT resources through a cloud services platform via the internet with pay-as-you-go pricing.", 
            "title": "What is Cloud Computing?"
        }, 
        {
            "location": "/blog/cloud-computing/#cloud-computing-basics", 
            "text": "Whether you are running applications that share photos to millions of mobile users or you\u2019re supporting the critical operations of your business, a cloud services platform provides rapid access to flexible and low cost IT resources. With cloud computing, you don\u2019t need to make large upfront investments in hardware and spend a lot of time on the heavy lifting of managing that hardware. Instead, you can provision exactly the right type and size of computing resources you need to power your newest bright idea or operate your IT department. You can access as many resources as you need, almost instantly, and only pay for what you use.", 
            "title": "Cloud Computing Basics"
        }, 
        {
            "location": "/blog/cloud-computing/#how-does-cloud-computing-work", 
            "text": "Cloud computing provides a simple way to access servers, storage, databases and a broad set of application services over the Internet. A Cloud services platform such as Amazon Web Services owns and maintains the network-connected hardware required for these application services, while you provision and use what you need via a web application.", 
            "title": "How Does Cloud Computing Work?"
        }, 
        {
            "location": "/blog/cloud-computing/#advantages-of-cloud-computing", 
            "text": "", 
            "title": "Advantages of cloud computing"
        }, 
        {
            "location": "/blog/cloud-computing/#trade-capital-expense-for-variable-expense", 
            "text": "Instead of having to invest heavily in data centers and servers before you know how you\u2019re going to use them, you can only pay when you consume computing resources, and only pay for how much you consume.", 
            "title": "Trade capital expense for variable expense"
        }, 
        {
            "location": "/blog/cloud-computing/#benefit-from-massive-economies-of-scale", 
            "text": "By using cloud computing, you can achieve a lower variable cost than you can get on your own. Because usage from hundreds of thousands of customers are aggregated in the cloud, providers such as Amazon Web Services can achieve higher economies of scale which translates into lower pay as you go prices.", 
            "title": "Benefit from massive economies of scale"
        }, 
        {
            "location": "/blog/cloud-computing/#stop-guessing-capacity", 
            "text": "Eliminate guessing on your infrastructure capacity needs. When you make a capacity decision prior to deploying an application, you often either end up sitting on expensive idle resources or dealing with limited capacity. With cloud computing, these problems go away. You can access as much or as little as you need, and scale up and down as required with only a few minutes notice.", 
            "title": "Stop guessing capacity"
        }, 
        {
            "location": "/blog/cloud-computing/#increase-speed-and-agility", 
            "text": "In a cloud computing environment, new IT resources are only ever a click away, which means you reduce the time it takes to make those resources available to your developers from weeks to just minutes. This results in a dramatic increase in agility for the organization, since the cost and time it takes to experiment and develop is significantly lower.", 
            "title": "Increase speed and agility"
        }, 
        {
            "location": "/blog/cloud-computing/#stop-spending-money-on-running-and-maintaining-data-centers", 
            "text": "Focus on projects that differentiate your business, not the infrastructure. Cloud computing lets you focus on your own customers, rather than on the heavy lifting of racking, stacking and powering servers.", 
            "title": "Stop spending money on running and maintaining data centers"
        }, 
        {
            "location": "/blog/cloud-computing/#go-global-in-minutes", 
            "text": "Easily deploy your application in multiple regions around the world with just a few clicks. This means you can provide a lower latency and better experience for your customers simply and at minimal cost.", 
            "title": "Go global in minutes"
        }, 
        {
            "location": "/blog/raft-consensus-algorithm/", 
            "text": "Raft Consensus Algorithm\n\n\nRead about the algorithm via the below links:\n\n\nRaft Consensus Algorithm - Github\n\n\nBlog on Medium\n\n\nWhy odd number of nodes?\n\n\nThe reason is that in a distributed transactional system with a raft algorithm you need a quorum ( majority ). Essentially a transaction is committed once more than 50% of nodes say that the transaction is committed.\n\n\nYou could do 4 nodes as well but you would not get any benefit over 3 nodes and you add additional overhead. 4 Nodescan only survive 1 failed node because 3 nodes are a majority and not 2. Therefore you need an uneven number. 3 nodes can survive 1 failure, 5 nodes can survive 2 failures, 7 nodes can survive 3 failures and so on...\n\n\nJust imagine how many node failures your cluster can tolerate. Formula is:\n\n\n1\nNumber of node failures that can be tolerated = Round (N/2) - 1\n\n\n\n\n\n\nApplying the above formula for your cluster it is Round (5/2) \u2013 1 =  2 Nodes.  Now say we had 6 nodes instead of 5.Re-applying the formula gives you Round(6/2) \u2013 1 = 2 Nodes again.  So the extra node doesn\u2019t add any tangible benefitfor the cluster. So replicating to that one extra node is just a performance overhead.\n\n\nOther way to put it, when you are thinking about availability and fault tolerance of your cluster, just think howmany node failures do you want your cluster to tolerate.  If you want N failures to tolerate, then you need 2N + 1nodes in your cluster. Apply this to our case, say, we wanted to tolerate 2 failures hence we have 2(2) + 1 = 5 nodesin our cluster. if you want a 3 node failure to be tolerated it will 2(3) + 1 = 7. This only goes as odd numbers.", 
            "title": "Raft Consesus Algorithm"
        }, 
        {
            "location": "/blog/raft-consensus-algorithm/#raft-consensus-algorithm", 
            "text": "Read about the algorithm via the below links:  Raft Consensus Algorithm - Github  Blog on Medium", 
            "title": "Raft Consensus Algorithm"
        }, 
        {
            "location": "/blog/raft-consensus-algorithm/#why-odd-number-of-nodes", 
            "text": "The reason is that in a distributed transactional system with a raft algorithm you need a quorum ( majority ). Essentially a transaction is committed once more than 50% of nodes say that the transaction is committed.  You could do 4 nodes as well but you would not get any benefit over 3 nodes and you add additional overhead. 4 Nodescan only survive 1 failed node because 3 nodes are a majority and not 2. Therefore you need an uneven number. 3 nodes can survive 1 failure, 5 nodes can survive 2 failures, 7 nodes can survive 3 failures and so on...  Just imagine how many node failures your cluster can tolerate. Formula is:  1 Number of node failures that can be tolerated = Round (N/2) - 1   Applying the above formula for your cluster it is Round (5/2) \u2013 1 =  2 Nodes.  Now say we had 6 nodes instead of 5.Re-applying the formula gives you Round(6/2) \u2013 1 = 2 Nodes again.  So the extra node doesn\u2019t add any tangible benefitfor the cluster. So replicating to that one extra node is just a performance overhead.  Other way to put it, when you are thinking about availability and fault tolerance of your cluster, just think howmany node failures do you want your cluster to tolerate.  If you want N failures to tolerate, then you need 2N + 1nodes in your cluster. Apply this to our case, say, we wanted to tolerate 2 failures hence we have 2(2) + 1 = 5 nodesin our cluster. if you want a 3 node failure to be tolerated it will 2(3) + 1 = 7. This only goes as odd numbers.", 
            "title": "Why odd number of nodes?"
        }, 
        {
            "location": "/blog/container-orchestration/", 
            "text": "Containers and Container Orchestration\n\n\nWhat are Containers?\n\n\nShipping containers are efficiently moved using different modes of transport \u2013 perhaps initially being carried by a truck to a port, then neatly stacked alongside thousands of other shipping containers on a huge container ship that carries them to the other side of the world. At no point in the journey do the contents of that container need to repacked or modified in any way.\n\n\nShipping containers are ubiquitous, standardized, and available anywhere in the world, and they're extremely simple to use \u2013 just open them up, load in your cargo, and lock the doors shut.\n\n\nThe contents of each container are kept isolated from that of the others; the container full of Mentos can safely sit next to the container full of soda without any risk of a reaction. Once a spot on the container ship has been booked, you can be confident that there's room for all of your packed cargo for the whole trip \u2013 there's no way for a neighboring container to steal more than its share of space.\n\n\n\n\nSoftware containers fulfill a similar role for your application. Packing the container involves defining what needs to be there for your application to work \u2013 libraries, configuration files, application binaries, and other parts of your technology stack. Once the container has been defined, that \nimage\n is used to create containers that run in any environment, from the developer's laptop to your test/QA rig, to the production data center, on-premises or in the cloud, without any changes. This consistency can be very useful: for example, a support engineer can spin up a container to replicate an issue and be confident that it exactly matches what's running in the field.\n\n\nContainers are very efficient and many of them can run on the same machine, allowing full use of all available resources. Linux containers and cgroups are used to make sure that there's no cross-contamination between containers: data files, libraries, ports, namespaces, and memory contents are all kept isolated. They also enforce upper boundaries on how much system resource (memory, storage, CPU, network bandwidth, and disk I/O) a container can consume so that a critical application isn't squeezed out by noisy neighbors.\n\n\nSounds a Lot Like a Virtual Machine (VM)?\n\n\nThere are a number of similarities between virtual machines (VMs) and containers \u2013 in particular, they both allow you to create an image and spin up one or more instances, then safely work in isolation within each one. Containers, however, have a number of advantages which make them better suited to building and deploying applications.\n\n\nEach instance of a VM must contain an entire operating system, all required libraries, and of course the actual application binaries. All of that software consumes several Gigabytes of storage and memory. In contrast, each container holds its application and any dependencies, but the same Linux kernel and libraries can be shared between multiple containers running on the host. The fact that each container imposes minimal overhead on storage, RAM, and CPU means that many can run on the same host, and each takes just a couple of seconds to launch.\n\n\nRunning many containers allows each one to focus on a specific task; multiple containers then work in concert to implement sophisticated applications. In such microservice architectures, each container can use different versions of programming languages and libraries that can be upgraded independently.\n\n\nDue to the isolation of capabilities within containers, the effort and risk associated with updating any given container is far lower than with a more monolithic architecture.\n\n\nOrchestration\n\n\nClearly, the process of deploying multiple containers to implement an application can be optimized through automation. This becomes more and more valuable as the number of containers and hosts grow. This type of automation is referred to as orchestration. Orchestration can include a number of features, including:\n\n\n\n\nProvisioning hosts\n\n\nInstantiating a set of containers\n\n\nRescheduling failed containers\n\n\nLinking containers together through agreed interfaces\n\n\nExposing services to machines outside of the cluster\n\n\nScaling out or down the cluster by adding or removing containers", 
            "title": "Containers and Container Orchestration"
        }, 
        {
            "location": "/blog/container-orchestration/#containers-and-container-orchestration", 
            "text": "", 
            "title": "Containers and Container Orchestration"
        }, 
        {
            "location": "/blog/container-orchestration/#what-are-containers", 
            "text": "Shipping containers are efficiently moved using different modes of transport \u2013 perhaps initially being carried by a truck to a port, then neatly stacked alongside thousands of other shipping containers on a huge container ship that carries them to the other side of the world. At no point in the journey do the contents of that container need to repacked or modified in any way.  Shipping containers are ubiquitous, standardized, and available anywhere in the world, and they're extremely simple to use \u2013 just open them up, load in your cargo, and lock the doors shut.  The contents of each container are kept isolated from that of the others; the container full of Mentos can safely sit next to the container full of soda without any risk of a reaction. Once a spot on the container ship has been booked, you can be confident that there's room for all of your packed cargo for the whole trip \u2013 there's no way for a neighboring container to steal more than its share of space.   Software containers fulfill a similar role for your application. Packing the container involves defining what needs to be there for your application to work \u2013 libraries, configuration files, application binaries, and other parts of your technology stack. Once the container has been defined, that  image  is used to create containers that run in any environment, from the developer's laptop to your test/QA rig, to the production data center, on-premises or in the cloud, without any changes. This consistency can be very useful: for example, a support engineer can spin up a container to replicate an issue and be confident that it exactly matches what's running in the field.  Containers are very efficient and many of them can run on the same machine, allowing full use of all available resources. Linux containers and cgroups are used to make sure that there's no cross-contamination between containers: data files, libraries, ports, namespaces, and memory contents are all kept isolated. They also enforce upper boundaries on how much system resource (memory, storage, CPU, network bandwidth, and disk I/O) a container can consume so that a critical application isn't squeezed out by noisy neighbors.", 
            "title": "What are Containers?"
        }, 
        {
            "location": "/blog/container-orchestration/#sounds-a-lot-like-a-virtual-machine-vm", 
            "text": "There are a number of similarities between virtual machines (VMs) and containers \u2013 in particular, they both allow you to create an image and spin up one or more instances, then safely work in isolation within each one. Containers, however, have a number of advantages which make them better suited to building and deploying applications.  Each instance of a VM must contain an entire operating system, all required libraries, and of course the actual application binaries. All of that software consumes several Gigabytes of storage and memory. In contrast, each container holds its application and any dependencies, but the same Linux kernel and libraries can be shared between multiple containers running on the host. The fact that each container imposes minimal overhead on storage, RAM, and CPU means that many can run on the same host, and each takes just a couple of seconds to launch.  Running many containers allows each one to focus on a specific task; multiple containers then work in concert to implement sophisticated applications. In such microservice architectures, each container can use different versions of programming languages and libraries that can be upgraded independently.  Due to the isolation of capabilities within containers, the effort and risk associated with updating any given container is far lower than with a more monolithic architecture.", 
            "title": "Sounds a Lot Like a Virtual Machine (VM)?"
        }, 
        {
            "location": "/blog/container-orchestration/#orchestration", 
            "text": "Clearly, the process of deploying multiple containers to implement an application can be optimized through automation. This becomes more and more valuable as the number of containers and hosts grow. This type of automation is referred to as orchestration. Orchestration can include a number of features, including:   Provisioning hosts  Instantiating a set of containers  Rescheduling failed containers  Linking containers together through agreed interfaces  Exposing services to machines outside of the cluster  Scaling out or down the cluster by adding or removing containers", 
            "title": "Orchestration"
        }, 
        {
            "location": "/blog/container-orchestration-tools/", 
            "text": "Container Orchestration Tools\n\n\nContainer orchestration tools can be broadly defined as providing an enterprise-level framework for integrating and managing containers at scale. Such tools aim to simplify container management and provide a framework not only for defining initial container deployment but also for managing multiple containers as one entity -- for purposes of availability, scaling, and networking.\n\n\nSome container orchestration tools to know about include:\n\n\nAmazon ECS\n -- The Amazon EC2 Container Service (ECS) supports Docker containers and lets you run applications on a managed cluster of Amazon EC2 instances.\n\n\nAzure Container Service (ACS)\n -- ACS lets you create a cluster of virtual machines that act as container hosts along with master machines that are used to manage your application containers.\n\n\nCloud Foundry\u2019s Diego\n -- Diego is a container management system that combines a scheduler, runner, and health manager. It is a rewrite of the Cloud Foundry runtime.\n\n\nCoreOS Fleet\n -- Fleet is a container management tool that lets you deploy Docker containers on hosts in a cluster as well as distribute services across a cluster.\n\n\nDocker Swarm\n -- Docker Swarm provides native clustering functionality for Docker containers, which lets you turn a group of Docker engines into a single, virtual Docker engine.\n\n\nGoogle Container Engine\n -- Google Container Engine, which is built on Kubernetes, lets you run Docker containers on the Google Cloud platform. It schedules containers into the cluster and manages them based on user-defined requirements.\n\n\nKubernetes\n -- Kubernetes is an orchestration system for Docker containers. It handles scheduling and manages workloads based on user-defined parameters.\n\n\nMesosphere Marathon\n -- Marathon is a container orchestration framework for Apache Mesos that is designed to launch long-running applications. It offers key features for running applications in a clustered environment.", 
            "title": "Container Orchestration Tools"
        }, 
        {
            "location": "/blog/container-orchestration-tools/#container-orchestration-tools", 
            "text": "Container orchestration tools can be broadly defined as providing an enterprise-level framework for integrating and managing containers at scale. Such tools aim to simplify container management and provide a framework not only for defining initial container deployment but also for managing multiple containers as one entity -- for purposes of availability, scaling, and networking.  Some container orchestration tools to know about include:  Amazon ECS  -- The Amazon EC2 Container Service (ECS) supports Docker containers and lets you run applications on a managed cluster of Amazon EC2 instances.  Azure Container Service (ACS)  -- ACS lets you create a cluster of virtual machines that act as container hosts along with master machines that are used to manage your application containers.  Cloud Foundry\u2019s Diego  -- Diego is a container management system that combines a scheduler, runner, and health manager. It is a rewrite of the Cloud Foundry runtime.  CoreOS Fleet  -- Fleet is a container management tool that lets you deploy Docker containers on hosts in a cluster as well as distribute services across a cluster.  Docker Swarm  -- Docker Swarm provides native clustering functionality for Docker containers, which lets you turn a group of Docker engines into a single, virtual Docker engine.  Google Container Engine  -- Google Container Engine, which is built on Kubernetes, lets you run Docker containers on the Google Cloud platform. It schedules containers into the cluster and manages them based on user-defined requirements.  Kubernetes  -- Kubernetes is an orchestration system for Docker containers. It handles scheduling and manages workloads based on user-defined parameters.  Mesosphere Marathon  -- Marathon is a container orchestration framework for Apache Mesos that is designed to launch long-running applications. It offers key features for running applications in a clustered environment.", 
            "title": "Container Orchestration Tools"
        }, 
        {
            "location": "/blog/internet-of-things/", 
            "text": "Internet Of Things\n\n\nBroadband Internet is become more widely available, the cost of connecting is decreasing, more devices are being created with Wi-Fi capabilities and sensors built into them, technology costs are going down, and smartphone penetration is sky-rocketing.  All of these things are creating a \"perfect storm\" for the IoT.\n\n\nThe Internet of things (IoT) is the network of physical devices, vehicles, home appliances and other items embedded with electronics, software, sensors, actuators, and connectivity which enables these objects to connect and exchange data. Each thing is uniquely identifiable through its embedded computing system but is able to inter-operate within the existing Internet infrastructure.\n\n\nSimply put, this is the concept of basically connecting any device with an on and off switch to the Internet (and/or to each other). This includes everything from cellphones, coffee makers, washing machines, headphones, lamps, wearable devices and almost anything else you can think of.  This also applies to components of machines, for example a jet engine of an airplane or the drill of an oil rig. As I mentioned, if it has an on and off switch then chances are it can be a part of the IoT.\n\n\nThe analyst firm Gartner says that by 2020 there will be over 26 billion connected devices... That's a lot of connections (some even estimate this number to be much higher, over 100 billion).  The IoT is a giant network of connected \"things\" (which also includes people).  The relationship will be between people-people, people-things, and things-things.", 
            "title": "Internet Of Things"
        }, 
        {
            "location": "/blog/internet-of-things/#internet-of-things", 
            "text": "Broadband Internet is become more widely available, the cost of connecting is decreasing, more devices are being created with Wi-Fi capabilities and sensors built into them, technology costs are going down, and smartphone penetration is sky-rocketing.  All of these things are creating a \"perfect storm\" for the IoT.  The Internet of things (IoT) is the network of physical devices, vehicles, home appliances and other items embedded with electronics, software, sensors, actuators, and connectivity which enables these objects to connect and exchange data. Each thing is uniquely identifiable through its embedded computing system but is able to inter-operate within the existing Internet infrastructure.  Simply put, this is the concept of basically connecting any device with an on and off switch to the Internet (and/or to each other). This includes everything from cellphones, coffee makers, washing machines, headphones, lamps, wearable devices and almost anything else you can think of.  This also applies to components of machines, for example a jet engine of an airplane or the drill of an oil rig. As I mentioned, if it has an on and off switch then chances are it can be a part of the IoT.  The analyst firm Gartner says that by 2020 there will be over 26 billion connected devices... That's a lot of connections (some even estimate this number to be much higher, over 100 billion).  The IoT is a giant network of connected \"things\" (which also includes people).  The relationship will be between people-people, people-things, and things-things.", 
            "title": "Internet Of Things"
        }, 
        {
            "location": "/blog/understanding-iaas-paas-saas/", 
            "text": "Understanding IAAS, PAAS and SAAS\n\n\nInfrastructure As A Service\n\n\nInfrastructure as a service (IaaS) is an instant computing infrastructure, provisioned and managed over the Internet. Quickly scale up and down with demand, and pay only for what you use.\n\n\nIaaS helps you avoid the expense and complexity of buying and managing your own physical servers and other datacenter infrastructure. Each resource is offered as a separate service component, and you only need to rent a particular one for as long as you need it. The cloud computing service provider manages the infrastructure, while you purchase, install, configure, and manage your own software\u2014operating systems, middleware, and applications.\n\n\n\n\nPlatform As A Service\n\n\nPlatform as a service (PaaS) is a complete development and deployment environment in the cloud, with resources that enable you to deliver everything from simple cloud-based apps to sophisticated, cloud-enabled enterprise applications. You purchase the resources you need from a cloud service provider on a pay-as-you-go basis and access them over a secure Internet connection.\n\n\nLike IaaS, PaaS includes infrastructure\u2014servers, storage, and networking\u2014but also middleware, development tools, business intelligence (BI) services, database management systems, and more. PaaS is designed to support the complete web application lifecycle: building, testing, deploying, managing, and updating.\n\n\nPaaS allows you to avoid the expense and complexity of buying and managing software licenses, the underlying application infrastructure and middleware or the development tools and other resources. You manage the applications and services you develop, and the cloud service provider typically manages everything else.\n\n\nSoftware As A Service\n\n\nSoftware as a service (SaaS) allows users to connect to and use cloud-based apps over the Internet. Common examples are email, calendaring, and office tools (such as Microsoft Office 365).\n\n\nSaaS provides a complete software solution that you purchase on a pay-as-you-go basis from a cloud service provider. You rent the use of an app for your organization, and your users connect to it over the Internet, usually with a web browser. All of the underlying infrastructure, middleware, app software, and app data are located in the service provider\u2019s data center. The service provider manages the hardware and software, and with the appropriate service agreement, will ensure the availability and the security of the app and your data as well. SaaS allows your organization to get quickly up and running with an app at minimal upfront cost.\n\n\n\n\nReferences\n\n\n\n\nMicrosoft Azure documentation\n\n\nIBM cloud learn", 
            "title": "IAAS, PAAS and SAAS"
        }, 
        {
            "location": "/blog/understanding-iaas-paas-saas/#understanding-iaas-paas-and-saas", 
            "text": "", 
            "title": "Understanding IAAS, PAAS and SAAS"
        }, 
        {
            "location": "/blog/understanding-iaas-paas-saas/#infrastructure-as-a-service", 
            "text": "Infrastructure as a service (IaaS) is an instant computing infrastructure, provisioned and managed over the Internet. Quickly scale up and down with demand, and pay only for what you use.  IaaS helps you avoid the expense and complexity of buying and managing your own physical servers and other datacenter infrastructure. Each resource is offered as a separate service component, and you only need to rent a particular one for as long as you need it. The cloud computing service provider manages the infrastructure, while you purchase, install, configure, and manage your own software\u2014operating systems, middleware, and applications.", 
            "title": "Infrastructure As A Service"
        }, 
        {
            "location": "/blog/understanding-iaas-paas-saas/#platform-as-a-service", 
            "text": "Platform as a service (PaaS) is a complete development and deployment environment in the cloud, with resources that enable you to deliver everything from simple cloud-based apps to sophisticated, cloud-enabled enterprise applications. You purchase the resources you need from a cloud service provider on a pay-as-you-go basis and access them over a secure Internet connection.  Like IaaS, PaaS includes infrastructure\u2014servers, storage, and networking\u2014but also middleware, development tools, business intelligence (BI) services, database management systems, and more. PaaS is designed to support the complete web application lifecycle: building, testing, deploying, managing, and updating.  PaaS allows you to avoid the expense and complexity of buying and managing software licenses, the underlying application infrastructure and middleware or the development tools and other resources. You manage the applications and services you develop, and the cloud service provider typically manages everything else.", 
            "title": "Platform As A Service"
        }, 
        {
            "location": "/blog/understanding-iaas-paas-saas/#software-as-a-service", 
            "text": "Software as a service (SaaS) allows users to connect to and use cloud-based apps over the Internet. Common examples are email, calendaring, and office tools (such as Microsoft Office 365).  SaaS provides a complete software solution that you purchase on a pay-as-you-go basis from a cloud service provider. You rent the use of an app for your organization, and your users connect to it over the Internet, usually with a web browser. All of the underlying infrastructure, middleware, app software, and app data are located in the service provider\u2019s data center. The service provider manages the hardware and software, and with the appropriate service agreement, will ensure the availability and the security of the app and your data as well. SaaS allows your organization to get quickly up and running with an app at minimal upfront cost.", 
            "title": "Software As A Service"
        }, 
        {
            "location": "/blog/understanding-iaas-paas-saas/#references", 
            "text": "Microsoft Azure documentation  IBM cloud learn", 
            "title": "References"
        }, 
        {
            "location": "/tools/openldap/", 
            "text": "OpenLDAP\n\n\nOpenLDAP is a opensource implementation of Lightweight Directory Access Protocol (LDAP).\n\n\nTable of contents\n\n\n\n\nInstallation\n\n\nConfiguration\n\n\nFAQ's\n\n\n\n\nReferences\n\n\n\n\nOpenldap\n\n\nZytrax\n\n\nInstalling BDB on Linux - The Ecommerce Blog", 
            "title": "Introduction"
        }, 
        {
            "location": "/tools/openldap/#openldap", 
            "text": "OpenLDAP is a opensource implementation of Lightweight Directory Access Protocol (LDAP).", 
            "title": "OpenLDAP"
        }, 
        {
            "location": "/tools/openldap/#table-of-contents", 
            "text": "Installation  Configuration  FAQ's", 
            "title": "Table of contents"
        }, 
        {
            "location": "/tools/openldap/#references", 
            "text": "Openldap  Zytrax  Installing BDB on Linux - The Ecommerce Blog", 
            "title": "References"
        }, 
        {
            "location": "/tools/openldap/install/", 
            "text": "Installing OpenLDAP server\n\n\nOpenLDAP has a dependency on Oracle's Berkeley DB hence we need to first install berkeley db.\n\n\nPrerequisite\n\n\nMake sure gcc, gcc-c++, make and other required packages are available on the server.\n\n\nRun the below command : to confirm and if not available install them.\n\n\n1\n2\nyum update -y \n\\\n\nyum install unzip gcc gcc-c++ make -y groff\n\n\n\n\n\n\nInstalling Berkeley Db\n\n\nCopy paste the below code into a shell script and execute it. Change the BDB_VERSION variable if you want to install a different version.\n\n\n1\n2\n3\n4\n5\n6\n7\nBDB_VERSION\n=\n4.8.30\n\ncurl -o db-\n${\nBDB_VERSION\n}\n.zip -fSL http://download.oracle.com/berkeley-db/db-\n${\nBDB_VERSION\n}\n.zip \n\\\n\nunzip db-\n${\nBDB_VERSION\n}\n.zip \n\\\n\n\ncd\n ./db-\n${\nBDB_VERSION\n}\n/build_unix \n\\\n\n../dist/configure --enable-cxx \n\\\n\nmake \n\\\n\nmake install\n\n\n\n\n\n\nInstalling OpenLDAP\n\n\nExport below env variable\n\n\n1\n2\n3\nexport\n \nCPPFLAGS\n=\n-I/usr/local/BerkeleyDB.4.8/include\n\n\nexport\n \nLDFLAGS\n=\n-L/usr/local/BerkeleyDB.4.8/lib\n\n\nexport\n \nLD_LIBRARY_PATH\n=\n/usr/local/BerkeleyDB.4.8/lib/\n\n\n\n\n\n\n\nCopy paste the below code into a shell script and execute it. Change the INSTALL_PATH if you wish to install in another path and OPENLDAP_VERSION if you want to install a different version. Make sure that the OpenLDAP version and the Berkeley Db version are compatible.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\nINSTALL_PATH\n=\n/opt/openldap\n\n\nOPENLDAP_VERSION\n=\n2.4.45\n\n\ncurl -o openldap-\n${\nOPENLDAP_VERSION\n}\n.tgz -fSL https://www.openldap.org/software/download/OpenLDAP/openldap-release/openldap-\n${\nOPENLDAP_VERSION\n}\n.tgz \n\\\n\ntar -xzvf openldap-\n${\nOPENLDAP_VERSION\n}\n.tgz \n\\\n\nls -lrt \n\\\n\n./openldap-\n${\nOPENLDAP_VERSION\n}\n/configure --prefix\n=\n${\nINSTALL_PATH\n}\n \n\\\n\n    --enable-slapd \n\\\n\n    --enable-bdb \n\\\n\n    --enable-overlays \n\\\n\n    --enable-syslog \n\\\n\n    --enable-accesslog \n\\\n\n    --enable-auditlog \n\\\n\n    --enable-dynlist \n\\\n\n    --enable-ppolicy \n\\\n\n    --enable-memberof \n\\\n\n    --enable-constraint \n\\\n\n    --enable-debug \n\\\n\nmake depend \n\\\n\nmake \n\\\n\nmake install", 
            "title": "Installation"
        }, 
        {
            "location": "/tools/openldap/install/#installing-openldap-server", 
            "text": "OpenLDAP has a dependency on Oracle's Berkeley DB hence we need to first install berkeley db.", 
            "title": "Installing OpenLDAP server"
        }, 
        {
            "location": "/tools/openldap/install/#prerequisite", 
            "text": "Make sure gcc, gcc-c++, make and other required packages are available on the server.  Run the below command : to confirm and if not available install them.  1\n2 yum update -y  \\ \nyum install unzip gcc gcc-c++ make -y groff", 
            "title": "Prerequisite"
        }, 
        {
            "location": "/tools/openldap/install/#installing-berkeley-db", 
            "text": "Copy paste the below code into a shell script and execute it. Change the BDB_VERSION variable if you want to install a different version.  1\n2\n3\n4\n5\n6\n7 BDB_VERSION = 4.8.30 \ncurl -o db- ${ BDB_VERSION } .zip -fSL http://download.oracle.com/berkeley-db/db- ${ BDB_VERSION } .zip  \\ \nunzip db- ${ BDB_VERSION } .zip  \\  cd  ./db- ${ BDB_VERSION } /build_unix  \\ \n../dist/configure --enable-cxx  \\ \nmake  \\ \nmake install", 
            "title": "Installing Berkeley Db"
        }, 
        {
            "location": "/tools/openldap/install/#installing-openldap", 
            "text": "Export below env variable  1\n2\n3 export   CPPFLAGS = -I/usr/local/BerkeleyDB.4.8/include  export   LDFLAGS = -L/usr/local/BerkeleyDB.4.8/lib  export   LD_LIBRARY_PATH = /usr/local/BerkeleyDB.4.8/lib/    Copy paste the below code into a shell script and execute it. Change the INSTALL_PATH if you wish to install in another path and OPENLDAP_VERSION if you want to install a different version. Make sure that the OpenLDAP version and the Berkeley Db version are compatible.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21 INSTALL_PATH = /opt/openldap  OPENLDAP_VERSION = 2.4.45 \n\ncurl -o openldap- ${ OPENLDAP_VERSION } .tgz -fSL https://www.openldap.org/software/download/OpenLDAP/openldap-release/openldap- ${ OPENLDAP_VERSION } .tgz  \\ \ntar -xzvf openldap- ${ OPENLDAP_VERSION } .tgz  \\ \nls -lrt  \\ \n./openldap- ${ OPENLDAP_VERSION } /configure --prefix = ${ INSTALL_PATH }   \\ \n    --enable-slapd  \\ \n    --enable-bdb  \\ \n    --enable-overlays  \\ \n    --enable-syslog  \\ \n    --enable-accesslog  \\ \n    --enable-auditlog  \\ \n    --enable-dynlist  \\ \n    --enable-ppolicy  \\ \n    --enable-memberof  \\ \n    --enable-constraint  \\ \n    --enable-debug  \\ \nmake depend  \\ \nmake  \\ \nmake install", 
            "title": "Installing OpenLDAP"
        }, 
        {
            "location": "/tools/openldap/config/", 
            "text": "Configuring OpenLDAP server\n\n\nslapd.conf file\n\n\nslapd.conf file holds all the configuration required for running the openLDAP server. Below is a example of the slapd.conf file.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n###### PrivateSquare.in slapd config file ############\n\n\n#\n\n\n# NOTES: inetorgperson picks up attributes and objectclasses\n\n\n#        from all three schemas\n\n\n#\n\ninclude         /opt/openldap/etc/openldap/schema/core.schema\ninclude         /opt/openldap/etc/openldap/schema/cosine.schema\ninclude         /opt/openldap/etc/openldap/schema/inetorgperson.schema\ninclude         /opt/openldap/etc/openldap/schema/dyngroup.schema\n\n\n# NO SECURITY - no access clause\n\n\n# defaults to anonymous access for read\n\n\n# only rootdn can write\n\n\n\n#access to *\n\n\n# by anonymous none\n\n\n# by * write\n\n\n\n# NO REFERRALS\n\n\n\n# DON\nT bother with ARGS file unless you feel strongly\n\n\n# slapd scripts stop scripts need this to work\n\npidfile     /opt/openldap/run/slapd.pid\nargsfile    /opt/openldap/run/slapd.args\n\n\n# enable a lot of logging - we might need it\n\n\n# but generates huge logs\n\nloglevel        -1\n\n\n# NO TLS-enabled connections\n\n\n\n####################################################################\n\n\n### Create configuration DIT in OpenLdap\n\n\n###\n\n\n### NOTE: the suffix is hardcoded as cn=config and\n\n\n### MUST not have a suffix directive\n\n\n### normal rules apply - rootdn can be anything you want\n\n\n### but MUST be under cn=config\n\n\n#######################################################################\n\ndatabase config\nrootdn \ncn=root,cn=config\n\nrootpw \n{\nSSHA\n}\nGT4+O2DLvYfJTqAM7VFIGCiY+Q+fGcgr\n\n\n# Private Square database -----------------------------\n\n\ndatabase bdb\nsuffix \ndc=privatesquare,dc=in\n\n\noverlay dynlist\ndynlist-attrset groupOfURLs memberURL owner\n\n\n# root or superuser\n\nrootdn \ncn=root,dc=privatesquare,dc=in\n\nrootpw \n{\nSSHA\n}\nGT4+O2DLvYfJTqAM7VFIGCiY+Q+fGcgr\n\n# # The database directory MUST exist prior to running slapd AND\n\n\n# # change path as necessary\n\ndirectory       /data/openldap\n\nindex   objectClass     eq\nindex   uid     eq\nindex   cn,gn,mail eq,sub\nindex sn eq,sub\nindex ou eq\n\ncachesize \n10000\n\ncheckpoint \n128\n \n15\n\n\n\n\n\n\n\n/opt/openldap is the install path of openLDAP and /data/openldap is the data path for the private square db.\n\n\nBuilding the Config Db\n\n\nBelow command builds the config db based on the entries in the slapd.conf file. The config db will be built into the path ${INSTALL_PATH}/etc/openldap/slapd.d\n\n\n1\n2\ncd\n \n${\nINSTALL_PATH\n}\n\n./sbin/slaptest -f ./etc/openldap/slapd.conf -F ./etc/openldap/slapd.d -u\n\n\n\n\n\n\nBuilding the Data Db\n\n\nFor building the data db you need to import the initial LDAP structure you prefer.\n\n\nThis is the entry I need for the private squate db.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n# Initial root entry for privatesquare.in\n\ndn: dc=privatesquare,dc=in\nobjectClass: top\nobjectClass: domain\ndc: privatesquare\n\ndn: ou=users,dc=privatesquare,dc=in\nobjectClass: top\nobjectClass: organizationalUnit\nou: users\n\ndn: ou=groups,dc=privatesquare,dc=in\nobjectClass: top\nobjectClass: organizationalUnit\nou: groups\n\n\n\n\n\n\ntouch the contents of the ldif into a initial.ldif file and run the below command to build the data db.\n\n\n1\n2\ncd\n \n${\nINSTALL_PATH\n}\n\n./sbin/slapadd -q -l \n[\nPath to the initail ldif file\n]\n -f ./etc/openldap/slapd.conf\n\n\n\n\n\n\nTesting the configuration\n\n\nTest the configuration by running the below command.\n\n\n1\n2\ncd\n \n${\nINSTALL_PATH\n}\n\n./sbin/slaptest -f ./etc/openldap/slapd.conf -F ./etc/openldap/slapd.d\n\n\n\n\n\n\nStart OpenLDAP\n\n\n1\n2\ncd\n \n${\nINSTALL_PATH\n}\n\n./libexec/slapd -h \nldap://hostname:port\n  -f ./etc/openldap/slapd.conf", 
            "title": "Configuration"
        }, 
        {
            "location": "/tools/openldap/config/#configuring-openldap-server", 
            "text": "", 
            "title": "Configuring OpenLDAP server"
        }, 
        {
            "location": "/tools/openldap/config/#slapdconf-file", 
            "text": "slapd.conf file holds all the configuration required for running the openLDAP server. Below is a example of the slapd.conf file.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66 ###### PrivateSquare.in slapd config file ############  #  # NOTES: inetorgperson picks up attributes and objectclasses  #        from all three schemas  # \ninclude         /opt/openldap/etc/openldap/schema/core.schema\ninclude         /opt/openldap/etc/openldap/schema/cosine.schema\ninclude         /opt/openldap/etc/openldap/schema/inetorgperson.schema\ninclude         /opt/openldap/etc/openldap/schema/dyngroup.schema # NO SECURITY - no access clause  # defaults to anonymous access for read  # only rootdn can write  #access to *  # by anonymous none  # by * write  # NO REFERRALS  # DON T bother with ARGS file unless you feel strongly  # slapd scripts stop scripts need this to work \npidfile     /opt/openldap/run/slapd.pid\nargsfile    /opt/openldap/run/slapd.args # enable a lot of logging - we might need it  # but generates huge logs \nloglevel        -1 # NO TLS-enabled connections  ####################################################################  ### Create configuration DIT in OpenLdap  ###  ### NOTE: the suffix is hardcoded as cn=config and  ### MUST not have a suffix directive  ### normal rules apply - rootdn can be anything you want  ### but MUST be under cn=config  ####################################################################### \ndatabase config\nrootdn  cn=root,cn=config \nrootpw  { SSHA } GT4+O2DLvYfJTqAM7VFIGCiY+Q+fGcgr # Private Square database ----------------------------- \n\ndatabase bdb\nsuffix  dc=privatesquare,dc=in \n\noverlay dynlist\ndynlist-attrset groupOfURLs memberURL owner # root or superuser \nrootdn  cn=root,dc=privatesquare,dc=in \nrootpw  { SSHA } GT4+O2DLvYfJTqAM7VFIGCiY+Q+fGcgr # # The database directory MUST exist prior to running slapd AND  # # change path as necessary \ndirectory       /data/openldap\n\nindex   objectClass     eq\nindex   uid     eq\nindex   cn,gn,mail eq,sub\nindex sn eq,sub\nindex ou eq\n\ncachesize  10000 \ncheckpoint  128   15    /opt/openldap is the install path of openLDAP and /data/openldap is the data path for the private square db.", 
            "title": "slapd.conf file"
        }, 
        {
            "location": "/tools/openldap/config/#building-the-config-db", 
            "text": "Below command builds the config db based on the entries in the slapd.conf file. The config db will be built into the path ${INSTALL_PATH}/etc/openldap/slapd.d  1\n2 cd   ${ INSTALL_PATH } \n./sbin/slaptest -f ./etc/openldap/slapd.conf -F ./etc/openldap/slapd.d -u", 
            "title": "Building the Config Db"
        }, 
        {
            "location": "/tools/openldap/config/#building-the-data-db", 
            "text": "For building the data db you need to import the initial LDAP structure you prefer.  This is the entry I need for the private squate db.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16 # Initial root entry for privatesquare.in\n\ndn: dc=privatesquare,dc=in\nobjectClass: top\nobjectClass: domain\ndc: privatesquare\n\ndn: ou=users,dc=privatesquare,dc=in\nobjectClass: top\nobjectClass: organizationalUnit\nou: users\n\ndn: ou=groups,dc=privatesquare,dc=in\nobjectClass: top\nobjectClass: organizationalUnit\nou: groups   touch the contents of the ldif into a initial.ldif file and run the below command to build the data db.  1\n2 cd   ${ INSTALL_PATH } \n./sbin/slapadd -q -l  [ Path to the initail ldif file ]  -f ./etc/openldap/slapd.conf", 
            "title": "Building the Data Db"
        }, 
        {
            "location": "/tools/openldap/config/#testing-the-configuration", 
            "text": "Test the configuration by running the below command.  1\n2 cd   ${ INSTALL_PATH } \n./sbin/slaptest -f ./etc/openldap/slapd.conf -F ./etc/openldap/slapd.d", 
            "title": "Testing the configuration"
        }, 
        {
            "location": "/tools/openldap/config/#start-openldap", 
            "text": "1\n2 cd   ${ INSTALL_PATH } \n./libexec/slapd -h  ldap://hostname:port   -f ./etc/openldap/slapd.conf", 
            "title": "Start OpenLDAP"
        }, 
        {
            "location": "/tools/openldap/ldap-server-error-codes/", 
            "text": "Server-Side Error Codes\n\n\nVarious LDAP specifications define a number of common result codes that may be included in responses to clients. These result codes include (but are not necessarily limited to):\n\n\n0: Success\n\n\nThis indicates that the operation completed successfully. It may be returned in response to an add, bind, delete, extended, modify, modify DN, or search operations.\n\n\nCompare operations will not return a success result. If a compare operation does not encounter an error during processing, then the server should return a result of either \"compare true\" or \"compare false\", based on whether the target entry had the specified attribute value.\n\n\n1: Operations Error\n\n\nThis is intended to indicate that the client has requested an operation at an inappropriate time or in an incorrect order. For example, it may be used if a client sends a non-bind request in the middle of a multi-stage bind operation.\n\n\nNote that some directory servers use this as a generic \"server error\" type result. This is not the intended use for this result code (the \"other\" result is a better choice for this), but clients may need to be aware of this possibility.\n\n\n2: Protocol Error\n\n\nThis generally indicates that the client request was improperly formatted in some way. For a bind operation, it may indicate that the client attempted to use an unsupported LDAP protocol version. For an extended operation, it may indicate that the server does not support the extended request type.\n\n\nNote that this result code can only be used if the server is able to at least partially decode the request in order to determine the message ID and operation type, since the server needs that information in order to craft an appropriate response.\n\n\n3: Time Limit Exceeded\n\n\nThis indicates that a search operation took longer to complete than allowed by the maximum time limit for that operation. This may be the time limit specified by the client in the search request, or it may be a time limit imposed by the server.\n\n\n4: Size Limit Exceeded\n\n\nThis indicates that a search operation would have returned more entries than were allowed for that operation. This may be the size limit specified by the client in the search request, or it may be a size limit imposed by the server. Note that the server may return a portion of the matching entries before this result.\n\n\n5: Compare False\n\n\nThis indicates that a compare operation was processed successfully but that the target entry did not match the provided attribute value assertion.\n\n\n6: Compare True\n\n\nThis indicates that a compare operation was processed successfully and that the target entry matched the provided attribute value assertion.\n\n\n7: Authentication Method Not Supported\n\n\nThis indicates that a bind operation failed because the server does not support the requested authentication type.\n\n\n8: Stronger Authentication Required\n\n\nThis indicates that the server requires that the client be authenticated with a stronger mechanism than has previously been performed in order to process the request.\n\n\nThis result code may be used in a notice of disconnection unsolicited notification if the server believes that the security of the connection has been compromised.\n\n\n10: Referral\n\n\nThis indicates that the server could not process the requested operation, but that it may succeed if attempted in another location, as specified by the referral URIs included in the response.\n\n\n11: Administrative Limit Exceeded\n\n\nThis indicates that an administrative limit was exceeded while processing the request. For example, some directory servers use this response to indicate that it would have required examining too many entries to process the request.\n\n\n12: Unavailable Critical Extension\n\n\nThis indicates that the request included a critical control that was not recognized or could not be processed.\n\n\n13: Confidentiality Required\n\n\nThis indicates that the requested operation is only allowed over a secure connection.\n\n\n14: SASL Bind in Progress\n\n\nThis indicates that the server requires more information from the client in order to complete the SASL bind operation. In such responses, the \"server SASL credentials\" element of the result message will often include information the client needs for subsequent phases of bind processing.\n\n\n16: No Such Attribute\n\n\nThis indicates that the client attempted to interact with an attribute that does not exist in the target entry (e.g., to remove an attribute or value that does not exist).\n\n\n17: Undefined Attribute Type\n\n\nThis indicates that the client request included an attribute type that is not defined in the server schema.\n\n\n18: Inappropriate Matching\n\n\nThis indicates that the client request attempted to perform an unsupported type of matching against an attribute. For example, this may be used if the attribute type does not have an appropriate matching rule for the type of matching requested for that attribute.\n\n\n19: Constraint Violation\n\n\nThis indicates that the client request would have caused the server to violate some constraint defined in the server (e.g., to add more than one value to a single-valued attribute).\n\n\n20: Attribute or Value Exists\n\n\nThis indicates that the client request attempted to add an attribute or value to an entry that already contained that attribute or value.\n\n\n21: Invalid Attribute Syntax\n\n\nThis indicates that the client request would have resulted in an attribute value that did not conform to the syntax for that attribute type.\n\n\n32: No Such Object\n\n\nThis indicates that the client request targeted an entry that does not exist. Note that some servers use this result for a bind request that targets a nonexistent user, even though \"invalid credentials\" is a more appropriate result for that case.\n\n\n33: Alias Problem\n\n\nThis indicates that a problem was encountered while interacting with an alias entry (e.g., the alias refers to an entry that does not exist).\n\n\n34: Invalid DN Syntax\n\n\nThis indicates that the request included a malformed or invalid DN or RDN.\n\n\n36: Alias Dereferencing Problem\n\n\nThis indicates that the client attempted to interact with an alias entry in a manner that was not allowed (e.g., for an operation that does not allow the use of aliases, or if the client does not have permission to access the entry referenced by the alias).\n\n\n48: Inappropriate Authentication\n\n\nThis indicates that the client attempted to perform a type of authentication that is not allowed for the target user (e.g., because the user does not have the necessary credentials for that type of authentication). This may also indicate that the client attempted to perform anonymous authentication when that is not allowed.\n\n\n49: Invalid Credentials\n\n\nThis indicates that the client attempted to bind as a user that does not exist, attempted to bind as a user that is not allowed to bind (e.g., because it has expired, because it has been locked because of too many failed authentication attempts, etc.), or attempted to bind with credentials that were not correct for the target user.\n\n\n50: Insufficient Access Rights\n\n\nThis indicates that the client does not have permission to perform the requested operation.\n\n\n51: Busy\n\n\nThis indicates that the server is currently too busy to process the requested operation.\n\n\n52: Unavailable\n\n\nThis indicates that the server is currently unavailable (e.g., because it is being shut down or is in a maintenance state).\n\n\n53: Unwilling to Perform\n\n\nThis indicates that the server is unwilling to process the requested operation for some reason.\n\n\n54: Loop Detected\n\n\nThis indicates that the server has detected an internal loop while processing the requested operation (e.g., if two alias entries reference each other).\n\n\n60: Sort Control Missing\n\n\nThis indicates that the search request included a virtual list view request control without also including the required server-side sort request control.\n\n\n61: Offset Range Error\n\n\nThis indicates that the search request included a virtual list view request control with an invalid offset or content count.\n\n\n64: Naming Violation\n\n\nThis indicates that the requested add or modify DN operation would have resulted in an entry that violates server naming restrictions (e.g., as might be imposed by a name form defined in the server schema).\n\n\n65: Object Class Violation\n\n\nThis indicates that the requested operation would have resulted in an entry that violates schema restrictions imposed by its object classes (e.g., to include an attribute that is not allowed to be present in the entry, or to omit an attribute that is required to be present in the entry).\n\n\n66: Not Allowed on Non-Leaf Entry\n\n\nThis indicates that the requested operation cannot be performed against an entry that has one or more subordinate entries. For example, a delete operation is normally not allowed to remove an entry that has one or more subordinates.\n\n\n67: Not Allowed on RDN\n\n\nThis indicates that the requested operation is not allowed because it would have altered the entry to remove an attribute value used in the entry's RDN.\n\n\n68: Entry Already Exists\n\n\nThis indicates that the requested add or modify DN operation could not be processed because another entry already exists with the DN that would have resulted from that operation.\n\n\n69: Object Class Modifications Prohibited\n\n\nThis indicates that the requested operation would have resulted in a disallowed change to the object classes contained in the entry (e.g., the operation would have changed the entry's structural object class).\n\n\n71: Affects Multiple DSAs\n\n\nThis indicates that the requested operation could not be processed because it would have required interacting with data in multiple directory server instances in a way that is not supported.\n\n\n76: Virtual List View Error\n\n\nThis indicates that a search operation failed because of processing associated with a virtual list view request control included in the request (e.g., if a necessary index was not in place to facilitate the virtual list view processing).\n\n\n80: Other\n\n\nThis indicates that some problem was encountered during processing that is not covered by any of the other defined result codes (e.g., a server error).\n\n\n118: Canceled\n\n\nThis indicates that an operation was canceled through the use of the cancel extended request. If an operation is canceled in this way, then this result code will be used for both the operation that was canceled and for the cancel extended operation itself.\n\n\n119: No Such Operation\n\n\nThis indicates that an attempt to cancel an operation via the cancel extended request was not successful because the server did not have any knowledge of the target operation. This often means that the server had already completed processing for the operation by the time it received and attempted to process the cancel request.\n\n\n120: Too Late\n\n\nThis indicates that an attempt to cancel an operation via the cancel extended request was not successful because processing for that operation had already reached a point beyond which it could be canceled.\n\n\n121: Cannot Cancel\n\n\nThis indicates that an attempt to cancel an operation via the cancel extended request was not successful because the target operation is not an operation that can be canceled. Operations that cannot be canceled include abandon, bind, unbind, and the cancel and StartTLS extended operations.\n\n\n122: Assertion Failed\n\n\nThis indicates that the requested operation could not be processed because the request included an LDAP assertion request control, but the assertion filter did not match the target entry.\n\n\n123: Authorization Denied\n\n\nThis indicates that the requested operation could not be processed because the request included a proxied authorization request control (or some similar control intended to specify an alternate authorization identity for the operation), but the client was not allowed to request the use of that alternate authorization identity.\n\n\n4096: Synchronization Refresh Required\n\n\nThis indicates that an attempt to use the content synchronization request control in order to perform an incremental update failed for some reason and the client will instead need to request an initial content.\n\n\n16654: No Operation\n\n\nThis indicates that the associated operation would likely have succeeded, to the extent that the server was able to make the determination, but was not actually processed because the request included the LDAP no operation control. Note that at present, the numeric value for this result code is not an official standard because the specification for the no operation request control has not progressed far enough to be assigned an official numeric value, but the value 16654 is in common use by a number of servers for this purpose.", 
            "title": "Server Error Codes"
        }, 
        {
            "location": "/tools/openldap/ldap-server-error-codes/#server-side-error-codes", 
            "text": "Various LDAP specifications define a number of common result codes that may be included in responses to clients. These result codes include (but are not necessarily limited to):", 
            "title": "Server-Side Error Codes"
        }, 
        {
            "location": "/tools/openldap/ldap-server-error-codes/#0-success", 
            "text": "This indicates that the operation completed successfully. It may be returned in response to an add, bind, delete, extended, modify, modify DN, or search operations.  Compare operations will not return a success result. If a compare operation does not encounter an error during processing, then the server should return a result of either \"compare true\" or \"compare false\", based on whether the target entry had the specified attribute value.", 
            "title": "0: Success"
        }, 
        {
            "location": "/tools/openldap/ldap-server-error-codes/#1-operations-error", 
            "text": "This is intended to indicate that the client has requested an operation at an inappropriate time or in an incorrect order. For example, it may be used if a client sends a non-bind request in the middle of a multi-stage bind operation.  Note that some directory servers use this as a generic \"server error\" type result. This is not the intended use for this result code (the \"other\" result is a better choice for this), but clients may need to be aware of this possibility.", 
            "title": "1: Operations Error"
        }, 
        {
            "location": "/tools/openldap/ldap-server-error-codes/#2-protocol-error", 
            "text": "This generally indicates that the client request was improperly formatted in some way. For a bind operation, it may indicate that the client attempted to use an unsupported LDAP protocol version. For an extended operation, it may indicate that the server does not support the extended request type.  Note that this result code can only be used if the server is able to at least partially decode the request in order to determine the message ID and operation type, since the server needs that information in order to craft an appropriate response.", 
            "title": "2: Protocol Error"
        }, 
        {
            "location": "/tools/openldap/ldap-server-error-codes/#3-time-limit-exceeded", 
            "text": "This indicates that a search operation took longer to complete than allowed by the maximum time limit for that operation. This may be the time limit specified by the client in the search request, or it may be a time limit imposed by the server.", 
            "title": "3: Time Limit Exceeded"
        }, 
        {
            "location": "/tools/openldap/ldap-server-error-codes/#4-size-limit-exceeded", 
            "text": "This indicates that a search operation would have returned more entries than were allowed for that operation. This may be the size limit specified by the client in the search request, or it may be a size limit imposed by the server. Note that the server may return a portion of the matching entries before this result.", 
            "title": "4: Size Limit Exceeded"
        }, 
        {
            "location": "/tools/openldap/ldap-server-error-codes/#5-compare-false", 
            "text": "This indicates that a compare operation was processed successfully but that the target entry did not match the provided attribute value assertion.", 
            "title": "5: Compare False"
        }, 
        {
            "location": "/tools/openldap/ldap-server-error-codes/#6-compare-true", 
            "text": "This indicates that a compare operation was processed successfully and that the target entry matched the provided attribute value assertion.", 
            "title": "6: Compare True"
        }, 
        {
            "location": "/tools/openldap/ldap-server-error-codes/#7-authentication-method-not-supported", 
            "text": "This indicates that a bind operation failed because the server does not support the requested authentication type.", 
            "title": "7: Authentication Method Not Supported"
        }, 
        {
            "location": "/tools/openldap/ldap-server-error-codes/#8-stronger-authentication-required", 
            "text": "This indicates that the server requires that the client be authenticated with a stronger mechanism than has previously been performed in order to process the request.  This result code may be used in a notice of disconnection unsolicited notification if the server believes that the security of the connection has been compromised.", 
            "title": "8: Stronger Authentication Required"
        }, 
        {
            "location": "/tools/openldap/ldap-server-error-codes/#10-referral", 
            "text": "This indicates that the server could not process the requested operation, but that it may succeed if attempted in another location, as specified by the referral URIs included in the response.", 
            "title": "10: Referral"
        }, 
        {
            "location": "/tools/openldap/ldap-server-error-codes/#11-administrative-limit-exceeded", 
            "text": "This indicates that an administrative limit was exceeded while processing the request. For example, some directory servers use this response to indicate that it would have required examining too many entries to process the request.", 
            "title": "11: Administrative Limit Exceeded"
        }, 
        {
            "location": "/tools/openldap/ldap-server-error-codes/#12-unavailable-critical-extension", 
            "text": "This indicates that the request included a critical control that was not recognized or could not be processed.", 
            "title": "12: Unavailable Critical Extension"
        }, 
        {
            "location": "/tools/openldap/ldap-server-error-codes/#13-confidentiality-required", 
            "text": "This indicates that the requested operation is only allowed over a secure connection.", 
            "title": "13: Confidentiality Required"
        }, 
        {
            "location": "/tools/openldap/ldap-server-error-codes/#14-sasl-bind-in-progress", 
            "text": "This indicates that the server requires more information from the client in order to complete the SASL bind operation. In such responses, the \"server SASL credentials\" element of the result message will often include information the client needs for subsequent phases of bind processing.", 
            "title": "14: SASL Bind in Progress"
        }, 
        {
            "location": "/tools/openldap/ldap-server-error-codes/#16-no-such-attribute", 
            "text": "This indicates that the client attempted to interact with an attribute that does not exist in the target entry (e.g., to remove an attribute or value that does not exist).", 
            "title": "16: No Such Attribute"
        }, 
        {
            "location": "/tools/openldap/ldap-server-error-codes/#17-undefined-attribute-type", 
            "text": "This indicates that the client request included an attribute type that is not defined in the server schema.", 
            "title": "17: Undefined Attribute Type"
        }, 
        {
            "location": "/tools/openldap/ldap-server-error-codes/#18-inappropriate-matching", 
            "text": "This indicates that the client request attempted to perform an unsupported type of matching against an attribute. For example, this may be used if the attribute type does not have an appropriate matching rule for the type of matching requested for that attribute.", 
            "title": "18: Inappropriate Matching"
        }, 
        {
            "location": "/tools/openldap/ldap-server-error-codes/#19-constraint-violation", 
            "text": "This indicates that the client request would have caused the server to violate some constraint defined in the server (e.g., to add more than one value to a single-valued attribute).", 
            "title": "19: Constraint Violation"
        }, 
        {
            "location": "/tools/openldap/ldap-server-error-codes/#20-attribute-or-value-exists", 
            "text": "This indicates that the client request attempted to add an attribute or value to an entry that already contained that attribute or value.", 
            "title": "20: Attribute or Value Exists"
        }, 
        {
            "location": "/tools/openldap/ldap-server-error-codes/#21-invalid-attribute-syntax", 
            "text": "This indicates that the client request would have resulted in an attribute value that did not conform to the syntax for that attribute type.", 
            "title": "21: Invalid Attribute Syntax"
        }, 
        {
            "location": "/tools/openldap/ldap-server-error-codes/#32-no-such-object", 
            "text": "This indicates that the client request targeted an entry that does not exist. Note that some servers use this result for a bind request that targets a nonexistent user, even though \"invalid credentials\" is a more appropriate result for that case.", 
            "title": "32: No Such Object"
        }, 
        {
            "location": "/tools/openldap/ldap-server-error-codes/#33-alias-problem", 
            "text": "This indicates that a problem was encountered while interacting with an alias entry (e.g., the alias refers to an entry that does not exist).", 
            "title": "33: Alias Problem"
        }, 
        {
            "location": "/tools/openldap/ldap-server-error-codes/#34-invalid-dn-syntax", 
            "text": "This indicates that the request included a malformed or invalid DN or RDN.", 
            "title": "34: Invalid DN Syntax"
        }, 
        {
            "location": "/tools/openldap/ldap-server-error-codes/#36-alias-dereferencing-problem", 
            "text": "This indicates that the client attempted to interact with an alias entry in a manner that was not allowed (e.g., for an operation that does not allow the use of aliases, or if the client does not have permission to access the entry referenced by the alias).", 
            "title": "36: Alias Dereferencing Problem"
        }, 
        {
            "location": "/tools/openldap/ldap-server-error-codes/#48-inappropriate-authentication", 
            "text": "This indicates that the client attempted to perform a type of authentication that is not allowed for the target user (e.g., because the user does not have the necessary credentials for that type of authentication). This may also indicate that the client attempted to perform anonymous authentication when that is not allowed.", 
            "title": "48: Inappropriate Authentication"
        }, 
        {
            "location": "/tools/openldap/ldap-server-error-codes/#49-invalid-credentials", 
            "text": "This indicates that the client attempted to bind as a user that does not exist, attempted to bind as a user that is not allowed to bind (e.g., because it has expired, because it has been locked because of too many failed authentication attempts, etc.), or attempted to bind with credentials that were not correct for the target user.", 
            "title": "49: Invalid Credentials"
        }, 
        {
            "location": "/tools/openldap/ldap-server-error-codes/#50-insufficient-access-rights", 
            "text": "This indicates that the client does not have permission to perform the requested operation.", 
            "title": "50: Insufficient Access Rights"
        }, 
        {
            "location": "/tools/openldap/ldap-server-error-codes/#51-busy", 
            "text": "This indicates that the server is currently too busy to process the requested operation.", 
            "title": "51: Busy"
        }, 
        {
            "location": "/tools/openldap/ldap-server-error-codes/#52-unavailable", 
            "text": "This indicates that the server is currently unavailable (e.g., because it is being shut down or is in a maintenance state).", 
            "title": "52: Unavailable"
        }, 
        {
            "location": "/tools/openldap/ldap-server-error-codes/#53-unwilling-to-perform", 
            "text": "This indicates that the server is unwilling to process the requested operation for some reason.", 
            "title": "53: Unwilling to Perform"
        }, 
        {
            "location": "/tools/openldap/ldap-server-error-codes/#54-loop-detected", 
            "text": "This indicates that the server has detected an internal loop while processing the requested operation (e.g., if two alias entries reference each other).", 
            "title": "54: Loop Detected"
        }, 
        {
            "location": "/tools/openldap/ldap-server-error-codes/#60-sort-control-missing", 
            "text": "This indicates that the search request included a virtual list view request control without also including the required server-side sort request control.", 
            "title": "60: Sort Control Missing"
        }, 
        {
            "location": "/tools/openldap/ldap-server-error-codes/#61-offset-range-error", 
            "text": "This indicates that the search request included a virtual list view request control with an invalid offset or content count.", 
            "title": "61: Offset Range Error"
        }, 
        {
            "location": "/tools/openldap/ldap-server-error-codes/#64-naming-violation", 
            "text": "This indicates that the requested add or modify DN operation would have resulted in an entry that violates server naming restrictions (e.g., as might be imposed by a name form defined in the server schema).", 
            "title": "64: Naming Violation"
        }, 
        {
            "location": "/tools/openldap/ldap-server-error-codes/#65-object-class-violation", 
            "text": "This indicates that the requested operation would have resulted in an entry that violates schema restrictions imposed by its object classes (e.g., to include an attribute that is not allowed to be present in the entry, or to omit an attribute that is required to be present in the entry).", 
            "title": "65: Object Class Violation"
        }, 
        {
            "location": "/tools/openldap/ldap-server-error-codes/#66-not-allowed-on-non-leaf-entry", 
            "text": "This indicates that the requested operation cannot be performed against an entry that has one or more subordinate entries. For example, a delete operation is normally not allowed to remove an entry that has one or more subordinates.", 
            "title": "66: Not Allowed on Non-Leaf Entry"
        }, 
        {
            "location": "/tools/openldap/ldap-server-error-codes/#67-not-allowed-on-rdn", 
            "text": "This indicates that the requested operation is not allowed because it would have altered the entry to remove an attribute value used in the entry's RDN.", 
            "title": "67: Not Allowed on RDN"
        }, 
        {
            "location": "/tools/openldap/ldap-server-error-codes/#68-entry-already-exists", 
            "text": "This indicates that the requested add or modify DN operation could not be processed because another entry already exists with the DN that would have resulted from that operation.", 
            "title": "68: Entry Already Exists"
        }, 
        {
            "location": "/tools/openldap/ldap-server-error-codes/#69-object-class-modifications-prohibited", 
            "text": "This indicates that the requested operation would have resulted in a disallowed change to the object classes contained in the entry (e.g., the operation would have changed the entry's structural object class).", 
            "title": "69: Object Class Modifications Prohibited"
        }, 
        {
            "location": "/tools/openldap/ldap-server-error-codes/#71-affects-multiple-dsas", 
            "text": "This indicates that the requested operation could not be processed because it would have required interacting with data in multiple directory server instances in a way that is not supported.", 
            "title": "71: Affects Multiple DSAs"
        }, 
        {
            "location": "/tools/openldap/ldap-server-error-codes/#76-virtual-list-view-error", 
            "text": "This indicates that a search operation failed because of processing associated with a virtual list view request control included in the request (e.g., if a necessary index was not in place to facilitate the virtual list view processing).", 
            "title": "76: Virtual List View Error"
        }, 
        {
            "location": "/tools/openldap/ldap-server-error-codes/#80-other", 
            "text": "This indicates that some problem was encountered during processing that is not covered by any of the other defined result codes (e.g., a server error).", 
            "title": "80: Other"
        }, 
        {
            "location": "/tools/openldap/ldap-server-error-codes/#118-canceled", 
            "text": "This indicates that an operation was canceled through the use of the cancel extended request. If an operation is canceled in this way, then this result code will be used for both the operation that was canceled and for the cancel extended operation itself.", 
            "title": "118: Canceled"
        }, 
        {
            "location": "/tools/openldap/ldap-server-error-codes/#119-no-such-operation", 
            "text": "This indicates that an attempt to cancel an operation via the cancel extended request was not successful because the server did not have any knowledge of the target operation. This often means that the server had already completed processing for the operation by the time it received and attempted to process the cancel request.", 
            "title": "119: No Such Operation"
        }, 
        {
            "location": "/tools/openldap/ldap-server-error-codes/#120-too-late", 
            "text": "This indicates that an attempt to cancel an operation via the cancel extended request was not successful because processing for that operation had already reached a point beyond which it could be canceled.", 
            "title": "120: Too Late"
        }, 
        {
            "location": "/tools/openldap/ldap-server-error-codes/#121-cannot-cancel", 
            "text": "This indicates that an attempt to cancel an operation via the cancel extended request was not successful because the target operation is not an operation that can be canceled. Operations that cannot be canceled include abandon, bind, unbind, and the cancel and StartTLS extended operations.", 
            "title": "121: Cannot Cancel"
        }, 
        {
            "location": "/tools/openldap/ldap-server-error-codes/#122-assertion-failed", 
            "text": "This indicates that the requested operation could not be processed because the request included an LDAP assertion request control, but the assertion filter did not match the target entry.", 
            "title": "122: Assertion Failed"
        }, 
        {
            "location": "/tools/openldap/ldap-server-error-codes/#123-authorization-denied", 
            "text": "This indicates that the requested operation could not be processed because the request included a proxied authorization request control (or some similar control intended to specify an alternate authorization identity for the operation), but the client was not allowed to request the use of that alternate authorization identity.", 
            "title": "123: Authorization Denied"
        }, 
        {
            "location": "/tools/openldap/ldap-server-error-codes/#4096-synchronization-refresh-required", 
            "text": "This indicates that an attempt to use the content synchronization request control in order to perform an incremental update failed for some reason and the client will instead need to request an initial content.", 
            "title": "4096: Synchronization Refresh Required"
        }, 
        {
            "location": "/tools/openldap/ldap-server-error-codes/#16654-no-operation", 
            "text": "This indicates that the associated operation would likely have succeeded, to the extent that the server was able to make the determination, but was not actually processed because the request included the LDAP no operation control. Note that at present, the numeric value for this result code is not an official standard because the specification for the no operation request control has not progressed far enough to be assigned an official numeric value, but the value 16654 is in common use by a number of servers for this purpose.", 
            "title": "16654: No Operation"
        }, 
        {
            "location": "/tools/openldap/faq/", 
            "text": "OpenLDAP FAQ's\n\n\nHow to generate password for slapd.conf file\n\n\n1\n2\n3\n/opt/openldap/sbin/slappasswd\nNew password:\nRe-enter new password:\n\n\n\n\n\n\ncopy the encrypted password and update the slapd.conf file.\n\n/opt/openldap here, is the install path of openLDAP.\n\n\n[ERROR]: BerkeleyDB not available\n\n\nIf you get one of these errors:\n\n\n\n\nerror:BerkeleyDB not available\n\n\nerror: Berkeley DB version mismatch\n\n\nerror : Berkeley DB library and header version mismatch\n\n\n\n\nthen check if the Berleley Db installation is done correctly. Follow the \ninstallation guide\n for more details on BDB installation.\n\n\nNext, export below env varibales before installing OpenLDAP.\n\n\n1\n2\n3\nexport\n \nCPPFLAGS\n=\n-I/usr/local/BerkeleyDB.4.8/include\n\n\nexport\n \nLDFLAGS\n=\n-L/usr/local/BerkeleyDB.4.8/lib\n\n\nexport\n \nLD_LIBRARY_PATH\n=\n/usr/local/BerkeleyDB.4.8/lib/\n\n\n\n\n\n\n\n/usr/local/BerkeleyDB.4.8 here, is the Berkeley DB install path.\n\n\nReference\n\n\nBerkeley DB version mismatch\n\n\n[ERROR]: soelim is required to build OpenLDAP\n\n\nInstall groff package for linux.\n\n\n1\nyum install groff -y\n\n\n\n\n\n\n[WARNING]: no DB_CONFIG file found in directory /data/openldap\n\n\nYou will get this error while configuring openLDAP if you have not configured a DB_CONFIG file in the data directory.\n\n\nDB_CONFIG\n file holds additional configuration for the Berkeley database.", 
            "title": "FAQ"
        }, 
        {
            "location": "/tools/openldap/faq/#openldap-faqs", 
            "text": "", 
            "title": "OpenLDAP FAQ's"
        }, 
        {
            "location": "/tools/openldap/faq/#how-to-generate-password-for-slapdconf-file", 
            "text": "1\n2\n3 /opt/openldap/sbin/slappasswd\nNew password:\nRe-enter new password:   copy the encrypted password and update the slapd.conf file. \n/opt/openldap here, is the install path of openLDAP.", 
            "title": "How to generate password for slapd.conf file"
        }, 
        {
            "location": "/tools/openldap/faq/#error-berkeleydb-not-available", 
            "text": "If you get one of these errors:   error:BerkeleyDB not available  error: Berkeley DB version mismatch  error : Berkeley DB library and header version mismatch   then check if the Berleley Db installation is done correctly. Follow the  installation guide  for more details on BDB installation.  Next, export below env varibales before installing OpenLDAP.  1\n2\n3 export   CPPFLAGS = -I/usr/local/BerkeleyDB.4.8/include  export   LDFLAGS = -L/usr/local/BerkeleyDB.4.8/lib  export   LD_LIBRARY_PATH = /usr/local/BerkeleyDB.4.8/lib/    /usr/local/BerkeleyDB.4.8 here, is the Berkeley DB install path.  Reference  Berkeley DB version mismatch", 
            "title": "[ERROR]: BerkeleyDB not available"
        }, 
        {
            "location": "/tools/openldap/faq/#error-soelim-is-required-to-build-openldap", 
            "text": "Install groff package for linux.  1 yum install groff -y", 
            "title": "[ERROR]: soelim is required to build OpenLDAP"
        }, 
        {
            "location": "/tools/openldap/faq/#warning-no-db_config-file-found-in-directory-dataopenldap", 
            "text": "You will get this error while configuring openLDAP if you have not configured a DB_CONFIG file in the data directory.  DB_CONFIG  file holds additional configuration for the Berkeley database.", 
            "title": "[WARNING]: no DB_CONFIG file found in directory /data/openldap"
        }, 
        {
            "location": "/tools/jenkins/", 
            "text": "", 
            "title": "Introduction"
        }, 
        {
            "location": "/docker/", 
            "text": "Docker\n\n\nOverview\n\n\nDocker is the world\u2019s leading software container platform. Developers use Docker to eliminate \u201cworks on my machine\u201d problems when collaborating on code with co-workers. Operators use Docker to run and manage apps side-by-side in isolated containers to get better compute density. Enterprises use Docker to build agile software delivery pipelines to ship new features faster, more securely and with confidence for both Linux and Windows Server apps.\n\n\n...more information\n\n\nWhat is a Container?\n\n\nUsing containers, everything required to make a piece of software run is packaged into isolated containers. Unlike VMs, containers do not bundle a full operating system - only libraries and settings required to make the software work are needed. This makes for efficient, lightweight, self-contained systems and guarantees that software will always run the same, regardless of where it\u2019s deployed.\n\n\n...more information\n\n\nVirtual Machine v/s Containers\n\n\n\n\n\n\nWhat makes docker different that VMs?\n\n\n\n\nGet Started with Docker\n\n\nPlease follow the \ngetting started guide\n.\n\n\nMandatory Reading\n\n\n\n\nDockerfile reference\n\n\nDockerfile best practices\n\n\nDockerfile linting\n\n\n\n\nFurther Reading\n\n\n\n\nTwelve Factor Container\n\n\nDocker metrics Prometheus\n\n\nDocker Datacenter (Docker EE)\n\n\nDocker Swarm Tutorials\n\n\nViktor Farcic's Presentations and Workshops", 
            "title": "Introduction"
        }, 
        {
            "location": "/docker/#docker", 
            "text": "", 
            "title": "Docker"
        }, 
        {
            "location": "/docker/#overview", 
            "text": "Docker is the world\u2019s leading software container platform. Developers use Docker to eliminate \u201cworks on my machine\u201d problems when collaborating on code with co-workers. Operators use Docker to run and manage apps side-by-side in isolated containers to get better compute density. Enterprises use Docker to build agile software delivery pipelines to ship new features faster, more securely and with confidence for both Linux and Windows Server apps.  ...more information", 
            "title": "Overview"
        }, 
        {
            "location": "/docker/#what-is-a-container", 
            "text": "Using containers, everything required to make a piece of software run is packaged into isolated containers. Unlike VMs, containers do not bundle a full operating system - only libraries and settings required to make the software work are needed. This makes for efficient, lightweight, self-contained systems and guarantees that software will always run the same, regardless of where it\u2019s deployed.  ...more information", 
            "title": "What is a Container?"
        }, 
        {
            "location": "/docker/#virtual-machine-vs-containers", 
            "text": "What makes docker different that VMs?", 
            "title": "Virtual Machine v/s Containers"
        }, 
        {
            "location": "/docker/#get-started-with-docker", 
            "text": "Please follow the  getting started guide .", 
            "title": "Get Started with Docker"
        }, 
        {
            "location": "/docker/#mandatory-reading", 
            "text": "Dockerfile reference  Dockerfile best practices  Dockerfile linting", 
            "title": "Mandatory Reading"
        }, 
        {
            "location": "/docker/#further-reading", 
            "text": "Twelve Factor Container  Docker metrics Prometheus  Docker Datacenter (Docker EE)  Docker Swarm Tutorials  Viktor Farcic's Presentations and Workshops", 
            "title": "Further Reading"
        }, 
        {
            "location": "/docker/docker-compose/", 
            "text": "Docker Compose\n\n\nOverview\n\n\nCompose is a tool for defining and running multi-container Docker applications. With Compose, you use a Compose file to configure your application\u2019s services. Then, using a single command, you create and start all the services from your configuration.\n\n\nUsing Compose is basically a three-step process:\n\n\n\n\nDefine your app\u2019s environment with a Dockerfile so it can be reproduced anywhere.\n\n\nDefine the services that make up your app in docker-compose.yml so they can be run together in an isolated environment.\n\n\nLastly, run docker-compose up and Compose will start and run your entire app.\n\n\n\n\nA docker-compose.yml looks like \nthis", 
            "title": "Docker Compose"
        }, 
        {
            "location": "/docker/docker-compose/#docker-compose", 
            "text": "", 
            "title": "Docker Compose"
        }, 
        {
            "location": "/docker/docker-compose/#overview", 
            "text": "Compose is a tool for defining and running multi-container Docker applications. With Compose, you use a Compose file to configure your application\u2019s services. Then, using a single command, you create and start all the services from your configuration.  Using Compose is basically a three-step process:   Define your app\u2019s environment with a Dockerfile so it can be reproduced anywhere.  Define the services that make up your app in docker-compose.yml so they can be run together in an isolated environment.  Lastly, run docker-compose up and Compose will start and run your entire app.   A docker-compose.yml looks like  this", 
            "title": "Overview"
        }, 
        {
            "location": "/docker/docker-machine/", 
            "text": "Docker Machine\n\n\nDocker Machine is a tool that lets you install Docker Engine on virtual hosts, and manage the hosts with docker-machine commands. You can use Machine to create Docker hosts on your local Mac or Windows box, on your company network, in your data center, or on cloud providers like AWS or Digital Ocean.\nUsing docker-machine commands, you can start, inspect, stop, and restart a managed host, upgrade the Docker client and daemon, and configure a Docker client to talk to your host.", 
            "title": "Docker Machine"
        }, 
        {
            "location": "/docker/docker-machine/#docker-machine", 
            "text": "Docker Machine is a tool that lets you install Docker Engine on virtual hosts, and manage the hosts with docker-machine commands. You can use Machine to create Docker hosts on your local Mac or Windows box, on your company network, in your data center, or on cloud providers like AWS or Digital Ocean.\nUsing docker-machine commands, you can start, inspect, stop, and restart a managed host, upgrade the Docker client and daemon, and configure a Docker client to talk to your host.", 
            "title": "Docker Machine"
        }, 
        {
            "location": "/docker/docker-networks/", 
            "text": "Docker Networks\n\n\nThis section provides an overview of Docker\u2019s default networking behavior, including the type of networks created by default and how to create your own user-defined networks. It also describes the resources required to create networks on a single host or across a cluster of hosts.\n\n\nDefault Networks\n\n\nWhen you install Docker, it creates three networks automatically. You can list these networks using the docker network ls command:\n\n\n1\n2\n3\n4\n5\n6\n$ docker network ls\n\nNETWORK ID          NAME                DRIVER\n7fca4eb8c647        bridge              bridge\n9f904ee27bf5        none                null\ncf03ee007fb4        host                host\n\n\n\n\n\n\n...more information\n\n\nUser-defined networks\n\n\nIt is recommended to use user-defined bridge networks to control which containers can communicate with each other, and also to enable automatic DNS resolution of container names to IP addresses.\n\n\n...more information\n\n\nBridge networks\n\n\nA bridge network is the most common type of network used in Docker. Bridge networks are similar to the default bridge network, but add some new features and remove some old abilities. The following examples create some bridge networks and perform some experiments on containers on these networks.\n\n\n1\n2\ndocker network create --driver bridge isolated_nw\ndocker network inspect isolated_nw\n\n\n\n\n\n\nAfter you create the network, you can launch containers on it using the docker run \n--network=\nNETWORK\n option.", 
            "title": "Docker Networks"
        }, 
        {
            "location": "/docker/docker-networks/#docker-networks", 
            "text": "This section provides an overview of Docker\u2019s default networking behavior, including the type of networks created by default and how to create your own user-defined networks. It also describes the resources required to create networks on a single host or across a cluster of hosts.", 
            "title": "Docker Networks"
        }, 
        {
            "location": "/docker/docker-networks/#default-networks", 
            "text": "When you install Docker, it creates three networks automatically. You can list these networks using the docker network ls command:  1\n2\n3\n4\n5\n6 $ docker network ls\n\nNETWORK ID          NAME                DRIVER\n7fca4eb8c647        bridge              bridge\n9f904ee27bf5        none                null\ncf03ee007fb4        host                host   ...more information", 
            "title": "Default Networks"
        }, 
        {
            "location": "/docker/docker-networks/#user-defined-networks", 
            "text": "It is recommended to use user-defined bridge networks to control which containers can communicate with each other, and also to enable automatic DNS resolution of container names to IP addresses.  ...more information", 
            "title": "User-defined networks"
        }, 
        {
            "location": "/docker/docker-networks/#bridge-networks", 
            "text": "A bridge network is the most common type of network used in Docker. Bridge networks are similar to the default bridge network, but add some new features and remove some old abilities. The following examples create some bridge networks and perform some experiments on containers on these networks.  1\n2 docker network create --driver bridge isolated_nw\ndocker network inspect isolated_nw   After you create the network, you can launch containers on it using the docker run  --network= NETWORK  option.", 
            "title": "Bridge networks"
        }, 
        {
            "location": "/docker/docker-secrets/", 
            "text": "Docker Secrets\n\nhttps://docs.docker.com/engine/swarm/secrets/#how-docker-manages-secrets", 
            "title": "Docker Secrets"
        }, 
        {
            "location": "/golang/", 
            "text": "Go Programing Language [golang]\n\n\n\n\nGo is an open source programming language that makes it easy to build simple, reliable, and efficient software.\n\n\n\n\n\n\nWhy Go?\n\n\n9 reasons to choose GoLang for your next web application\n\n\nInstallation\n\n\nSetting up your workspace\n\n\n\n\nImportant links\n\n\n\n\nGetting started with Go\n\n\nJSON Validator\n\n\nJSON to Go Struct\n\n\nCurl to Go\n\n\nPointers Vs Values\n\n\nGo for Java programers\n\n\nFetch JSON from an API\n\n\nGo Slices (arrays / lists)\n\n\nForEach loop\n\n\nGo Report card\n\n\nBest Practices\n\n\nInterfaces in go", 
            "title": "Introduction"
        }, 
        {
            "location": "/golang/#go-programing-language-golang", 
            "text": "Go is an open source programming language that makes it easy to build simple, reliable, and efficient software.    Why Go?  9 reasons to choose GoLang for your next web application  Installation  Setting up your workspace", 
            "title": "Go Programing Language [golang]"
        }, 
        {
            "location": "/golang/#important-links", 
            "text": "Getting started with Go  JSON Validator  JSON to Go Struct  Curl to Go  Pointers Vs Values  Go for Java programers  Fetch JSON from an API  Go Slices (arrays / lists)  ForEach loop  Go Report card  Best Practices  Interfaces in go", 
            "title": "Important links"
        }, 
        {
            "location": "/golang/http-request/", 
            "text": "Http request in Go\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\npackage\n \nmain\n\n\n\nimport\n \n(\n\n    \nnet/http\n\n    \nbytes\n\n    \nfmt\n\n    \nio/ioutil\n\n    \nlog\n\n    \nnet/url\n\n\n)\n\n\n\nfunc\n \nmain\n()\n \n{\n\n    \n//Call the http function to create and make a http request\n\n\n}\n\n\n\n// AuthUser represents the credential for Authentication\n\n\ntype\n \nAuthUser\n \nstruct\n \n{\n\n    \nUsername\n \nstring\n\n    \nPassword\n \nstring\n\n\n}\n\n\n\n/*\n\n\nError prints error\n\n\n@param err error  error details\n\n\n@return void\n\n\n*/\n\n\nfunc\n \nError\n(\nerr\n \nerror\n,\n \nerrorMessage\n \nstring\n)\n \n{\n\n    \nif\n \nerr\n \n!=\n \nnil\n \n{\n\n        \nlog\n.\nPrintln\n(\nerrorMessage\n)\n\n        \nlog\n.\nFatal\n(\nerr\n)\n\n    \n}\n\n\n}\n\n\n\n/*\n\n\nCreateBaseRequest create the base request for a HTTP request\n\n\n@param method   string          http request method eg: GET, POST, etc\n\n\n@param url      string          http request url\n\n\n@param body     []byte          request body\n\n\n@param user     m.AuthUser      User authentication details\n\n\n@param verbose  boolean         prints verbose logs if set to true\n\n\n@return *http.Request   HTTP base request\n\n\n*/\n\n\nfunc\n \nCreateBaseRequest\n(\nmethod\n,\n \nurl\n \nstring\n,\n \nbody\n \n[]\nbyte\n,\n \nuser\n \nAuthUser\n,\n \nverbose\n \nbool\n)\n \n*\nhttp\n.\nRequest\n \n{\n\n    \nreq\n,\n \nerr\n \n:=\n \nhttp\n.\nNewRequest\n(\nmethod\n,\n \nurl\n,\n \nbytes\n.\nNewBuffer\n(\nbody\n))\n\n    \nreq\n.\nSetBasicAuth\n(\nuser\n.\nUsername\n,\n \nuser\n.\nPassword\n)\n\n    \nreq\n.\nHeader\n.\nSet\n(\nContent-Type\n,\n \napplication/json\n)\n\n    \nreq\n.\nHeader\n.\nSet\n(\nAccept\n,\n \napplication/json\n)\n\n    \nError\n(\nerr\n,\n \nError creating the request\n)\n\n\n    \nif\n \nverbose\n \n{\n\n        \nfmt\n.\nPrintln\n(\nRequest Url:\n,\n \nreq\n.\nURL\n)\n\n        \nfmt\n.\nPrintln\n(\nRequest Headers:\n,\n \nreq\n.\nHeader\n)\n\n        \nfmt\n.\nPrintln\n(\nRequest Body:\n,\n \nreq\n.\nBody\n)\n\n    \n}\n\n\n    \nreturn\n \nreq\n\n\n}\n\n\n\n/*\n\n\nHTTPRequest makes a request to the remote server via a proxy server\n\n\n@param user     m.AuthUser      User authentication details\n\n\n@param req      *http.Request   HTTP base request\n\n\n@param verbose  boolean         prints verbose logs if set to true\n\n\n@return []byte  response body\n\n\n@return string  response status\n\n\n*/\n\n\nfunc\n \nHTTPRequest\n(\nuser\n \nAuthUser\n,\n \nreq\n \n*\nhttp\n.\nRequest\n,\n \nverbose\n \nbool\n)\n \n([]\nbyte\n,\n \nstring\n)\n \n{\n\n\n    \nclient\n \n:=\n \nhttp\n.\nClient\n{}\n\n\n    \nresp\n,\n \nerr\n \n:=\n \nclient\n.\nDo\n(\nreq\n)\n\n    \nError\n(\nerr\n,\n \nThere was a problem in making the request\n)\n\n\n    \ndefer\n \nresp\n.\nBody\n.\nClose\n()\n\n    \nrespBody\n,\n \nerr\n \n:=\n \nioutil\n.\nReadAll\n(\nresp\n.\nBody\n)\n\n    \nError\n(\nerr\n,\n \nThere was a problem reading the response body\n)\n\n\n    \nif\n \nverbose\n \n{\n\n        \nfmt\n.\nPrintln\n(\nResponse Headers:\n,\n \nresp\n.\nHeader\n)\n\n        \nfmt\n.\nPrintln\n(\nResponse Status:\n,\n \nresp\n.\nStatus\n)\n\n        \nfmt\n.\nPrintln\n(\nResponse Body:\n,\n \nstring\n(\nrespBody\n))\n\n    \n}\n\n    \nreturn\n \nrespBody\n,\n \nresp\n.\nStatus\n\n\n}", 
            "title": "Http Request in Go"
        }, 
        {
            "location": "/golang/http-request/#http-request-in-go", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84 package   main  import   ( \n     net/http \n     bytes \n     fmt \n     io/ioutil \n     log \n     net/url  )  func   main ()   { \n     //Call the http function to create and make a http request  }  // AuthUser represents the credential for Authentication  type   AuthUser   struct   { \n     Username   string \n     Password   string  }  /*  Error prints error  @param err error  error details  @return void  */  func   Error ( err   error ,   errorMessage   string )   { \n     if   err   !=   nil   { \n         log . Println ( errorMessage ) \n         log . Fatal ( err ) \n     }  }  /*  CreateBaseRequest create the base request for a HTTP request  @param method   string          http request method eg: GET, POST, etc  @param url      string          http request url  @param body     []byte          request body  @param user     m.AuthUser      User authentication details  @param verbose  boolean         prints verbose logs if set to true  @return *http.Request   HTTP base request  */  func   CreateBaseRequest ( method ,   url   string ,   body   [] byte ,   user   AuthUser ,   verbose   bool )   * http . Request   { \n     req ,   err   :=   http . NewRequest ( method ,   url ,   bytes . NewBuffer ( body )) \n     req . SetBasicAuth ( user . Username ,   user . Password ) \n     req . Header . Set ( Content-Type ,   application/json ) \n     req . Header . Set ( Accept ,   application/json ) \n     Error ( err ,   Error creating the request ) \n\n     if   verbose   { \n         fmt . Println ( Request Url: ,   req . URL ) \n         fmt . Println ( Request Headers: ,   req . Header ) \n         fmt . Println ( Request Body: ,   req . Body ) \n     } \n\n     return   req  }  /*  HTTPRequest makes a request to the remote server via a proxy server  @param user     m.AuthUser      User authentication details  @param req      *http.Request   HTTP base request  @param verbose  boolean         prints verbose logs if set to true  @return []byte  response body  @return string  response status  */  func   HTTPRequest ( user   AuthUser ,   req   * http . Request ,   verbose   bool )   ([] byte ,   string )   { \n\n     client   :=   http . Client {} \n\n     resp ,   err   :=   client . Do ( req ) \n     Error ( err ,   There was a problem in making the request ) \n\n     defer   resp . Body . Close () \n     respBody ,   err   :=   ioutil . ReadAll ( resp . Body ) \n     Error ( err ,   There was a problem reading the response body ) \n\n     if   verbose   { \n         fmt . Println ( Response Headers: ,   resp . Header ) \n         fmt . Println ( Response Status: ,   resp . Status ) \n         fmt . Println ( Response Body: ,   string ( respBody )) \n     } \n     return   respBody ,   resp . Status  }", 
            "title": "Http request in Go"
        }, 
        {
            "location": "/golang/http-request-proxy/", 
            "text": "HTTP Request Via A Proxy\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\npackage\n \nmain\n\n\n\nimport\n \n(\n\n    \nnet/http\n\n    \nbytes\n\n    \nfmt\n\n    \nio/ioutil\n\n    \nlog\n\n    \nnet/url\n\n\n)\n\n\n\nfunc\n \nmain\n()\n \n{\n\n    \n//Call the http function to create and make a http request\n\n\n}\n\n\n\n// AuthUser represents the credential for Authentication\n\n\ntype\n \nAuthUser\n \nstruct\n \n{\n\n    \nUsername\n \nstring\n\n    \nPassword\n \nstring\n\n\n}\n\n\n\n/*\n\n\nError prints error\n\n\n@param err error  error details\n\n\n@return void\n\n\n*/\n\n\nfunc\n \nError\n(\nerr\n \nerror\n,\n \nerrorMessage\n \nstring\n)\n \n{\n\n    \nif\n \nerr\n \n!=\n \nnil\n \n{\n\n        \nlog\n.\nPrintln\n(\nerrorMessage\n)\n\n        \nlog\n.\nFatal\n(\nerr\n)\n\n    \n}\n\n\n}\n\n\n\n/*\n\n\nCreateBaseRequest create the base request for a HTTP request\n\n\n@param method   string          http request method eg: GET, POST, etc\n\n\n@param url      string          http request url\n\n\n@param body     []byte          request body\n\n\n@param user     m.AuthUser      User authentication details\n\n\n@param verbose  boolean         prints verbose logs if set to true\n\n\n@return *http.Request   HTTP base request\n\n\n*/\n\n\nfunc\n \nCreateBaseRequest\n(\nmethod\n,\n \nurl\n \nstring\n,\n \nbody\n \n[]\nbyte\n,\n \nuser\n \nAuthUser\n,\n \nverbose\n \nbool\n)\n \n*\nhttp\n.\nRequest\n \n{\n\n    \nreq\n,\n \nerr\n \n:=\n \nhttp\n.\nNewRequest\n(\nmethod\n,\n \nurl\n,\n \nbytes\n.\nNewBuffer\n(\nbody\n))\n\n    \nreq\n.\nSetBasicAuth\n(\nuser\n.\nUsername\n,\n \nuser\n.\nPassword\n)\n\n    \nreq\n.\nHeader\n.\nSet\n(\nContent-Type\n,\n \napplication/json\n)\n\n    \nreq\n.\nHeader\n.\nSet\n(\nAccept\n,\n \napplication/json\n)\n\n    \nError\n(\nerr\n,\n \nError creating the request\n)\n\n\n    \nif\n \nverbose\n \n{\n\n        \nfmt\n.\nPrintln\n(\nRequest Url:\n,\n \nreq\n.\nURL\n)\n\n        \nfmt\n.\nPrintln\n(\nRequest Headers:\n,\n \nreq\n.\nHeader\n)\n\n        \nfmt\n.\nPrintln\n(\nRequest Body:\n,\n \nreq\n.\nBody\n)\n\n    \n}\n\n\n    \nreturn\n \nreq\n\n\n}\n\n\n\n/*\n\n\nHTTPRequest makes a request to the remote server via a proxy server\n\n\n@param user     m.AuthUser      User authentication details\n\n\n@param req      *http.Request   HTTP base request\n\n\n@param verbose  boolean         prints verbose logs if set to true\n\n\n@return []byte  response body\n\n\n@return string  response status\n\n\n*/\n\n\nfunc\n \nHTTPRequest\n(\nuser\n \nAuthUser\n,\n \nreq\n \n*\nhttp\n.\nRequest\n,\n \nverbose\n \nbool\n)\n \n([]\nbyte\n,\n \nstring\n)\n \n{\n\n\n    \nclient\n \n:=\n \nhttp\n.\nClient\n{}\n\n\n    \n// Proxy Settings\n\n    \nproxyURL\n \n:=\n \nhttp://proxy-host:proxy-port\n\n    \nproxyUrl\n,\n \n_\n \n:=\n \nurl\n.\nParse\n(\nproxyURL\n)\n\n\n    \ntransport\n \n:=\n \nhttp\n.\nTransport\n{\n\n        \nProxy\n:\n \nhttp\n.\nProxyURL\n(\nproxyUrl\n),\n\n    \n}\n\n\n    \nclient\n.\nTransport\n \n=\n \ntransport\n\n\n    \nresp\n,\n \nerr\n \n:=\n \nclient\n.\nDo\n(\nreq\n)\n\n    \nError\n(\nerr\n,\n \nThere was a problem in making the request\n)\n\n\n    \ndefer\n \nresp\n.\nBody\n.\nClose\n()\n\n    \nrespBody\n,\n \nerr\n \n:=\n \nioutil\n.\nReadAll\n(\nresp\n.\nBody\n)\n\n    \nError\n(\nerr\n,\n \nThere was a problem reading the response body\n)\n\n\n    \nif\n \nverbose\n \n{\n\n        \nfmt\n.\nPrintln\n(\nResponse Headers:\n,\n \nresp\n.\nHeader\n)\n\n        \nfmt\n.\nPrintln\n(\nResponse Status:\n,\n \nresp\n.\nStatus\n)\n\n        \nfmt\n.\nPrintln\n(\nResponse Body:\n,\n \nstring\n(\nrespBody\n))\n\n    \n}\n\n    \nreturn\n \nrespBody\n,\n \nresp\n.\nStatus\n\n\n}", 
            "title": "Http Request via a Proxy"
        }, 
        {
            "location": "/golang/http-request-proxy/#http-request-via-a-proxy", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94 package   main  import   ( \n     net/http \n     bytes \n     fmt \n     io/ioutil \n     log \n     net/url  )  func   main ()   { \n     //Call the http function to create and make a http request  }  // AuthUser represents the credential for Authentication  type   AuthUser   struct   { \n     Username   string \n     Password   string  }  /*  Error prints error  @param err error  error details  @return void  */  func   Error ( err   error ,   errorMessage   string )   { \n     if   err   !=   nil   { \n         log . Println ( errorMessage ) \n         log . Fatal ( err ) \n     }  }  /*  CreateBaseRequest create the base request for a HTTP request  @param method   string          http request method eg: GET, POST, etc  @param url      string          http request url  @param body     []byte          request body  @param user     m.AuthUser      User authentication details  @param verbose  boolean         prints verbose logs if set to true  @return *http.Request   HTTP base request  */  func   CreateBaseRequest ( method ,   url   string ,   body   [] byte ,   user   AuthUser ,   verbose   bool )   * http . Request   { \n     req ,   err   :=   http . NewRequest ( method ,   url ,   bytes . NewBuffer ( body )) \n     req . SetBasicAuth ( user . Username ,   user . Password ) \n     req . Header . Set ( Content-Type ,   application/json ) \n     req . Header . Set ( Accept ,   application/json ) \n     Error ( err ,   Error creating the request ) \n\n     if   verbose   { \n         fmt . Println ( Request Url: ,   req . URL ) \n         fmt . Println ( Request Headers: ,   req . Header ) \n         fmt . Println ( Request Body: ,   req . Body ) \n     } \n\n     return   req  }  /*  HTTPRequest makes a request to the remote server via a proxy server  @param user     m.AuthUser      User authentication details  @param req      *http.Request   HTTP base request  @param verbose  boolean         prints verbose logs if set to true  @return []byte  response body  @return string  response status  */  func   HTTPRequest ( user   AuthUser ,   req   * http . Request ,   verbose   bool )   ([] byte ,   string )   { \n\n     client   :=   http . Client {} \n\n     // Proxy Settings \n     proxyURL   :=   http://proxy-host:proxy-port \n     proxyUrl ,   _   :=   url . Parse ( proxyURL ) \n\n     transport   :=   http . Transport { \n         Proxy :   http . ProxyURL ( proxyUrl ), \n     } \n\n     client . Transport   =   transport \n\n     resp ,   err   :=   client . Do ( req ) \n     Error ( err ,   There was a problem in making the request ) \n\n     defer   resp . Body . Close () \n     respBody ,   err   :=   ioutil . ReadAll ( resp . Body ) \n     Error ( err ,   There was a problem reading the response body ) \n\n     if   verbose   { \n         fmt . Println ( Response Headers: ,   resp . Header ) \n         fmt . Println ( Response Status: ,   resp . Status ) \n         fmt . Println ( Response Body: ,   string ( respBody )) \n     } \n     return   respBody ,   resp . Status  }", 
            "title": "HTTP Request Via A Proxy"
        }, 
        {
            "location": "/golang/string-functions/", 
            "text": "String Functions in Go\n\n\nTitle\n\n\nCapitalize a string\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\npackage\n \nmain\n\n\n\nimport\n \n(\n\n    \nfmt\n\n    \nstrings\n\n\n)\n\n\n\nfunc\n \nmain\n()\n \n{\n\n    \nstring\n \n:=\n \nmy name is allan\n\n    \nfmt\n.\nPrintln\n(\nTitle : \n,\n \nstrings\n.\nTitle\n(\nstring\n))\n\n\n}\n\n\n// Output:\n\n\n//Title :  My Name Is Allan\n\n\n\n\n\n\n\nToLower\n\n\nConvert a string to lower case\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\npackage\n \nmain\n\n\n\nimport\n \n(\n\n    \nfmt\n\n    \nstrings\n\n\n)\n\n\n\nfunc\n \nmain\n()\n \n{\n\n    \nstring\n \n:=\n \nMY Name IS AllAn\n\n    \nfmt\n.\nPrintln\n(\nToLower : \n,\n \nstrings\n.\nToLower\n(\nstring\n))\n\n\n}\n\n\n//Output:\n\n\n//ToLower :  my name is allan\n\n\n\n\n\n\n\nToUpper\n\n\nConver a string to upper case\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\npackage\n \nmain\n\n\n\nimport\n \n(\n\n    \nfmt\n\n    \nstrings\n\n\n)\n\n\n\nfunc\n \nmain\n()\n \n{\n\n    \nstring\n \n:=\n \nMY Name IS AllAn\n\n\nfmt\n.\nPrintln\n(\nToUpper : \n,\n \nstrings\n.\nToUpper\n(\nstring\n))\n\n\n}\n\n\n//Output:\n\n\n//ToUpper :  MY NAME IS ALLAN\n\n\n\n\n\n\n\nContains\n\n\nChecks if a string contains a substring\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\npackage\n \nmain\n\n\n\nimport\n \n(\n\n    \nfmt\n\n    \nstrings\n\n\n)\n\n\n\nfunc\n \nmain\n()\n \n{\n\n    \nstring\n \n:=\n \nMy name is Allan\n\n    \nfmt\n.\nPrintln\n(\nContains : \n,\n \nstrings\n.\nContains\n(\nstring\n,\n \nAllan\n))\n\n    \nfmt\n.\nPrintln\n(\nContains : \n,\n \nstrings\n.\nContains\n(\nstring\n,\n \nsomething\n))\n\n\n}\n\n\n//Output:\n\n\n//Contains :  true\n\n\n//Contains :  false\n\n\n\n\n\n\n\nReplace\n\n\nReplace a substring with another\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\npackage\n \nmain\n\n\n\nimport\n \n(\n\n    \nfmt\n\n    \nstrings\n\n\n)\n\n\n\nfunc\n \nmain\n()\n \n{\n\n    \nstring\n \n:=\n \nMy name is Allan\n\n    \nfmt\n.\nPrintln\n(\nReplace : \n,\n \nstrings\n.\nReplace\n(\nstring\n,\n \nAllan\n,\n \nAllan Selvan\n,\n \n1\n))\n\n\n}\n\n\n//Output:\n\n\n//Replace :  My name is Allan Selvan\n\n\n\n\n\n\n\nSplit\n\n\nSplit a string\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\npackage\n \nmain\n\n\n\nimport\n \n(\n\n    \nfmt\n\n    \nstrings\n\n\n)\n\n\n\nfunc\n \nmain\n()\n \n{\n\n    \nstring\n \n:=\n \nstone,paper,scissor\n\n    \nfmt\n.\nPrintln\n(\nSplit : \n,\n \nstrings\n.\nSplit\n(\nstring\n,\n \n,\n))\n\n\n}\n\n\n//Output:\n\n\n//Split :  [stone paper scissor]\n\n\n\n\n\n\n\nJoin\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\npackage\n \nmain\n\n\n\nimport\n \n(\n\n    \nfmt\n\n    \nstrings\n\n\n)\n\n\n\nfunc\n \nmain\n()\n \n{\n\n    \nstring\n \n:=\n \n[]\nstring\n{\nstone\n,\n \npaper\n,\n \nscissor\n}\n\n    \nfmt\n.\nPrintln\n(\nJoin : \n,\n \nstrings\n.\nJoin\n(\nstring\n,\n \n,\n))\n\n\n}\n\n\n//Output:\n\n\n//Join :  stone,paper,scissor", 
            "title": "String Functions"
        }, 
        {
            "location": "/golang/string-functions/#string-functions-in-go", 
            "text": "", 
            "title": "String Functions in Go"
        }, 
        {
            "location": "/golang/string-functions/#title", 
            "text": "Capitalize a string   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 package   main  import   ( \n     fmt \n     strings  )  func   main ()   { \n     string   :=   my name is allan \n     fmt . Println ( Title :  ,   strings . Title ( string ))  }  // Output:  //Title :  My Name Is Allan", 
            "title": "Title"
        }, 
        {
            "location": "/golang/string-functions/#tolower", 
            "text": "Convert a string to lower case   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 package   main  import   ( \n     fmt \n     strings  )  func   main ()   { \n     string   :=   MY Name IS AllAn \n     fmt . Println ( ToLower :  ,   strings . ToLower ( string ))  }  //Output:  //ToLower :  my name is allan", 
            "title": "ToLower"
        }, 
        {
            "location": "/golang/string-functions/#toupper", 
            "text": "Conver a string to upper case   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 package   main  import   ( \n     fmt \n     strings  )  func   main ()   { \n     string   :=   MY Name IS AllAn  fmt . Println ( ToUpper :  ,   strings . ToUpper ( string ))  }  //Output:  //ToUpper :  MY NAME IS ALLAN", 
            "title": "ToUpper"
        }, 
        {
            "location": "/golang/string-functions/#contains", 
            "text": "Checks if a string contains a substring   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15 package   main  import   ( \n     fmt \n     strings  )  func   main ()   { \n     string   :=   My name is Allan \n     fmt . Println ( Contains :  ,   strings . Contains ( string ,   Allan )) \n     fmt . Println ( Contains :  ,   strings . Contains ( string ,   something ))  }  //Output:  //Contains :  true  //Contains :  false", 
            "title": "Contains"
        }, 
        {
            "location": "/golang/string-functions/#replace", 
            "text": "Replace a substring with another   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 package   main  import   ( \n     fmt \n     strings  )  func   main ()   { \n     string   :=   My name is Allan \n     fmt . Println ( Replace :  ,   strings . Replace ( string ,   Allan ,   Allan Selvan ,   1 ))  }  //Output:  //Replace :  My name is Allan Selvan", 
            "title": "Replace"
        }, 
        {
            "location": "/golang/string-functions/#split", 
            "text": "Split a string   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 package   main  import   ( \n     fmt \n     strings  )  func   main ()   { \n     string   :=   stone,paper,scissor \n     fmt . Println ( Split :  ,   strings . Split ( string ,   , ))  }  //Output:  //Split :  [stone paper scissor]", 
            "title": "Split"
        }, 
        {
            "location": "/golang/string-functions/#join", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 package   main  import   ( \n     fmt \n     strings  )  func   main ()   { \n     string   :=   [] string { stone ,   paper ,   scissor } \n     fmt . Println ( Join :  ,   strings . Join ( string ,   , ))  }  //Output:  //Join :  stone,paper,scissor", 
            "title": "Join"
        }, 
        {
            "location": "/golang/slices/", 
            "text": "Slices in Go\n\n\nDeclaring a slice\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\npackage\n \nmain\n\n\n\nimport\n \nfmt\n\n\n\nfunc\n \nmain\n()\n \n{\n\n    \nletters\n \n:=\n \n[]\nstring\n{\na\n,\n \nb\n,\n \nc\n,\n \nd\n}\n\n    \nfmt\n.\nPrintln\n(\nletters\n)\n\n\n}\n\n\n//Output: \n\n\n//[a b c d]\n\n\n\n\n\n\n\nAdding a entry to a slice\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\npackage\n \nmain\n\n\n\nimport\n \nfmt\n\n\n\nfunc\n \nmain\n()\n \n{\n\n    \nletters\n \n:=\n \n[]\nstring\n{\na\n,\n \nb\n,\n \nc\n,\n \nd\n}\n\n    \nletters\n \n=\n \nappend\n(\nletters\n,\n \ne\n)\n\n    \nfmt\n.\nPrintln\n(\nletters\n)\n\n\n}\n\n\n//Output: \n\n\n//[a b c d e]\n\n\n\n\n\n\n\nGet the length of a slice\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\npackage\n \nmain\n\n\n\nimport\n \nfmt\n\n\n\nfunc\n \nmain\n()\n \n{\n\n    \nletters\n \n:=\n \n[]\nstring\n{\na\n,\n \nb\n,\n \nc\n,\n \nd\n}\n\n    \nfmt\n.\nPrintln\n(\nletters\n)\n\n    \nfmt\n.\nPrintln\n(\nLength of the slice is : \n,\n \nlen\n(\nletters\n))\n\n\n}\n\n\n//Output:\n\n\n//[a b c d]\n\n\n//Length of the slice is :  4\n\n\n\n\n\n\n\nCheck if a entry exists in a slice\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\npackage\n \nmain\n\n\n\nimport\n \nfmt\n\n\n\nfunc\n \nmain\n()\n \n{\n\n    \nletters\n \n:=\n \n[]\nstring\n{\na\n,\n \nb\n,\n \nc\n,\n \nd\n}\n\n    \nfmt\n.\nPrintln\n(\nentryExists\n(\nletters\n,\n \na\n))\n\n    \nfmt\n.\nPrintln\n(\nentryExists\n(\nletters\n,\n \ne\n))\n\n\n}\n\n\n\nfunc\n \nentryExists\n(\nslice\n \n[]\nstring\n,\n \nentry\n \nstring\n)\n \nbool\n{\n\n    \nfor\n \ni\n:=\n0\n;\n \ni\nlen\n(\nslice\n);\ni\n++\n{\n\n        \nif\n \nslice\n[\ni\n]\n \n==\n \nentry\n \n{\n\n            \nreturn\n \ntrue\n\n        \n}\n\n    \n}\n\n    \nreturn\n \nfalse\n\n\n}\n\n\n//Output:\n\n\n//true\n\n\n//false\n\n\n\n\n\n\n\nGet the index of a entry in a slice\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\npackage\n \nmain\n\n\n\nimport\n \nfmt\n\n\n\nfunc\n \nmain\n()\n \n{\n\n    \nletters\n \n:=\n \n[]\nstring\n{\na\n,\n \nb\n,\n \nc\n,\n \nd\n}\n\n    \nfmt\n.\nPrintln\n(\ngetSliceIndex\n(\nletters\n,\n \nc\n))\n\n    \nfmt\n.\nPrintln\n(\ngetSliceIndex\n(\nletters\n,\n \ne\n))\n\n\n}\n\n\n\nfunc\n \ngetSliceIndex\n(\nslice\n \n[]\nstring\n,\n \nentry\n \nstring\n)\n \nint\n{\n\n    \nfor\n \ni\n:=\n0\n;\n \ni\nlen\n(\nslice\n);\ni\n++\n{\n\n        \nif\n \nslice\n[\ni\n]\n \n==\n \nentry\n \n{\n\n            \nreturn\n \ni\n\n        \n}\n\n    \n}\n\n    \nreturn\n \n-\n1\n\n\n}\n\n\n//Output:\n\n\n//2\n\n\n//-1\n\n\n\n\n\n\n\nDelete a entry from a slice\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\npackage\n \nmain\n\n\n\nimport\n \n(\n\n    \nfmt\n\n    \nlog\n\n    \nos\n\n\n)\n\n\n\nfunc\n \nmain\n()\n \n{\n\n    \nletters\n \n:=\n \n[]\nstring\n{\na\n,\n \nb\n,\n \nc\n,\n \nd\n}\n\n    \nfmt\n.\nPrintln\n(\nremoveEntryFromSlice\n(\nletters\n,\n \nc\n))\n\n    \nfmt\n.\nPrintln\n(\nremoveEntryFromSlice\n(\nletters\n,\n \ne\n))\n\n\n}\n\n\n\nfunc\n \ngetSliceIndex\n(\nslice\n \n[]\nstring\n,\n \nentry\n \nstring\n)\n \nint\n{\n\n    \nfor\n \ni\n:=\n0\n;\n \ni\nlen\n(\nslice\n);\ni\n++\n{\n\n        \nif\n \nslice\n[\ni\n]\n \n==\n \nentry\n \n{\n\n            \nreturn\n \ni\n\n        \n}\n\n    \n}\n\n    \nreturn\n \n-\n1\n\n\n}\n\n\n\nfunc\n \nremoveEntryFromSlice\n(\nslice\n \n[]\nstring\n,\n \nentry\n \nstring\n)\n \n[]\nstring\n{\n\n    \ni\n \n:=\n \ngetSliceIndex\n(\nslice\n,\n \nentry\n)\n\n    \nif\n \ni\n \n==\n \n-\n1\n \n{\n\n        \nlog\n.\nPrintf\n(\nThe entry %s does not exist in the array\n,\n \nentry\n)\n\n        \nos\n.\nExit\n(\n1\n)\n\n    \n}\n\n    \nreturn\n \nappend\n(\nslice\n[:\ni\n],\n \nslice\n[\ni\n+\n1\n:]\n...\n)\n\n\n}\n\n\n//Output:\n\n\n//[a b d]\n\n\n//2018/03/20 21:18:42 The entry e does not exist in the array\n\n\n\n\n\n\n\nReference", 
            "title": "Slices in Go"
        }, 
        {
            "location": "/golang/slices/#slices-in-go", 
            "text": "", 
            "title": "Slices in Go"
        }, 
        {
            "location": "/golang/slices/#declaring-a-slice", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 package   main  import   fmt  func   main ()   { \n     letters   :=   [] string { a ,   b ,   c ,   d } \n     fmt . Println ( letters )  }  //Output:   //[a b c d]", 
            "title": "Declaring a slice"
        }, 
        {
            "location": "/golang/slices/#adding-a-entry-to-a-slice", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 package   main  import   fmt  func   main ()   { \n     letters   :=   [] string { a ,   b ,   c ,   d } \n     letters   =   append ( letters ,   e ) \n     fmt . Println ( letters )  }  //Output:   //[a b c d e]", 
            "title": "Adding a entry to a slice"
        }, 
        {
            "location": "/golang/slices/#get-the-length-of-a-slice", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 package   main  import   fmt  func   main ()   { \n     letters   :=   [] string { a ,   b ,   c ,   d } \n     fmt . Println ( letters ) \n     fmt . Println ( Length of the slice is :  ,   len ( letters ))  }  //Output:  //[a b c d]  //Length of the slice is :  4", 
            "title": "Get the length of a slice"
        }, 
        {
            "location": "/golang/slices/#check-if-a-entry-exists-in-a-slice", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21 package   main  import   fmt  func   main ()   { \n     letters   :=   [] string { a ,   b ,   c ,   d } \n     fmt . Println ( entryExists ( letters ,   a )) \n     fmt . Println ( entryExists ( letters ,   e ))  }  func   entryExists ( slice   [] string ,   entry   string )   bool { \n     for   i := 0 ;   i len ( slice ); i ++ { \n         if   slice [ i ]   ==   entry   { \n             return   true \n         } \n     } \n     return   false  }  //Output:  //true  //false", 
            "title": "Check if a entry exists in a slice"
        }, 
        {
            "location": "/golang/slices/#get-the-index-of-a-entry-in-a-slice", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21 package   main  import   fmt  func   main ()   { \n     letters   :=   [] string { a ,   b ,   c ,   d } \n     fmt . Println ( getSliceIndex ( letters ,   c )) \n     fmt . Println ( getSliceIndex ( letters ,   e ))  }  func   getSliceIndex ( slice   [] string ,   entry   string )   int { \n     for   i := 0 ;   i len ( slice ); i ++ { \n         if   slice [ i ]   ==   entry   { \n             return   i \n         } \n     } \n     return   - 1  }  //Output:  //2  //-1", 
            "title": "Get the index of a entry in a slice"
        }, 
        {
            "location": "/golang/slices/#delete-a-entry-from-a-slice", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34 package   main  import   ( \n     fmt \n     log \n     os  )  func   main ()   { \n     letters   :=   [] string { a ,   b ,   c ,   d } \n     fmt . Println ( removeEntryFromSlice ( letters ,   c )) \n     fmt . Println ( removeEntryFromSlice ( letters ,   e ))  }  func   getSliceIndex ( slice   [] string ,   entry   string )   int { \n     for   i := 0 ;   i len ( slice ); i ++ { \n         if   slice [ i ]   ==   entry   { \n             return   i \n         } \n     } \n     return   - 1  }  func   removeEntryFromSlice ( slice   [] string ,   entry   string )   [] string { \n     i   :=   getSliceIndex ( slice ,   entry ) \n     if   i   ==   - 1   { \n         log . Printf ( The entry %s does not exist in the array ,   entry ) \n         os . Exit ( 1 ) \n     } \n     return   append ( slice [: i ],   slice [ i + 1 :] ... )  }  //Output:  //[a b d]  //2018/03/20 21:18:42 The entry e does not exist in the array    Reference", 
            "title": "Delete a entry from a slice"
        }, 
        {
            "location": "/golang/modify-usage-flag/", 
            "text": "Modify Usage flag in go\n\n\nThis can be done by modifying flag.Usage function.\n\n\n1\n2\n3\nflag\n.\nUsage\n \n=\n \nfunc\n()\n \n{\n\n    \nfmt\n.\nFprintf\n(\nos\n.\nStderr\n,\n \nPrint the Usage.\\n\n)\n\n\n}\n\n\n\n\n\n\n\nExample code:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\npackage\n \nmain\n\n\n\nimport\n \n(\n\n    \nflag\n\n    \nfmt\n\n    \nos\n\n\n)\n\n\n\nfunc\n \nmain\n()\n \n{\n\n    \nPrintHelp\n()\n\n\n}\n\n\n\nfunc\n \nPrintHelp\n()\n \n{\n\n    \nhelpString\n \n:=\n \n`\n\n\n     Usage: ./nexus-repository-cli.exe [required prameters] [option] [parameters...]\n\n\n\n     [Required Prameters]:\n\n\n     -nexusUrl string\n\n\n         Nexus server URL (default \nhttp://localhost:8081/nexus\n)\n\n\n     -username string\n\n\n            Username for authentication\n\n\n      -password string\n\n\n            Password for authentication\n\n\n\n     [options]\n\n\n      -list\n\n\n            List the repositories in Nexus. Optional parameters: repoType, repoPolicy\n\n\n\n     [parameters]\n\n\n      -repoType string\n\n\n            Type of a repository. Possible values : hosted/proxy/group\n\n\n      -repoPolicy string\n\n\n            Policy of the hosted repository. Possible values : snapshot/release\n\n\n      -verbose\n\n\n            Set this flag for Debug logs.\n\n\n    `\n\n    \nflag\n.\nUsage\n \n=\n \nfunc\n()\n \n{\n\n        \nfmt\n.\nFprintf\n(\nos\n.\nStderr\n,\n \nhelpString\n)\n\n    \n}\n\n    \nflag\n.\nUsage\n()\n\n\n}", 
            "title": "Modify Go Usage flag"
        }, 
        {
            "location": "/golang/modify-usage-flag/#modify-usage-flag-in-go", 
            "text": "This can be done by modifying flag.Usage function.  1\n2\n3 flag . Usage   =   func ()   { \n     fmt . Fprintf ( os . Stderr ,   Print the Usage.\\n )  }    Example code:   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41 package   main  import   ( \n     flag \n     fmt \n     os  )  func   main ()   { \n     PrintHelp ()  }  func   PrintHelp ()   { \n     helpString   :=   `       Usage: ./nexus-repository-cli.exe [required prameters] [option] [parameters...]       [Required Prameters]:       -nexusUrl string           Nexus server URL (default  http://localhost:8081/nexus )       -username string              Username for authentication        -password string              Password for authentication       [options]        -list              List the repositories in Nexus. Optional parameters: repoType, repoPolicy       [parameters]        -repoType string              Type of a repository. Possible values : hosted/proxy/group        -repoPolicy string              Policy of the hosted repository. Possible values : snapshot/release        -verbose              Set this flag for Debug logs.      ` \n     flag . Usage   =   func ()   { \n         fmt . Fprintf ( os . Stderr ,   helpString ) \n     } \n     flag . Usage ()  }", 
            "title": "Modify Usage flag in go"
        }, 
        {
            "location": "/aws/", 
            "text": "Amazon Web Services\n\n\nAmazon Web Services (AWS) is a secure cloud services platform, offering compute power, database storage, content delivery and other functionality to help businesses scale and grow.", 
            "title": "Introduction"
        }, 
        {
            "location": "/aws/#amazon-web-services", 
            "text": "Amazon Web Services (AWS) is a secure cloud services platform, offering compute power, database storage, content delivery and other functionality to help businesses scale and grow.", 
            "title": "Amazon Web Services"
        }, 
        {
            "location": "/blog/cloud-computing/", 
            "text": "What is Cloud Computing?\n\n\nCloud computing is the on-demand delivery of compute power, database storage, applications, and other IT resources through a cloud services platform via the internet with pay-as-you-go pricing.\n\n\nCloud Computing Basics\n\n\nWhether you are running applications that share photos to millions of mobile users or you\u2019re supporting the critical operations of your business, a cloud services platform provides rapid access to flexible and low cost IT resources. With cloud computing, you don\u2019t need to make large upfront investments in hardware and spend a lot of time on the heavy lifting of managing that hardware. Instead, you can provision exactly the right type and size of computing resources you need to power your newest bright idea or operate your IT department. You can access as many resources as you need, almost instantly, and only pay for what you use.\n\n\nHow Does Cloud Computing Work?\n\n\nCloud computing provides a simple way to access servers, storage, databases and a broad set of application services over the Internet. A Cloud services platform such as Amazon Web Services owns and maintains the network-connected hardware required for these application services, while you provision and use what you need via a web application.\n\n\nAdvantages of cloud computing\n\n\nTrade capital expense for variable expense\n\n\nInstead of having to invest heavily in data centers and servers before you know how you\u2019re going to use them, you can only pay when you consume computing resources, and only pay for how much you consume.\n\n\nBenefit from massive economies of scale\n\n\nBy using cloud computing, you can achieve a lower variable cost than you can get on your own. Because usage from hundreds of thousands of customers are aggregated in the cloud, providers such as Amazon Web Services can achieve higher economies of scale which translates into lower pay as you go prices.\n\n\nStop guessing capacity\n\n\nEliminate guessing on your infrastructure capacity needs. When you make a capacity decision prior to deploying an application, you often either end up sitting on expensive idle resources or dealing with limited capacity. With cloud computing, these problems go away. You can access as much or as little as you need, and scale up and down as required with only a few minutes notice.\n\n\nIncrease speed and agility\n\n\nIn a cloud computing environment, new IT resources are only ever a click away, which means you reduce the time it takes to make those resources available to your developers from weeks to just minutes. This results in a dramatic increase in agility for the organization, since the cost and time it takes to experiment and develop is significantly lower.\n\n\nStop spending money on running and maintaining data centers\n\n\nFocus on projects that differentiate your business, not the infrastructure. Cloud computing lets you focus on your own customers, rather than on the heavy lifting of racking, stacking and powering servers.\n\n\nGo global in minutes\n\n\nEasily deploy your application in multiple regions around the world with just a few clicks. This means you can provide a lower latency and better experience for your customers simply and at minimal cost.", 
            "title": "Cloud Computing"
        }, 
        {
            "location": "/blog/cloud-computing/#what-is-cloud-computing", 
            "text": "Cloud computing is the on-demand delivery of compute power, database storage, applications, and other IT resources through a cloud services platform via the internet with pay-as-you-go pricing.", 
            "title": "What is Cloud Computing?"
        }, 
        {
            "location": "/blog/cloud-computing/#cloud-computing-basics", 
            "text": "Whether you are running applications that share photos to millions of mobile users or you\u2019re supporting the critical operations of your business, a cloud services platform provides rapid access to flexible and low cost IT resources. With cloud computing, you don\u2019t need to make large upfront investments in hardware and spend a lot of time on the heavy lifting of managing that hardware. Instead, you can provision exactly the right type and size of computing resources you need to power your newest bright idea or operate your IT department. You can access as many resources as you need, almost instantly, and only pay for what you use.", 
            "title": "Cloud Computing Basics"
        }, 
        {
            "location": "/blog/cloud-computing/#how-does-cloud-computing-work", 
            "text": "Cloud computing provides a simple way to access servers, storage, databases and a broad set of application services over the Internet. A Cloud services platform such as Amazon Web Services owns and maintains the network-connected hardware required for these application services, while you provision and use what you need via a web application.", 
            "title": "How Does Cloud Computing Work?"
        }, 
        {
            "location": "/blog/cloud-computing/#advantages-of-cloud-computing", 
            "text": "", 
            "title": "Advantages of cloud computing"
        }, 
        {
            "location": "/blog/cloud-computing/#trade-capital-expense-for-variable-expense", 
            "text": "Instead of having to invest heavily in data centers and servers before you know how you\u2019re going to use them, you can only pay when you consume computing resources, and only pay for how much you consume.", 
            "title": "Trade capital expense for variable expense"
        }, 
        {
            "location": "/blog/cloud-computing/#benefit-from-massive-economies-of-scale", 
            "text": "By using cloud computing, you can achieve a lower variable cost than you can get on your own. Because usage from hundreds of thousands of customers are aggregated in the cloud, providers such as Amazon Web Services can achieve higher economies of scale which translates into lower pay as you go prices.", 
            "title": "Benefit from massive economies of scale"
        }, 
        {
            "location": "/blog/cloud-computing/#stop-guessing-capacity", 
            "text": "Eliminate guessing on your infrastructure capacity needs. When you make a capacity decision prior to deploying an application, you often either end up sitting on expensive idle resources or dealing with limited capacity. With cloud computing, these problems go away. You can access as much or as little as you need, and scale up and down as required with only a few minutes notice.", 
            "title": "Stop guessing capacity"
        }, 
        {
            "location": "/blog/cloud-computing/#increase-speed-and-agility", 
            "text": "In a cloud computing environment, new IT resources are only ever a click away, which means you reduce the time it takes to make those resources available to your developers from weeks to just minutes. This results in a dramatic increase in agility for the organization, since the cost and time it takes to experiment and develop is significantly lower.", 
            "title": "Increase speed and agility"
        }, 
        {
            "location": "/blog/cloud-computing/#stop-spending-money-on-running-and-maintaining-data-centers", 
            "text": "Focus on projects that differentiate your business, not the infrastructure. Cloud computing lets you focus on your own customers, rather than on the heavy lifting of racking, stacking and powering servers.", 
            "title": "Stop spending money on running and maintaining data centers"
        }, 
        {
            "location": "/blog/cloud-computing/#go-global-in-minutes", 
            "text": "Easily deploy your application in multiple regions around the world with just a few clicks. This means you can provide a lower latency and better experience for your customers simply and at minimal cost.", 
            "title": "Go global in minutes"
        }, 
        {
            "location": "/aws/global-infrastructure/", 
            "text": "AWS Global Infrastructure\n\n\nUnderstanding the difference between a region, an Availabity Zone (AZ) and an Edge Location\n\n\nRegion\n\n\n\n\nA region is a physical location in the world which consists of two or more Availability Zones.\n\n\nA AWS region consists of an independent collection of AWS computing resources in a defined geography.\n\n\n\n\nAvailability Zones\n\n\n\n\nAn Availability Zone is one or more discrete data centers, each with redundant power, networking and connectivity, housed in separate facilities.\n\n\nAvalability zones are distinct locations from within a AWS region that are engineered to be isolated from failures.\n\n\n\n\nEdge Locations\n\n\n\n\nEdge locations are endpoints for AWS which are used for cacheing content. Typically it consists of CloudFront which is Amazon's Content Delivery Network (CDN)", 
            "title": "Global Infrastructure"
        }, 
        {
            "location": "/aws/global-infrastructure/#aws-global-infrastructure", 
            "text": "", 
            "title": "AWS Global Infrastructure"
        }, 
        {
            "location": "/aws/global-infrastructure/#understanding-the-difference-between-a-region-an-availabity-zone-az-and-an-edge-location", 
            "text": "", 
            "title": "Understanding the difference between a region, an Availabity Zone (AZ) and an Edge Location"
        }, 
        {
            "location": "/aws/global-infrastructure/#region", 
            "text": "A region is a physical location in the world which consists of two or more Availability Zones.  A AWS region consists of an independent collection of AWS computing resources in a defined geography.", 
            "title": "Region"
        }, 
        {
            "location": "/aws/global-infrastructure/#availability-zones", 
            "text": "An Availability Zone is one or more discrete data centers, each with redundant power, networking and connectivity, housed in separate facilities.  Avalability zones are distinct locations from within a AWS region that are engineered to be isolated from failures.", 
            "title": "Availability Zones"
        }, 
        {
            "location": "/aws/global-infrastructure/#edge-locations", 
            "text": "Edge locations are endpoints for AWS which are used for cacheing content. Typically it consists of CloudFront which is Amazon's Content Delivery Network (CDN)", 
            "title": "Edge Locations"
        }, 
        {
            "location": "/aws/services/security-identity-compliance/", 
            "text": "Security, Identity and Compliance\n\n\nIAM [Identity and Access management]\n\n\nIAM allows you to,\n\n\n\n\nmanage users and their level of access to the AWS console\n\n\nhave a centralised control of your AWS account\n\n\nhave shared access to your AWS account\n\n\nconfigure granular permissions\n\n\nhave identity Federation\n\n\nuse multifactor Authentication\n\n\nprovide temporary access for users/devices and services when necessary\n\n\nset up a custom password policies and password rotation policy\n\n\n\n\n\n\nInfo\n\n\nIAM supports PCI DSS Compliance\n\n\n\n\nIAM Management console\n\n\nThe IAM management console is global (its not specific to a region at this time) to the AWS environment and it allows you to manage all the benfits listed above. Its also list the security status of the AWS environment.\n\n\n\n\nCrtical terms\n\n\n\n\nUsers\n - End Users\n\n\nGroups\n - A collection of users under one set of permissions\n\n\nRoles\n - You create roles and can then assign them to AWS resources\n\n\nPolicies\n - A document that defines one or more permission. A policy can be attached to users, a group or a role. Policy documents are writen in JSON language.\n\n\nRoot account\n - Root account is simply the email ID you use to sing-up to the AWS account. Root account gives you unlimited amount of access to the AWS account.\n\n\n\n\n\n\nNote\n\n\nAlways setup MFA (Multifactor Authentication) on your root account\n\n\n\n\n\n\nInfo\n\n\nA role can be attached to a running EC2 instance via the AWS management UI or via the aws CLI\n\n\n\n\nWhen new users are created the have no access in the aws console. They are assigned a access key id, a secret key and a password (used for login in to the AWS console) which can be viewed only once during the creation of the user and cannot be retrived later, but can be generated again.\n\n\nThe access and security keys are not the same as the password and cannot be used to login to the AWS console. These are meant for accessing the AWS console from the API or a CLI. On the other hand a username and password cannot be used to access AWS via the API or CLI.\n\n\n\n\nInfo\n\n\nPower user role provides full access to AWS services and resources, but does not allow management of Users and groups.\n\n\n\n\nCognito\n\n\nIs a way of doing device authentication.Authenticate using FB, google etc.\n\n\nUse Cognito as a authentication service to get temporary access to certain AWS resources.\n\n\nGuardDuty\n\n\nIt monitors for malicious activities on your AWS environment.\n\n\nInspector\n\n\nInstalled on Virtual machines and EC2 instances to run a whole bunch of tests agains it to check for security vulnerabilities.\n\n\nCan be scheduled to run weekly, monthly, etc.\n\n\nGenerates a report and gives you a severity report of the vulnerabilities.\n\n\nMacie\n\n\nWill scan your s3 bucket to look for information that contain a personally identifiable information PII like names, adresses, passport numbers etc and alert you.\n\n\nCertificate Manager\n\n\nGet ssl certificates for free if your using AWS services and registering the domains through route 53.\n\n\nFor Managing SSL certificates.\n\n\nCloudHSM \u2013 Secure Key Storage and Cryptographic Operations\n\n\nHSM is short for Hardware Security Module. It is a piece of hardware \u2014 a dedicated appliance that provides secure key storage and a set of cryptographic operations within a tamper-resistant enclosure. You can store your keys within an HSM and use them to encrypt and decrypt data while keeping them safe and sound and under your full control. You are the only one with access to the keys stored in an HSM.\n\n\nEach of your CloudHSMs has an IP address within your Amazon Virtual Private Cloud (VPC). You\u2019ll receive administrator credentials for the appliance, allowing you to create and manage cryptographic keys, create user accounts, and perform cryptographic operations using those accounts. We do not have access to your keys; they remain under your control at all times. In Luna SA terminology, we have Admin credentials and you have both HSM Admin and HSM Partition Owner credentials.\n\n\nDedicated bits of hardware used to store your keys eg: private and public keys.\n\n\nThe keys may be used to access your EC2 instances. Can used these keys to encrypy AWS objects.\n\n\nDirectory Service\n\n\nA way of integration AD with AWS services.\n\n\nWAF : Web Application Firewall\n\n\nPrevents cross site scripting, SQL injections etc. Prevents malicious users.\n\n\nShield\n\n\nShield is basically DDOS mitigation.\n\n\nArtifact\n\n\nFor Audit and compliance. Way of downloading and inspecting Amazons documentations.", 
            "title": "Security, Identity and Compliance"
        }, 
        {
            "location": "/aws/services/security-identity-compliance/#security-identity-and-compliance", 
            "text": "", 
            "title": "Security, Identity and Compliance"
        }, 
        {
            "location": "/aws/services/security-identity-compliance/#iam-identity-and-access-management", 
            "text": "IAM allows you to,   manage users and their level of access to the AWS console  have a centralised control of your AWS account  have shared access to your AWS account  configure granular permissions  have identity Federation  use multifactor Authentication  provide temporary access for users/devices and services when necessary  set up a custom password policies and password rotation policy    Info  IAM supports PCI DSS Compliance", 
            "title": "IAM [Identity and Access management]"
        }, 
        {
            "location": "/aws/services/security-identity-compliance/#iam-management-console", 
            "text": "The IAM management console is global (its not specific to a region at this time) to the AWS environment and it allows you to manage all the benfits listed above. Its also list the security status of the AWS environment.", 
            "title": "IAM Management console"
        }, 
        {
            "location": "/aws/services/security-identity-compliance/#crtical-terms", 
            "text": "Users  - End Users  Groups  - A collection of users under one set of permissions  Roles  - You create roles and can then assign them to AWS resources  Policies  - A document that defines one or more permission. A policy can be attached to users, a group or a role. Policy documents are writen in JSON language.  Root account  - Root account is simply the email ID you use to sing-up to the AWS account. Root account gives you unlimited amount of access to the AWS account.    Note  Always setup MFA (Multifactor Authentication) on your root account    Info  A role can be attached to a running EC2 instance via the AWS management UI or via the aws CLI   When new users are created the have no access in the aws console. They are assigned a access key id, a secret key and a password (used for login in to the AWS console) which can be viewed only once during the creation of the user and cannot be retrived later, but can be generated again.  The access and security keys are not the same as the password and cannot be used to login to the AWS console. These are meant for accessing the AWS console from the API or a CLI. On the other hand a username and password cannot be used to access AWS via the API or CLI.   Info  Power user role provides full access to AWS services and resources, but does not allow management of Users and groups.", 
            "title": "Crtical terms"
        }, 
        {
            "location": "/aws/services/security-identity-compliance/#cognito", 
            "text": "Is a way of doing device authentication.Authenticate using FB, google etc.  Use Cognito as a authentication service to get temporary access to certain AWS resources.", 
            "title": "Cognito"
        }, 
        {
            "location": "/aws/services/security-identity-compliance/#guardduty", 
            "text": "It monitors for malicious activities on your AWS environment.", 
            "title": "GuardDuty"
        }, 
        {
            "location": "/aws/services/security-identity-compliance/#inspector", 
            "text": "Installed on Virtual machines and EC2 instances to run a whole bunch of tests agains it to check for security vulnerabilities.  Can be scheduled to run weekly, monthly, etc.  Generates a report and gives you a severity report of the vulnerabilities.", 
            "title": "Inspector"
        }, 
        {
            "location": "/aws/services/security-identity-compliance/#macie", 
            "text": "Will scan your s3 bucket to look for information that contain a personally identifiable information PII like names, adresses, passport numbers etc and alert you.", 
            "title": "Macie"
        }, 
        {
            "location": "/aws/services/security-identity-compliance/#certificate-manager", 
            "text": "Get ssl certificates for free if your using AWS services and registering the domains through route 53.  For Managing SSL certificates.", 
            "title": "Certificate Manager"
        }, 
        {
            "location": "/aws/services/security-identity-compliance/#cloudhsm-secure-key-storage-and-cryptographic-operations", 
            "text": "HSM is short for Hardware Security Module. It is a piece of hardware \u2014 a dedicated appliance that provides secure key storage and a set of cryptographic operations within a tamper-resistant enclosure. You can store your keys within an HSM and use them to encrypt and decrypt data while keeping them safe and sound and under your full control. You are the only one with access to the keys stored in an HSM.  Each of your CloudHSMs has an IP address within your Amazon Virtual Private Cloud (VPC). You\u2019ll receive administrator credentials for the appliance, allowing you to create and manage cryptographic keys, create user accounts, and perform cryptographic operations using those accounts. We do not have access to your keys; they remain under your control at all times. In Luna SA terminology, we have Admin credentials and you have both HSM Admin and HSM Partition Owner credentials.  Dedicated bits of hardware used to store your keys eg: private and public keys.  The keys may be used to access your EC2 instances. Can used these keys to encrypy AWS objects.", 
            "title": "CloudHSM \u2013 Secure Key Storage and Cryptographic Operations"
        }, 
        {
            "location": "/aws/services/security-identity-compliance/#directory-service", 
            "text": "A way of integration AD with AWS services.", 
            "title": "Directory Service"
        }, 
        {
            "location": "/aws/services/security-identity-compliance/#waf-web-application-firewall", 
            "text": "Prevents cross site scripting, SQL injections etc. Prevents malicious users.", 
            "title": "WAF : Web Application Firewall"
        }, 
        {
            "location": "/aws/services/security-identity-compliance/#shield", 
            "text": "Shield is basically DDOS mitigation.", 
            "title": "Shield"
        }, 
        {
            "location": "/aws/services/security-identity-compliance/#artifact", 
            "text": "For Audit and compliance. Way of downloading and inspecting Amazons documentations.", 
            "title": "Artifact"
        }, 
        {
            "location": "/aws/services/compute/", 
            "text": "Compute\n\n\nEC2 : Elastic Compute Cloud\n\n\nVirtual servers inside the AWS platform\n\n\nAmazon Elastic Compute Cloud (Amazon EC2) provides scalable computing capacity in the Amazon Web Services (AWS) cloud. Using Amazon EC2 eliminates your need to invest in hardware up front, so you can develop and deploy applications faster. You can use Amazon EC2 to launch as many or as few virtual servers as you need, configure security and networking, and manage storage. Amazon EC2 enables you to scale up or down to handle changes in requirements or spikes in popularity, reducing your need to forecast traffic.\n\n\nFeatures of Amazon EC2\n\n\nAmazon EC2 provides the following features:\n\n\nVirtual computing environments, known as instances\n\n\n\n\nPreconfigured templates for your instances, known as Amazon Machine Images (AMIs), that package the bits you need for your server (including the operating system and additional software)\n\n\nVarious configurations of CPU, memory, storage, and networking capacity for your instances, known as instance types\n\n\nSecure login information for your instances using key pairs (AWS stores the public key, and you store the private key in a secure place)\n\n\nStorage volumes for temporary data that's deleted when you stop or terminate your instance, known as instance store volumes\n\n\nPersistent storage volumes for your data using Amazon Elastic Block Store, known as Amazon EBS volumes\n\n\nMultiple physical locations for your resources, such as instances and Amazon EBS volumes, known as regions and Availability Zones\n\n\nA firewall that enables you to specify the protocols, ports, and source IP ranges that can reach your instances using security groups\n\n\nStatic IPv4 addresses for dynamic cloud computing, known as Elastic IP addresses\n\n\nMetadata, known as tags, that you can create and assign to your Amazon EC2 resources\n\n\nVirtual networks you can create that are logically isolated from the rest of the AWS cloud, and that you can optionally connect to your own network, known as virtual private clouds (VPCs)\n\n\n\n\nAmazon EC2 options\n\n\nOn Demand\n\n\n\n\nAllows you to pay a fixed rate by the hour (or by the second) with no commitment\n\n\nUsers that want the low cost and flexibility of Amazon EC2 without any upfront payment or long-term commitment\n\n\nFor applications with short term, spiky, or unpredictable workloads that cannot be interrupted and for applications being developed or tested\n\n\n\n\nReserved\n\n\n\n\nProvides you with a capacity reservation and offers a significant discount on the hourly charge for an instance. 1 year or 3 year terms\n\n\nFor applications with a steady state or predictable usage\n\n\nUsers able to make upfront payments to reduce their total Computing costs even further\n\n\n\n\nTypes of Reserved instances:\n\n\n\n\nStandard reserved instances (Upto 75% off on demands)\n\n\nConvertible reserved instances (Upto 54% off on demand) capability to change the attributes of the RI's as long as the exchange results in the creation of RI's of equal or greater value\n\n\nScheduled reserved instances available to launch within the time windows you reserve. This option allows you to match your capacity reservation to a predictable recurring schedule that only requires a fraction of a day, a week or a month.\n\n\n\n\nSpot\n\n\n\n\nEnables you to bid whatever price you want for instance capacity, providing for even greater savings if your applications have flixible start and end times\n\n\nFor applications that are only feasible at very low compute prices\n\n\n\n\nDedicated Hosts\n\n\n\n\nPhysical servers dedicated for your use. Dedicated Hosts can help you reduce costs by allowing you to use your existing server-bound software licenses\n\n\nUseful for regulatory requirements that may not support multi-tenant virtualization\n\n\nGreat for licensing which does not support multi-tenancy or cloud deployments\n\n\nCan be purchased On-Demand (hourly)\n\n\nCan be purchased as a reservation for upto 70% off on the On-Demand price\n\n\n\n\nEC2 instance types\n\n\nD2 Density\n\nR4 RAM\n\nM4 Main choice for general purpose apps\n\nC4 Compute\n\nG2 Graphics\n\nI2 IOPS\n\nF1 FPGA\n\nT2 Low cost general purpose apps\n\nP2 Graphics\n\nX1 Extreme memory  \n\n\n\n\nNote\n\n\n\n\none subnet = one availability zone (while creation of EC2 instance)\n\n\nEC2 instances and the EBS volumes should be in the same availability zone\n\n\nTo move EC2 instances from one region/availability zone to another create a image of the instance, move the image to the region and spin up a new instance using that image\n\n\n\n\n\n\nSecurity Groups\n\n\n\n\nA security group is a virtual firewall which controlls trafic to your EC2 instances\n\n\n1 instance can have multiple security groups\n\n\nIt is the first line of defense against hackers\n\n\nAll inbound traffic is blocked by default\n\n\nAny changes to the security group is applied immediately\n\n\nAs soon as you add an inbound rule allowing traffic in, that traffic is automatically allowed back out(Security Groups are STATEFUL)\n\n\nYou can specify allow rules but not deny rules\n\n\n\n\n\n\nInfo\n\n\nYou cannot block specific IP addresses using Security Groups, instead you need to use Network Access Control Lists\n\n\n\n\nEBS : Elastic Block Storage\n\n\nAmazon EBS allows you to create storage volumes and attach them to your EC2 instances. Once attached you can create a file system on top of these volumes, run a database, or use them in any other way you would use a block device. Amazon EBS volumes are placed in a specific Availability Zone, where they are automatically replicated to protect you from the failure of a single component.\n\n\n\n\nInfo\n\n\nYou cannot mount 1 EBS volume to multiple EC2 instances, instead use EFS\n\n\nTo move an EBS volume from one availability zone to another, create a snapshot or image of the volume and then you will be able to create a volume from the snapshot or image into another availability zone\n\n\nYou can change EBS volume sizes on the fly, including changing the size and storage type\n\n\n\n\nEBS Volume Types\n\n\nGeneral Purpose SSD (GP2)\n\n\n\n\nGeneral purpose, balances both price and performance\n\n\nUsed most often\n\n\nRatio of 3 IOPS per GB with up to 10000 IOPS and the ability to burst up to 3000 IOPS for extended periods of time for volumes at 3334 GB and above\n\n\n\n\nProvisioned IOPS SSD (IO1)\n\n\n\n\nDesigned for I/O intensive applications such as large relational or NoSQL databases\n\n\nUse if you need more than 10000 IOPS\n\n\nCan provision upto 20000 IOPS per volume\n\n\n\n\nThroughput Optimized HDD (ST1)\n\n\n\n\nTypically used for Big data, data warehouses or log processing\n\n\nLarge amounts of sequencial data\n\n\nOptimized throughput\n\n\nCannot be boot volumes\n\n\n\n\nCold HDD (SC1)\n\n\n\n\nLowest cost storage for infrequently accessed workloads\n\n\nFile server\n\n\nCannot be boot volumes\n\n\n\n\nMagnetic Standard\n\n\n\n\nLowest cost per GB of all the EBS volume types that is bootable\n\n\nMagnetic volumes are ideal for workloads where data is accessed infrequently and applications where the lowest storage cost is important\n\n\n\n\n\n\nIOPS\n\n\nInput/output operations per second (IOPS) is an input/output performance measurement used to characterize computer storage devices like hard disk drives (HDD), solid state drives (SSD), and storage area networks (SAN). Like benchmarks, IOPS numbers published by storage device manufacturers do not directly relate to real-world application performance.\n\n\n\n\n\n\nThroughput\n\n\nThe most common value from a disk manufacturer is how much throughput a certain disk can deliver. This number is usually expressed in Megabytes / Second (MB/s) and it is easy to belive that this would be the most important factor to look at. The maximum throughput for a disk could be for example 140 MB/s\n\n\n\n\nSnapshots\n\n\n\n\nVolumes exist in EBS and Snapshots are saved on S3 but you will not be able to see the bucket it is saved in\n\n\nSnapshots are point in time copies of Volumes\n\n\nSnapshots are incremental - this means that only the blocks that have changed since your last snapshot are moved to S3\n\n\nIt may take some time to create the first snapshot\n\n\nTo create a snapshot for Amazon EBS volumes that serve as root devices, you should stop the instance before taking the snapshot\n\n\nHowever you can take a snap while the instance is running\n\n\nYou can create AMI's from both Volumes and Snapshots\n\n\nSnapshots of encrypted volumes are encrypted automatically\n\n\nVolumes restored from encrypted snapshots are encrypted automatically\n\n\nYou can share snapshots, but only if they are unencrypted. These shapshots can be shared with other AWS accounts or made public. This is because the encryption keys are tried to your own AWS account\n\n\n\n\nRAID : Redundant Array of Independant Disks\n\n\nIf you do not create the required I/O from individual disks you can create a RAID\n\n\n\n\nRAID 0 - Striped, No redundancy, Good performance\n\n\nRAID 1 - Mirrored, Redundancy\n\n\nRAID 5 - Good for reads, bad for writes, AWS does not recommend ever putting RAID 5's on EBS\n\n\nRAID 10 - Striped and Mirrored, Good redundancy and good performance\n\n\n\n\n\n\nHow can I take a Snapshot of a RAID Array?\n\n\n\n\nStop the application from writing to the disk\n\n\n\n\nFlush all caches to the disk\n\n\n\n\n\n\nHow can we do this?\n\n\n\n\nFreeze the file system\n\n\nUnmount the RAID Array\n\n\nShut down the associated EC2 instance\n\n\n\n\n\n\n\n\n\n\nInstance Store Volumes\n\n\n\n\nInstance store volumes are sometimes called Ephemeral Storage\n\n\n\n\nEBS vs Instance Store\n\n\n\n\nAll AMIs are categorized as either backed by Amazon EBS or backed by instance store\n\n\nFor EBS volumes : The root device for an instance launched from the AMI is an Amazon EBS volume created from an Amazon EBS snapshot\n\n\nFor Instance Store Volumes : The root device for an instance launched from the AMI is an instance store volume created from a template stored in Amazon S3\n\n\nInstance store volumes cannot be stopped. If the underlying host fails, you will loose your data\n\n\nEBS backed instances can be stopped. You will not loose the data on this instance if it is stopped\n\n\nYou can reboot both, you will not loose your data\n\n\nBy default both root volumes will be deleted on termination of the instance, however with EBS volumes, you can tell AWS to keep the root device volume.\n\n\n\n\nELB : Elastic LoadBalancer\n\n\nElastic Load Balancing automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, and IP addresses. It can handle the varying load of your application traffic in a single Availability Zone or across multiple Availability Zones.\n\n\nElastic Loadbalacer types\n\n\nApplication Load Balancer\n\n\nApplication Load Balancer is best suited for load balancing of HTTP and HTTPS traffic and provides advanced request routing targeted at the delivery of modern application architectures, including microservices and containers. Operating at the individual request level (Layer 7), Application Load Balancer routes traffic to targets within Amazon Virtual Private Cloud (Amazon VPC) based on the content of the request.\n\n\nNetwork Load Balancer\n\n\nNetwork Load Balancer is best suited for load balancing of TCP traffic where extreme performance is required. Operating at the connection level (Layer 4), Network Load Balancer routes traffic to targets within Amazon Virtual Private Cloud (Amazon VPC) and is capable of handling millions of requests per second while maintaining ultra-low latencies. Network Load Balancer is also optimized to handle sudden and volatile traffic patterns.\n\n\nClassic Load Balancer\n\n\nClassic Load Balancer provides basic load balancing across multiple Amazon EC2 instances and operates at both the request level and connection level. Classic Load Balancer is intended for applications that were built within the EC2-Classic network.\n\n\n\n\nInfo\n\n\n\n\nInstances monitored by ELB are reported as InService or OutOfService\n\n\nHealth Checks check the instance health bu talking to it\n\n\nAn ELB does not have a IP addres, they only have a DNS name\n\n\n\n\n\n\nMetadata\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\ncurl http://169.254.169.254/latest/meta-data/\n\nOutput:\n\nami-id\nami-launch-index\nami-manifest-path\nblock-device-mapping/\nhostname\ninstance-action\ninstance-id\ninstance-type\nlocal-hostname\nlocal-ipv4\nmac\nmetrics/\nnetwork/\nplacement/\nprofile\npublic-hostname\npublic-ipv4\npublic-keys/\nreservation-id\nsecurity-groups\n\n\n\n\n\n\n\n\nMetadata examples\n\n\n1\n2\ncurl http://169.254.169.254/latest/meta-data/instance-id  \nOutput : i-06d52c4eb25a5ea90\n\n\n\n\n\n\n1\n2\ncurl http://169.254.169.254/latest/meta-data/hostname  \nOutput : ip-172-31-17-126.eu-west-1.compute.internal\n\n\n\n\n\n\n1\n2\ncurl http://169.254.169.254/latest/meta-data/security-groups  \nOutput : ps-basic-http\n\n\n\n\n\n\n\n\nECS : Elastic Container Services\n\n\nRun and manage docker containers\n\n\nAmazon Elastic Container Service (Amazon ECS) is a highly scalable, fast, container management service that makes it easy to run, stop, and manage Docker containers on a cluster.\n\n\nYou can host your cluster on a serverless infrastructure that is managed by Amazon ECS by launching your services or tasks using the Fargate launch type.\n\n\nEC2 Auto Scaling\n\n\nScale compute capacity to meet demands\n\n\nAWS Auto Scaling monitors your applications and automatically adjusts capacity to maintain steady, predictable performance at the lowest possible cost.\n\n\nUsing AWS Auto Scaling, you can setup scaling for multiple resources across multiple services in minutes. AWS Auto Scaling provides a simple, powerful user interface that lets you build scaling plans for Amazon EC2 instances\n\n\nElastic Beanstalk\n\n\nAmazon Web Services (AWS) comprises dozens of services, each of which exposes an area of functionality. While the variety of services offers flexibility for how you want to manage your AWS infrastructure, it can be challenging to figure out which services to use and how to provision them.\n\n\nWith Elastic Beanstalk, you can quickly deploy and manage applications in the AWS Cloud without worrying about the infrastructure that runs those applications. AWS Elastic Beanstalk reduces management complexity without restricting choice or control. You simply upload your application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring.\n\n\nFor people who do not want to understand how AWS works and only care about the functioning of their application.\n\n\nLambda\n\n\nRun you code in response to events.\n\n\nLightsail\n\n\nLaunch and manage virtual private servers (VPS).\n\n\nFor people who dont want to understand AWS and the underlying infrastructure. They dont want to know about VPC's and security groups.\n\n\nThis service will just provision you with a server, A fixed IP addrres for login and you can start working on it.\n\n\nComes with a management console for managing the server.\n\n\nBatch\n\n\nRun batch jobs at any scale.\nBatch computing on the cloud.", 
            "title": "Compute"
        }, 
        {
            "location": "/aws/services/compute/#compute", 
            "text": "", 
            "title": "Compute"
        }, 
        {
            "location": "/aws/services/compute/#ec2-elastic-compute-cloud", 
            "text": "Virtual servers inside the AWS platform  Amazon Elastic Compute Cloud (Amazon EC2) provides scalable computing capacity in the Amazon Web Services (AWS) cloud. Using Amazon EC2 eliminates your need to invest in hardware up front, so you can develop and deploy applications faster. You can use Amazon EC2 to launch as many or as few virtual servers as you need, configure security and networking, and manage storage. Amazon EC2 enables you to scale up or down to handle changes in requirements or spikes in popularity, reducing your need to forecast traffic.", 
            "title": "EC2 : Elastic Compute Cloud"
        }, 
        {
            "location": "/aws/services/compute/#features-of-amazon-ec2", 
            "text": "Amazon EC2 provides the following features:  Virtual computing environments, known as instances   Preconfigured templates for your instances, known as Amazon Machine Images (AMIs), that package the bits you need for your server (including the operating system and additional software)  Various configurations of CPU, memory, storage, and networking capacity for your instances, known as instance types  Secure login information for your instances using key pairs (AWS stores the public key, and you store the private key in a secure place)  Storage volumes for temporary data that's deleted when you stop or terminate your instance, known as instance store volumes  Persistent storage volumes for your data using Amazon Elastic Block Store, known as Amazon EBS volumes  Multiple physical locations for your resources, such as instances and Amazon EBS volumes, known as regions and Availability Zones  A firewall that enables you to specify the protocols, ports, and source IP ranges that can reach your instances using security groups  Static IPv4 addresses for dynamic cloud computing, known as Elastic IP addresses  Metadata, known as tags, that you can create and assign to your Amazon EC2 resources  Virtual networks you can create that are logically isolated from the rest of the AWS cloud, and that you can optionally connect to your own network, known as virtual private clouds (VPCs)", 
            "title": "Features of Amazon EC2"
        }, 
        {
            "location": "/aws/services/compute/#amazon-ec2-options", 
            "text": "", 
            "title": "Amazon EC2 options"
        }, 
        {
            "location": "/aws/services/compute/#on-demand", 
            "text": "Allows you to pay a fixed rate by the hour (or by the second) with no commitment  Users that want the low cost and flexibility of Amazon EC2 without any upfront payment or long-term commitment  For applications with short term, spiky, or unpredictable workloads that cannot be interrupted and for applications being developed or tested", 
            "title": "On Demand"
        }, 
        {
            "location": "/aws/services/compute/#reserved", 
            "text": "Provides you with a capacity reservation and offers a significant discount on the hourly charge for an instance. 1 year or 3 year terms  For applications with a steady state or predictable usage  Users able to make upfront payments to reduce their total Computing costs even further   Types of Reserved instances:   Standard reserved instances (Upto 75% off on demands)  Convertible reserved instances (Upto 54% off on demand) capability to change the attributes of the RI's as long as the exchange results in the creation of RI's of equal or greater value  Scheduled reserved instances available to launch within the time windows you reserve. This option allows you to match your capacity reservation to a predictable recurring schedule that only requires a fraction of a day, a week or a month.", 
            "title": "Reserved"
        }, 
        {
            "location": "/aws/services/compute/#spot", 
            "text": "Enables you to bid whatever price you want for instance capacity, providing for even greater savings if your applications have flixible start and end times  For applications that are only feasible at very low compute prices", 
            "title": "Spot"
        }, 
        {
            "location": "/aws/services/compute/#dedicated-hosts", 
            "text": "Physical servers dedicated for your use. Dedicated Hosts can help you reduce costs by allowing you to use your existing server-bound software licenses  Useful for regulatory requirements that may not support multi-tenant virtualization  Great for licensing which does not support multi-tenancy or cloud deployments  Can be purchased On-Demand (hourly)  Can be purchased as a reservation for upto 70% off on the On-Demand price", 
            "title": "Dedicated Hosts"
        }, 
        {
            "location": "/aws/services/compute/#ec2-instance-types", 
            "text": "D2 Density \nR4 RAM \nM4 Main choice for general purpose apps \nC4 Compute \nG2 Graphics \nI2 IOPS \nF1 FPGA \nT2 Low cost general purpose apps \nP2 Graphics \nX1 Extreme memory     Note   one subnet = one availability zone (while creation of EC2 instance)  EC2 instances and the EBS volumes should be in the same availability zone  To move EC2 instances from one region/availability zone to another create a image of the instance, move the image to the region and spin up a new instance using that image", 
            "title": "EC2 instance types"
        }, 
        {
            "location": "/aws/services/compute/#security-groups", 
            "text": "A security group is a virtual firewall which controlls trafic to your EC2 instances  1 instance can have multiple security groups  It is the first line of defense against hackers  All inbound traffic is blocked by default  Any changes to the security group is applied immediately  As soon as you add an inbound rule allowing traffic in, that traffic is automatically allowed back out(Security Groups are STATEFUL)  You can specify allow rules but not deny rules    Info  You cannot block specific IP addresses using Security Groups, instead you need to use Network Access Control Lists", 
            "title": "Security Groups"
        }, 
        {
            "location": "/aws/services/compute/#ebs-elastic-block-storage", 
            "text": "Amazon EBS allows you to create storage volumes and attach them to your EC2 instances. Once attached you can create a file system on top of these volumes, run a database, or use them in any other way you would use a block device. Amazon EBS volumes are placed in a specific Availability Zone, where they are automatically replicated to protect you from the failure of a single component.   Info  You cannot mount 1 EBS volume to multiple EC2 instances, instead use EFS  To move an EBS volume from one availability zone to another, create a snapshot or image of the volume and then you will be able to create a volume from the snapshot or image into another availability zone  You can change EBS volume sizes on the fly, including changing the size and storage type", 
            "title": "EBS : Elastic Block Storage"
        }, 
        {
            "location": "/aws/services/compute/#ebs-volume-types", 
            "text": "", 
            "title": "EBS Volume Types"
        }, 
        {
            "location": "/aws/services/compute/#general-purpose-ssd-gp2", 
            "text": "General purpose, balances both price and performance  Used most often  Ratio of 3 IOPS per GB with up to 10000 IOPS and the ability to burst up to 3000 IOPS for extended periods of time for volumes at 3334 GB and above", 
            "title": "General Purpose SSD (GP2)"
        }, 
        {
            "location": "/aws/services/compute/#provisioned-iops-ssd-io1", 
            "text": "Designed for I/O intensive applications such as large relational or NoSQL databases  Use if you need more than 10000 IOPS  Can provision upto 20000 IOPS per volume", 
            "title": "Provisioned IOPS SSD (IO1)"
        }, 
        {
            "location": "/aws/services/compute/#throughput-optimized-hdd-st1", 
            "text": "Typically used for Big data, data warehouses or log processing  Large amounts of sequencial data  Optimized throughput  Cannot be boot volumes", 
            "title": "Throughput Optimized HDD (ST1)"
        }, 
        {
            "location": "/aws/services/compute/#cold-hdd-sc1", 
            "text": "Lowest cost storage for infrequently accessed workloads  File server  Cannot be boot volumes", 
            "title": "Cold HDD (SC1)"
        }, 
        {
            "location": "/aws/services/compute/#magnetic-standard", 
            "text": "Lowest cost per GB of all the EBS volume types that is bootable  Magnetic volumes are ideal for workloads where data is accessed infrequently and applications where the lowest storage cost is important    IOPS  Input/output operations per second (IOPS) is an input/output performance measurement used to characterize computer storage devices like hard disk drives (HDD), solid state drives (SSD), and storage area networks (SAN). Like benchmarks, IOPS numbers published by storage device manufacturers do not directly relate to real-world application performance.    Throughput  The most common value from a disk manufacturer is how much throughput a certain disk can deliver. This number is usually expressed in Megabytes / Second (MB/s) and it is easy to belive that this would be the most important factor to look at. The maximum throughput for a disk could be for example 140 MB/s", 
            "title": "Magnetic Standard"
        }, 
        {
            "location": "/aws/services/compute/#snapshots", 
            "text": "Volumes exist in EBS and Snapshots are saved on S3 but you will not be able to see the bucket it is saved in  Snapshots are point in time copies of Volumes  Snapshots are incremental - this means that only the blocks that have changed since your last snapshot are moved to S3  It may take some time to create the first snapshot  To create a snapshot for Amazon EBS volumes that serve as root devices, you should stop the instance before taking the snapshot  However you can take a snap while the instance is running  You can create AMI's from both Volumes and Snapshots  Snapshots of encrypted volumes are encrypted automatically  Volumes restored from encrypted snapshots are encrypted automatically  You can share snapshots, but only if they are unencrypted. These shapshots can be shared with other AWS accounts or made public. This is because the encryption keys are tried to your own AWS account", 
            "title": "Snapshots"
        }, 
        {
            "location": "/aws/services/compute/#raid-redundant-array-of-independant-disks", 
            "text": "If you do not create the required I/O from individual disks you can create a RAID   RAID 0 - Striped, No redundancy, Good performance  RAID 1 - Mirrored, Redundancy  RAID 5 - Good for reads, bad for writes, AWS does not recommend ever putting RAID 5's on EBS  RAID 10 - Striped and Mirrored, Good redundancy and good performance    How can I take a Snapshot of a RAID Array?   Stop the application from writing to the disk   Flush all caches to the disk    How can we do this?   Freeze the file system  Unmount the RAID Array  Shut down the associated EC2 instance", 
            "title": "RAID : Redundant Array of Independant Disks"
        }, 
        {
            "location": "/aws/services/compute/#instance-store-volumes", 
            "text": "Instance store volumes are sometimes called Ephemeral Storage", 
            "title": "Instance Store Volumes"
        }, 
        {
            "location": "/aws/services/compute/#ebs-vs-instance-store", 
            "text": "All AMIs are categorized as either backed by Amazon EBS or backed by instance store  For EBS volumes : The root device for an instance launched from the AMI is an Amazon EBS volume created from an Amazon EBS snapshot  For Instance Store Volumes : The root device for an instance launched from the AMI is an instance store volume created from a template stored in Amazon S3  Instance store volumes cannot be stopped. If the underlying host fails, you will loose your data  EBS backed instances can be stopped. You will not loose the data on this instance if it is stopped  You can reboot both, you will not loose your data  By default both root volumes will be deleted on termination of the instance, however with EBS volumes, you can tell AWS to keep the root device volume.", 
            "title": "EBS vs Instance Store"
        }, 
        {
            "location": "/aws/services/compute/#elb-elastic-loadbalancer", 
            "text": "Elastic Load Balancing automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, and IP addresses. It can handle the varying load of your application traffic in a single Availability Zone or across multiple Availability Zones.", 
            "title": "ELB : Elastic LoadBalancer"
        }, 
        {
            "location": "/aws/services/compute/#elastic-loadbalacer-types", 
            "text": "", 
            "title": "Elastic Loadbalacer types"
        }, 
        {
            "location": "/aws/services/compute/#application-load-balancer", 
            "text": "Application Load Balancer is best suited for load balancing of HTTP and HTTPS traffic and provides advanced request routing targeted at the delivery of modern application architectures, including microservices and containers. Operating at the individual request level (Layer 7), Application Load Balancer routes traffic to targets within Amazon Virtual Private Cloud (Amazon VPC) based on the content of the request.", 
            "title": "Application Load Balancer"
        }, 
        {
            "location": "/aws/services/compute/#network-load-balancer", 
            "text": "Network Load Balancer is best suited for load balancing of TCP traffic where extreme performance is required. Operating at the connection level (Layer 4), Network Load Balancer routes traffic to targets within Amazon Virtual Private Cloud (Amazon VPC) and is capable of handling millions of requests per second while maintaining ultra-low latencies. Network Load Balancer is also optimized to handle sudden and volatile traffic patterns.", 
            "title": "Network Load Balancer"
        }, 
        {
            "location": "/aws/services/compute/#classic-load-balancer", 
            "text": "Classic Load Balancer provides basic load balancing across multiple Amazon EC2 instances and operates at both the request level and connection level. Classic Load Balancer is intended for applications that were built within the EC2-Classic network.   Info   Instances monitored by ELB are reported as InService or OutOfService  Health Checks check the instance health bu talking to it  An ELB does not have a IP addres, they only have a DNS name", 
            "title": "Classic Load Balancer"
        }, 
        {
            "location": "/aws/services/compute/#metadata", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24 curl http://169.254.169.254/latest/meta-data/\n\nOutput:\n\nami-id\nami-launch-index\nami-manifest-path\nblock-device-mapping/\nhostname\ninstance-action\ninstance-id\ninstance-type\nlocal-hostname\nlocal-ipv4\nmac\nmetrics/\nnetwork/\nplacement/\nprofile\npublic-hostname\npublic-ipv4\npublic-keys/\nreservation-id\nsecurity-groups    Metadata examples  1\n2 curl http://169.254.169.254/latest/meta-data/instance-id  \nOutput : i-06d52c4eb25a5ea90   1\n2 curl http://169.254.169.254/latest/meta-data/hostname  \nOutput : ip-172-31-17-126.eu-west-1.compute.internal   1\n2 curl http://169.254.169.254/latest/meta-data/security-groups  \nOutput : ps-basic-http", 
            "title": "Metadata"
        }, 
        {
            "location": "/aws/services/compute/#ecs-elastic-container-services", 
            "text": "Run and manage docker containers  Amazon Elastic Container Service (Amazon ECS) is a highly scalable, fast, container management service that makes it easy to run, stop, and manage Docker containers on a cluster.  You can host your cluster on a serverless infrastructure that is managed by Amazon ECS by launching your services or tasks using the Fargate launch type.", 
            "title": "ECS : Elastic Container Services"
        }, 
        {
            "location": "/aws/services/compute/#ec2-auto-scaling", 
            "text": "Scale compute capacity to meet demands  AWS Auto Scaling monitors your applications and automatically adjusts capacity to maintain steady, predictable performance at the lowest possible cost.  Using AWS Auto Scaling, you can setup scaling for multiple resources across multiple services in minutes. AWS Auto Scaling provides a simple, powerful user interface that lets you build scaling plans for Amazon EC2 instances", 
            "title": "EC2 Auto Scaling"
        }, 
        {
            "location": "/aws/services/compute/#elastic-beanstalk", 
            "text": "Amazon Web Services (AWS) comprises dozens of services, each of which exposes an area of functionality. While the variety of services offers flexibility for how you want to manage your AWS infrastructure, it can be challenging to figure out which services to use and how to provision them.  With Elastic Beanstalk, you can quickly deploy and manage applications in the AWS Cloud without worrying about the infrastructure that runs those applications. AWS Elastic Beanstalk reduces management complexity without restricting choice or control. You simply upload your application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring.  For people who do not want to understand how AWS works and only care about the functioning of their application.", 
            "title": "Elastic Beanstalk"
        }, 
        {
            "location": "/aws/services/compute/#lambda", 
            "text": "Run you code in response to events.", 
            "title": "Lambda"
        }, 
        {
            "location": "/aws/services/compute/#lightsail", 
            "text": "Launch and manage virtual private servers (VPS).  For people who dont want to understand AWS and the underlying infrastructure. They dont want to know about VPC's and security groups.  This service will just provision you with a server, A fixed IP addrres for login and you can start working on it.  Comes with a management console for managing the server.", 
            "title": "Lightsail"
        }, 
        {
            "location": "/aws/services/compute/#batch", 
            "text": "Run batch jobs at any scale.\nBatch computing on the cloud.", 
            "title": "Batch"
        }, 
        {
            "location": "/aws/services/storage/", 
            "text": "Storage\n\n\nS3 [Simple storage service]\n\n\nAmazon S3 is one of the oldest storages service on AWS and it provides access to secure, durable,reliable, fast, inexpensive and highly-scalable data storage infrastructure.\n\n\nS3 Basics\n\n\n\n\nS3 is a safe place to store your files\n\n\nIt is a object based storage i.e. it allows you to upload files. Example: text files, videos, images, word files etc\n\n\nThe data is spread across multiple devices and facilities\n\n\nFiles can be from 0 Bytes to 5 TB. The largest object that can be uploaded in a single PUT is 5 gigabytes\n\n\nThere is unlimited storage\n\n\nFiles are stored in buckets\n\n\nS3 is a universal namespace, that is, bucket names must be globally unique\n\n\nBuilt for 99.99% availability\n\n\nSupports versioning\n\n\nSupports encryption\n\n\nData can be secured using Access Control Lists and Bucket Policies\n\n\nYou can load files to the S3 bucket much faster by enabling multipart upload\n\n\n\n\n\n\nInfo\n\n\nS3 is object based, it is not suitable to install a operating system or a databse on it\n\n\nSuccessful uploads will generate a HTTP 200 status code\n\n\nBy default buckets are private and all objects stored inside them are private\n\n\n\n\n\n\nHow does a S3 bucket URL look like?\n\n\nhttps://s3-{region}.amazonaws.com/{bucket-name}\n\n\n\n\nData consistency Model\n\n\n\n\n\n\nRead after Write consistency for PUTS of new Objects. You will be able to read the data as soon as you upload it.\n\n\n\n\n\n\nEventual Consistency for overwrite PUTS and DELETES (can take some time to propagate). Thus if you try to read updated data you either get the new data or the old data but the data is never currupted or incosistent.\n\n\n\n\n\n\nS3 is a simple key, value store\n\n\nS3 is Object based and Objects consists of the following:\n\n\n1\n2\n3\n4\n5\n6\n7\nKey\n \n:\n \nThe name of the object\n\n\nValue\n \n:\n \nThe data which is made up of a sequence of bytes\n\n\nVersion ID\n \n:\n \nfor versioning\n\n\nMetadata\n \n:\n \nData about the data you are storing\n\n\nSubresources\n\n\n    Access Control List\n \n:\n \nWho can access this object. Allows you to do fine grain permission\n\n\n    Torrent\n \n:\n \nSupport for bit torrent protocol\n\n\n\n\n\n\n\nStorage Tiers/Classes\n\n\nS3 - Standard\n\n\n99.99% availability, 99.(11 x 9's)% durability, stored redundantly across multiple devices in multiple facilities and is designed to sustain the loss of 2 facilities concurrently.\n\n\nS3 - IA (Infrequently Accessed)\n\n\nFor data that is accessed less frequently, but requires rapid access when needed. Lower fee than S3, but you are charged a retrieval fee (Per GB retrieved).\n\n\nReduced Redundancy Storage [RRS]\n\n\nDesigned to provide 99.99% availability and 99.99% durability of objects over a given year. Durability is less than S3 and cost is much lower than S3. Can be used to keep files that can be easily regenerated.\n\n\n\n\nExample\n\n\nWe can store images in an S3 bucket and the thumbnails in an RRS bucket. In the event the thunbnails are lost, they can be easily regenerated.\n\n\n\n\n\n\nSummary\n\n\n\n\n\n\nGlacier\n\n\nGlacier is an entremely low-cost storage service for data archival. It stores data for as little as $0.01 per gigabyte per month and is optimised for data that is infrequently accessed and for which retrieval times of 3 - 5 hours are suitable.\n\n\n\n\nInfo\n\n\nThe Glacier storage class is designed for data that is retained for more than 90 days. Objects archived to Glacier storage class incur costs for atleast 90 days of storage even if they are deleted or overwritten earlier.\n\n\n\n\n\n\nS3 vs Glacier\n\n\n\n\n\n\nS3 - Security\n\n\n\n\nBy default, all newly created buckets are PRIVATE\n\n\nYou can setup access control using:\n\n\nBucket Policies\n\n\nAccess Control Lists\n\n\n\n\n\n\nS3 buckets can be configured to create access logs, which log all requests made to the S3 bucket. These access logs can be stored to another bucket.\n\n\n\n\nS3 - Encryption\n\n\n\n\n\n\nIn Transit:\n\n\n\n\nSSL/TLS\n\n\n\n\n\n\n\n\nAt Rest:\n\n\n\n\nClient side encryption\n\n\nServer side encryption -\n\n\nwith Amazon S3 managed keys (SSE-S3)\n\n\nwith KMS (Key Management Service) (SSE-KMS)\n\n\nwith customer provided keys (SSE-C)\n\n\n\n\n\n\n\n\n\n\n\n\nS3 - Versioning\n\n\n\n\nStores all versions of an object (including all writes and even if you delete an object)\n\n\nGreat backup tool\n\n\nOnce enabled versioning cannot be disabled, only suspended\n\n\nIntegrates with lifecycle rules\n\n\nVersioning's MFA delete capability, which uses multi-factor authentication, can be used to provide an additional layer of security\n\n\nYou pay for each version of the object.\n\n\n\n\nS3 - Lifecycle Rules\n\n\nLifecycle rules will help you manage your storage costs by controlling the lifecycle of your objects. Create Lifecycle rules to automatically transition your objects to the S3-IA storage class, archive them to Glacier storage class and remove them after a specified time period.\n\n\n\n\nCan be used in conjunction with versioning. Versioning can be turned on or off.\n\n\nCan be applied to current versions and previous versions.\n\n\nYou can setup the following options using Lifecycle rules:\n\n\nTransition an object to the Standard - Infrequent Access storage class (128Kb and 30 days after the creation date)\n\n\nArchive to Glacier storage class (30 days after IA, if relevant)\n\n\nPermanently delete\n\n\n\n\n\n\n\n\nS3 - Cross Region Replication\n\n\n\n\nVersioning should be enabled on both the source and destination buckets\n\n\nRegions must be unique\n\n\nFiles in an existing bucket are not replicated automatically. All subsequent updated files will be replicated automatically\n\n\nYou cannot replicate to multiple buckets or use daisy chaining (at this time)\n\n\nDelete markers are replicated\n\n\nDeleting individual versions or delete markers will not be replicated.\n\n\n\n\nS3 - Static Websites\n\n\n\n\nYou can use S3 to host static websites\n\n\nServerless\n\n\nVery cheap, scales automatically\n\n\nStatic only, cannot host dynamic sites.\n\n\n\n\n\n\nHow does a S3 static website hosting URL look like?\n\n\nhttps://{bucket-name}.s3-website-{region}.amazonaws.com\n\n\n\n\nS3 - Charges\n\n\nA S3 bucket is charged for:\n\n\n\n\nStorage\n\n\nRequests\n\n\nStorage Management Pricing - tagging data to track costs against a criteria\n\n\nData Transfer Pricing - moving data within/between the S3 buckets\n\n\nTransfer accelaration\n\n\n\n\n\n\nWhat is transfer accelaration?\n\n\nAmazon S3 transfer accelaration enables fast, easy and secure transfers of files over long distances between the end users and the S3 bucket.\n\n\nTransfer accelaration takes advantage of Amazons CloudFront's globally distributed edge locations. As the data arrives at an edge location, data is routed to Amazon S3 over an optimized network path.\n\n\n\n\n\n\n\n\nFaq\n\n\nAmazon S3 FAQ\n\n\n\n\nSnowball\n\n\nMove GB's of data to the Amazon data center without using broadband line or wifi. Write it phisically to a disk which is then moved to the data center of AWS.\n\n\nSnowball can:\n\n\n\n\nImport to S3\n\n\nExport from S3\n\n\n\n\nThere are three type of snowballs:\n\n\n\n\nSnowball - 80 TB Storage capacity\n\n\nSnowball Edge - 100 TB Storage + Compute capacity (mini version of an AWS datacenter in a box)\n\n\nSnowmobile - 100 PB per snow mobile\n\n\n\n\n\n\nInfo\n\n\nimport/export service allows you to send your data in a disk to Amazon to be loaded into your S3 bucket.\n\n\n\n\nStorage gateway\n\n\nAWS Storage Gateway is a hybrid storage service that enables your on-premises applications to seamlessly use AWS cloud storage. You can use the service for backup and archiving, disaster recovery, cloud bursting, storage tiering, and migration. Your applications connect to the service through a gateway appliance using standard storage protocols,such as NFS and iSCSI.\n\n\nStorage Gateway Types\n\n\nFile Gateway\n\n\nFiles are stored as objects in your S3 buckets, accessed through a Network File System (NFS) mount point.\nOwnership, permissions and timestamps are stored in S3 metadata of the object associated with the file.\n\n\nFor flat files, stored directly on S3.\n\n\n\n\nVolume Gateway\n\n\nThe volume interface presents your application with disk volumes using the iSCSI block protocol.\n\n\nData written to these volumes can be asynchronously backed up as point-in-time snapshots of your volume, and stored in the cloud as an Amazon EBS snapshots.\n\n\nSnapshots are incremental backups that capture only changes blocks. All snapshot storage is also compressed to minimize your storage charges.\n\n\nStored Volumes\n\n\nEntire dataset is stored on site and is asynchronously backed up to S3. The size of stored volumes can be 1 GB to 16 TB.\n\n\nCached Volumes\n\n\nEntire dataset is stored on S3 and the most frequently accessed data is cached on-site. The size of cached volumes can be 1 GB to 32 TB.\n\n\nTape Gateway (VTL)\n\n\nUsed for backup and uses popular applications like NetBackup, Backup Exec, Veeam etc.\n\n\n\n\nVTL - Virtual Tape Library\n\n\n\n\nEFS [Elastic file system]\n\n\nAmazon EFS provides scalable file storage for use with Amazon EC2. You can create an EFS file system and configure your instances to mount the file system. You can use an EFS file system as a common data source for workloads and applications running on multiple instances.\n\n\nBasically Network Attached storage. Can mount them to multiple virtual machines.", 
            "title": "Storage"
        }, 
        {
            "location": "/aws/services/storage/#storage", 
            "text": "", 
            "title": "Storage"
        }, 
        {
            "location": "/aws/services/storage/#s3-simple-storage-service", 
            "text": "Amazon S3 is one of the oldest storages service on AWS and it provides access to secure, durable,reliable, fast, inexpensive and highly-scalable data storage infrastructure.", 
            "title": "S3 [Simple storage service]"
        }, 
        {
            "location": "/aws/services/storage/#s3-basics", 
            "text": "S3 is a safe place to store your files  It is a object based storage i.e. it allows you to upload files. Example: text files, videos, images, word files etc  The data is spread across multiple devices and facilities  Files can be from 0 Bytes to 5 TB. The largest object that can be uploaded in a single PUT is 5 gigabytes  There is unlimited storage  Files are stored in buckets  S3 is a universal namespace, that is, bucket names must be globally unique  Built for 99.99% availability  Supports versioning  Supports encryption  Data can be secured using Access Control Lists and Bucket Policies  You can load files to the S3 bucket much faster by enabling multipart upload    Info  S3 is object based, it is not suitable to install a operating system or a databse on it  Successful uploads will generate a HTTP 200 status code  By default buckets are private and all objects stored inside them are private    How does a S3 bucket URL look like?  https://s3-{region}.amazonaws.com/{bucket-name}", 
            "title": "S3 Basics"
        }, 
        {
            "location": "/aws/services/storage/#data-consistency-model", 
            "text": "Read after Write consistency for PUTS of new Objects. You will be able to read the data as soon as you upload it.    Eventual Consistency for overwrite PUTS and DELETES (can take some time to propagate). Thus if you try to read updated data you either get the new data or the old data but the data is never currupted or incosistent.", 
            "title": "Data consistency Model"
        }, 
        {
            "location": "/aws/services/storage/#s3-is-a-simple-key-value-store", 
            "text": "S3 is Object based and Objects consists of the following:  1\n2\n3\n4\n5\n6\n7 Key   :   The name of the object  Value   :   The data which is made up of a sequence of bytes  Version ID   :   for versioning  Metadata   :   Data about the data you are storing  Subresources      Access Control List   :   Who can access this object. Allows you to do fine grain permission      Torrent   :   Support for bit torrent protocol", 
            "title": "S3 is a simple key, value store"
        }, 
        {
            "location": "/aws/services/storage/#storage-tiersclasses", 
            "text": "", 
            "title": "Storage Tiers/Classes"
        }, 
        {
            "location": "/aws/services/storage/#s3-standard", 
            "text": "99.99% availability, 99.(11 x 9's)% durability, stored redundantly across multiple devices in multiple facilities and is designed to sustain the loss of 2 facilities concurrently.", 
            "title": "S3 - Standard"
        }, 
        {
            "location": "/aws/services/storage/#s3-ia-infrequently-accessed", 
            "text": "For data that is accessed less frequently, but requires rapid access when needed. Lower fee than S3, but you are charged a retrieval fee (Per GB retrieved).", 
            "title": "S3 - IA (Infrequently Accessed)"
        }, 
        {
            "location": "/aws/services/storage/#reduced-redundancy-storage-rrs", 
            "text": "Designed to provide 99.99% availability and 99.99% durability of objects over a given year. Durability is less than S3 and cost is much lower than S3. Can be used to keep files that can be easily regenerated.   Example  We can store images in an S3 bucket and the thumbnails in an RRS bucket. In the event the thunbnails are lost, they can be easily regenerated.    Summary", 
            "title": "Reduced Redundancy Storage [RRS]"
        }, 
        {
            "location": "/aws/services/storage/#glacier", 
            "text": "Glacier is an entremely low-cost storage service for data archival. It stores data for as little as $0.01 per gigabyte per month and is optimised for data that is infrequently accessed and for which retrieval times of 3 - 5 hours are suitable.   Info  The Glacier storage class is designed for data that is retained for more than 90 days. Objects archived to Glacier storage class incur costs for atleast 90 days of storage even if they are deleted or overwritten earlier.    S3 vs Glacier", 
            "title": "Glacier"
        }, 
        {
            "location": "/aws/services/storage/#s3-security", 
            "text": "By default, all newly created buckets are PRIVATE  You can setup access control using:  Bucket Policies  Access Control Lists    S3 buckets can be configured to create access logs, which log all requests made to the S3 bucket. These access logs can be stored to another bucket.", 
            "title": "S3 - Security"
        }, 
        {
            "location": "/aws/services/storage/#s3-encryption", 
            "text": "In Transit:   SSL/TLS     At Rest:   Client side encryption  Server side encryption -  with Amazon S3 managed keys (SSE-S3)  with KMS (Key Management Service) (SSE-KMS)  with customer provided keys (SSE-C)", 
            "title": "S3 - Encryption"
        }, 
        {
            "location": "/aws/services/storage/#s3-versioning", 
            "text": "Stores all versions of an object (including all writes and even if you delete an object)  Great backup tool  Once enabled versioning cannot be disabled, only suspended  Integrates with lifecycle rules  Versioning's MFA delete capability, which uses multi-factor authentication, can be used to provide an additional layer of security  You pay for each version of the object.", 
            "title": "S3 - Versioning"
        }, 
        {
            "location": "/aws/services/storage/#s3-lifecycle-rules", 
            "text": "Lifecycle rules will help you manage your storage costs by controlling the lifecycle of your objects. Create Lifecycle rules to automatically transition your objects to the S3-IA storage class, archive them to Glacier storage class and remove them after a specified time period.   Can be used in conjunction with versioning. Versioning can be turned on or off.  Can be applied to current versions and previous versions.  You can setup the following options using Lifecycle rules:  Transition an object to the Standard - Infrequent Access storage class (128Kb and 30 days after the creation date)  Archive to Glacier storage class (30 days after IA, if relevant)  Permanently delete", 
            "title": "S3 - Lifecycle Rules"
        }, 
        {
            "location": "/aws/services/storage/#s3-cross-region-replication", 
            "text": "Versioning should be enabled on both the source and destination buckets  Regions must be unique  Files in an existing bucket are not replicated automatically. All subsequent updated files will be replicated automatically  You cannot replicate to multiple buckets or use daisy chaining (at this time)  Delete markers are replicated  Deleting individual versions or delete markers will not be replicated.", 
            "title": "S3 - Cross Region Replication"
        }, 
        {
            "location": "/aws/services/storage/#s3-static-websites", 
            "text": "You can use S3 to host static websites  Serverless  Very cheap, scales automatically  Static only, cannot host dynamic sites.    How does a S3 static website hosting URL look like?  https://{bucket-name}.s3-website-{region}.amazonaws.com", 
            "title": "S3 - Static Websites"
        }, 
        {
            "location": "/aws/services/storage/#s3-charges", 
            "text": "A S3 bucket is charged for:   Storage  Requests  Storage Management Pricing - tagging data to track costs against a criteria  Data Transfer Pricing - moving data within/between the S3 buckets  Transfer accelaration    What is transfer accelaration?  Amazon S3 transfer accelaration enables fast, easy and secure transfers of files over long distances between the end users and the S3 bucket.  Transfer accelaration takes advantage of Amazons CloudFront's globally distributed edge locations. As the data arrives at an edge location, data is routed to Amazon S3 over an optimized network path.     Faq  Amazon S3 FAQ", 
            "title": "S3 - Charges"
        }, 
        {
            "location": "/aws/services/storage/#snowball", 
            "text": "Move GB's of data to the Amazon data center without using broadband line or wifi. Write it phisically to a disk which is then moved to the data center of AWS.  Snowball can:   Import to S3  Export from S3   There are three type of snowballs:   Snowball - 80 TB Storage capacity  Snowball Edge - 100 TB Storage + Compute capacity (mini version of an AWS datacenter in a box)  Snowmobile - 100 PB per snow mobile    Info  import/export service allows you to send your data in a disk to Amazon to be loaded into your S3 bucket.", 
            "title": "Snowball"
        }, 
        {
            "location": "/aws/services/storage/#storage-gateway", 
            "text": "AWS Storage Gateway is a hybrid storage service that enables your on-premises applications to seamlessly use AWS cloud storage. You can use the service for backup and archiving, disaster recovery, cloud bursting, storage tiering, and migration. Your applications connect to the service through a gateway appliance using standard storage protocols,such as NFS and iSCSI.", 
            "title": "Storage gateway"
        }, 
        {
            "location": "/aws/services/storage/#storage-gateway-types", 
            "text": "", 
            "title": "Storage Gateway Types"
        }, 
        {
            "location": "/aws/services/storage/#file-gateway", 
            "text": "Files are stored as objects in your S3 buckets, accessed through a Network File System (NFS) mount point.\nOwnership, permissions and timestamps are stored in S3 metadata of the object associated with the file.  For flat files, stored directly on S3.", 
            "title": "File Gateway"
        }, 
        {
            "location": "/aws/services/storage/#volume-gateway", 
            "text": "The volume interface presents your application with disk volumes using the iSCSI block protocol.  Data written to these volumes can be asynchronously backed up as point-in-time snapshots of your volume, and stored in the cloud as an Amazon EBS snapshots.  Snapshots are incremental backups that capture only changes blocks. All snapshot storage is also compressed to minimize your storage charges.", 
            "title": "Volume Gateway"
        }, 
        {
            "location": "/aws/services/storage/#stored-volumes", 
            "text": "Entire dataset is stored on site and is asynchronously backed up to S3. The size of stored volumes can be 1 GB to 16 TB.", 
            "title": "Stored Volumes"
        }, 
        {
            "location": "/aws/services/storage/#cached-volumes", 
            "text": "Entire dataset is stored on S3 and the most frequently accessed data is cached on-site. The size of cached volumes can be 1 GB to 32 TB.", 
            "title": "Cached Volumes"
        }, 
        {
            "location": "/aws/services/storage/#tape-gateway-vtl", 
            "text": "Used for backup and uses popular applications like NetBackup, Backup Exec, Veeam etc.   VTL - Virtual Tape Library", 
            "title": "Tape Gateway (VTL)"
        }, 
        {
            "location": "/aws/services/storage/#efs-elastic-file-system", 
            "text": "Amazon EFS provides scalable file storage for use with Amazon EC2. You can create an EFS file system and configure your instances to mount the file system. You can use an EFS file system as a common data source for workloads and applications running on multiple instances.  Basically Network Attached storage. Can mount them to multiple virtual machines.", 
            "title": "EFS [Elastic file system]"
        }, 
        {
            "location": "/aws/services/networking-and-content-delivery/", 
            "text": "Networking and content delivery\n\n\nVPC : Virtual Private Cloud\n\n\nAmazon Virtual private cloud. Can be considered as a virtual datacenter.\n\n\nAmazon Virtual Private Cloud (Amazon VPC) enables you to launch AWS resources into a virtual network that you've defined. This virtual network closely resembles a traditional network that you'd operate in your own data center, with the benefits of using the scalable infrastructure of AWS.\n\n\nConfigure firewall, availability zones, network sider address ranges,  Network ACL's, route tables etc.\n\n\nCloudFront\n\n\nAmazons content delivery network service. A Content Delivery Network (CDN) is a system of distributed servers (network) that deliver webpages and other web content to a user base on the geogra[hic locations of the user, the origin of the webpage and a content delivery server.\n\n\n\n\nWhat is CDN?\n\n\n\n\n\n\nAmazons CloudFront can be used to deliver your entire website including dynamic, static, streaming and ineractive content using a global network of edge locations. Requests for your content are automatically routed to the nearest edge location, so content is delivered with the best possible performance.\n\n\nKey Terminology\n\n\n\n\nEdge Location -\n This is the location where content will be cached. This is separate to an AWS region or Availability zone.\n\n\nOrigin -\n This is the origin of all the files that the CDN will distribute. This can be either an S3 Bucket, an Ec2 Instance, An Elastic Load Balancer or Route53.\n\n\nDistribution -\n This is the name given to the CDN which consists of a collection of Edge Locations.\n\n\nWeb Distribution -\n Typically used for websites\n\n\nRTMP -\n Used for Media Streaming\n\n\n\n\n\n\n\n\n\n\nInfo\n\n\n\n\nAmazon CloudFront also works with any non-AWS origin server, which stores the original, definitive versions of your files\n\n\nEdge locations are not just READ only, you can write to them too\n\n\nObjects are cached for the life of the TTL (Time To Live)\n\n\nYou can clear cached objects, but you will be charged\n\n\nYou can restrict view access using signed url's or signed cookies\n\n\nYou can create multiple origins per destribution\n\n\nYou can set restrictions based on Geo locations (Whitelist/Blacklist)\n\n\n\n\n\n\nRoute 53\n\n\nAmazons DNS service.\n\n\nAPI Gateway\n\n\nFor creating a serverless website.\n\n\nA way of creating your own API for other services to talk to.\n\n\nDirect connect\n\n\nIs a way of running a dedicated line from your Corporate head office or data center to AWS.", 
            "title": "Networking and Content Delivery"
        }, 
        {
            "location": "/aws/services/networking-and-content-delivery/#networking-and-content-delivery", 
            "text": "", 
            "title": "Networking and content delivery"
        }, 
        {
            "location": "/aws/services/networking-and-content-delivery/#vpc-virtual-private-cloud", 
            "text": "Amazon Virtual private cloud. Can be considered as a virtual datacenter.  Amazon Virtual Private Cloud (Amazon VPC) enables you to launch AWS resources into a virtual network that you've defined. This virtual network closely resembles a traditional network that you'd operate in your own data center, with the benefits of using the scalable infrastructure of AWS.  Configure firewall, availability zones, network sider address ranges,  Network ACL's, route tables etc.", 
            "title": "VPC : Virtual Private Cloud"
        }, 
        {
            "location": "/aws/services/networking-and-content-delivery/#cloudfront", 
            "text": "Amazons content delivery network service. A Content Delivery Network (CDN) is a system of distributed servers (network) that deliver webpages and other web content to a user base on the geogra[hic locations of the user, the origin of the webpage and a content delivery server.   What is CDN?    Amazons CloudFront can be used to deliver your entire website including dynamic, static, streaming and ineractive content using a global network of edge locations. Requests for your content are automatically routed to the nearest edge location, so content is delivered with the best possible performance.", 
            "title": "CloudFront"
        }, 
        {
            "location": "/aws/services/networking-and-content-delivery/#key-terminology", 
            "text": "Edge Location -  This is the location where content will be cached. This is separate to an AWS region or Availability zone.  Origin -  This is the origin of all the files that the CDN will distribute. This can be either an S3 Bucket, an Ec2 Instance, An Elastic Load Balancer or Route53.  Distribution -  This is the name given to the CDN which consists of a collection of Edge Locations.  Web Distribution -  Typically used for websites  RTMP -  Used for Media Streaming      Info   Amazon CloudFront also works with any non-AWS origin server, which stores the original, definitive versions of your files  Edge locations are not just READ only, you can write to them too  Objects are cached for the life of the TTL (Time To Live)  You can clear cached objects, but you will be charged  You can restrict view access using signed url's or signed cookies  You can create multiple origins per destribution  You can set restrictions based on Geo locations (Whitelist/Blacklist)", 
            "title": "Key Terminology"
        }, 
        {
            "location": "/aws/services/networking-and-content-delivery/#route-53", 
            "text": "Amazons DNS service.", 
            "title": "Route 53"
        }, 
        {
            "location": "/aws/services/networking-and-content-delivery/#api-gateway", 
            "text": "For creating a serverless website.  A way of creating your own API for other services to talk to.", 
            "title": "API Gateway"
        }, 
        {
            "location": "/aws/services/networking-and-content-delivery/#direct-connect", 
            "text": "Is a way of running a dedicated line from your Corporate head office or data center to AWS.", 
            "title": "Direct connect"
        }, 
        {
            "location": "/aws/services/management-tools/", 
            "text": "Management tools\n\n\nCloudWatch\n\n\nAmazon CloudWatch is a monitoring service for AWS cloud resources and the applications you run on AWS.\n\n\n\n\nDashboards - Create awesome dashboards to see what is happening with your AWS environment\n\n\nAlarms - Allows you to set alarms that notify you when particular thresholds are hit\n\n\nEvents - CloudWatch events helps you to respond to syaye changes in your AWS resources\n\n\nLogs - CloudWatch logs helps you to aggregate, monitor and store logs\n\n\n\n\n\n\nNote\n\n\nStandard Monitoring = 5 min (default)\n\n\nDetailed Monitoring = 1 min\n\n\n\n\nCloudFormation\n\n\nIt is a way of scripting infrastructure.\n\n\nAWS CloudFormation provides a common language for you to describe and provision all the infrastructure resources in your cloud environment. CloudFormation allows you to use a simple text file to model and provision, in an automated and secure manner\n\n\nWith a cloudformation template you can deploy your applications with ease.\n\n\nThe scripts are reusable.\n\n\nCloudTrail\n\n\nLogs changes to your AWS environment.\n\n\nIt is turned on by default and it keeps a trail of the changes for a week.\n\n\nIf turned on it makes it easy to determine if your environment is hacked.\n\n\nConfig\n\n\nMonitors the configuration of your AWS environment.\n\n\nMaintains a snapshot thus making it possible to move back and forward across weeks.\n\n\nVisualise your AWS environments.\n\n\nOpsWorks\n\n\nSimilar to elastic beanstack.\n\n\nUses chef and puppet to automate the environment.\n\n\nService catalog\n\n\nA way for managing a catalog of service allowed to use on the AWS environment.\n\n\nYou can decide on which Virtual server images, Operating systems. databases etc can be used on the AWS environment.\n\n\nBasically used by big organizations for governance and compliance requirements\n\n\nSystems Manager\n\n\nInterface for maintaining your AWS resources.\n\n\nTypically used for EC2.\n\n\nFor patch maintenance eg: security patches across thousands of EC2 instances.\n\n\nCan group all the resources by different departments or applications for planning the patch updates.\n\n\nTrusted Advisor\n\n\nWould give you advice on multiple deciplines like security, would advice if you have left any ports open that could be a security risk. It can tell you how you can save money on AWS. Can be thought of as an Advisor or an accountant that you trust and that gives you the best advice for your AWS environment.\n\n\nManaged services\n\n\nAmazon provides managed services for AWS, i.e., if you do not want to worry about your EC2 instances or the auto scale feature you can opt for the managed services.", 
            "title": "Management Tools"
        }, 
        {
            "location": "/aws/services/management-tools/#management-tools", 
            "text": "", 
            "title": "Management tools"
        }, 
        {
            "location": "/aws/services/management-tools/#cloudwatch", 
            "text": "Amazon CloudWatch is a monitoring service for AWS cloud resources and the applications you run on AWS.   Dashboards - Create awesome dashboards to see what is happening with your AWS environment  Alarms - Allows you to set alarms that notify you when particular thresholds are hit  Events - CloudWatch events helps you to respond to syaye changes in your AWS resources  Logs - CloudWatch logs helps you to aggregate, monitor and store logs    Note  Standard Monitoring = 5 min (default)  Detailed Monitoring = 1 min", 
            "title": "CloudWatch"
        }, 
        {
            "location": "/aws/services/management-tools/#cloudformation", 
            "text": "It is a way of scripting infrastructure.  AWS CloudFormation provides a common language for you to describe and provision all the infrastructure resources in your cloud environment. CloudFormation allows you to use a simple text file to model and provision, in an automated and secure manner  With a cloudformation template you can deploy your applications with ease.  The scripts are reusable.", 
            "title": "CloudFormation"
        }, 
        {
            "location": "/aws/services/management-tools/#cloudtrail", 
            "text": "Logs changes to your AWS environment.  It is turned on by default and it keeps a trail of the changes for a week.  If turned on it makes it easy to determine if your environment is hacked.", 
            "title": "CloudTrail"
        }, 
        {
            "location": "/aws/services/management-tools/#config", 
            "text": "Monitors the configuration of your AWS environment.  Maintains a snapshot thus making it possible to move back and forward across weeks.  Visualise your AWS environments.", 
            "title": "Config"
        }, 
        {
            "location": "/aws/services/management-tools/#opsworks", 
            "text": "Similar to elastic beanstack.  Uses chef and puppet to automate the environment.", 
            "title": "OpsWorks"
        }, 
        {
            "location": "/aws/services/management-tools/#service-catalog", 
            "text": "A way for managing a catalog of service allowed to use on the AWS environment.  You can decide on which Virtual server images, Operating systems. databases etc can be used on the AWS environment.  Basically used by big organizations for governance and compliance requirements", 
            "title": "Service catalog"
        }, 
        {
            "location": "/aws/services/management-tools/#systems-manager", 
            "text": "Interface for maintaining your AWS resources.  Typically used for EC2.  For patch maintenance eg: security patches across thousands of EC2 instances.  Can group all the resources by different departments or applications for planning the patch updates.", 
            "title": "Systems Manager"
        }, 
        {
            "location": "/aws/services/management-tools/#trusted-advisor", 
            "text": "Would give you advice on multiple deciplines like security, would advice if you have left any ports open that could be a security risk. It can tell you how you can save money on AWS. Can be thought of as an Advisor or an accountant that you trust and that gives you the best advice for your AWS environment.", 
            "title": "Trusted Advisor"
        }, 
        {
            "location": "/aws/services/management-tools/#managed-services", 
            "text": "Amazon provides managed services for AWS, i.e., if you do not want to worry about your EC2 instances or the auto scale feature you can opt for the managed services.", 
            "title": "Managed services"
        }, 
        {
            "location": "/aws/services/database/", 
            "text": "Database\n\n\nRDS : Relation database service\n\n\nAmazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching and backups.\n\n\nAmazon RDS is available on several database instance types - optimized for memory, performance or I/O - and provides you with six familiar database engines to choose from, including Amazon Aurora, PostgreSQL, MySQL, MariaDB, Oracle, and Microsoft SQL Server. You can use the AWS Database Migration Service to easily migrate or replicate your existing databases to Amazon RDS.\n\n\nDynamo DB\n\n\nNon relational db (will be covered later)\n\n\nElasticache\n\n\nFor caching commonly executed queries.\n\n\nAmazon ElastiCache offers fully managed Redis and Memcached. Seamlessly deploy, operate, and scale popular open source compatible in-memory data stores. Build data-intensive apps or improve the performance of your existing apps by retrieving data from high throughput and low latency in-memory data stores. Amazon ElastiCache is a popular choice for Gaming, Ad-Tech, Financial Services, Healthcare, and IoT apps.\n\n\nRedshift\n\n\nFor data warehousing and business intelligence.", 
            "title": "Database"
        }, 
        {
            "location": "/aws/services/database/#database", 
            "text": "", 
            "title": "Database"
        }, 
        {
            "location": "/aws/services/database/#rds-relation-database-service", 
            "text": "Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching and backups.  Amazon RDS is available on several database instance types - optimized for memory, performance or I/O - and provides you with six familiar database engines to choose from, including Amazon Aurora, PostgreSQL, MySQL, MariaDB, Oracle, and Microsoft SQL Server. You can use the AWS Database Migration Service to easily migrate or replicate your existing databases to Amazon RDS.", 
            "title": "RDS : Relation database service"
        }, 
        {
            "location": "/aws/services/database/#dynamo-db", 
            "text": "Non relational db (will be covered later)", 
            "title": "Dynamo DB"
        }, 
        {
            "location": "/aws/services/database/#elasticache", 
            "text": "For caching commonly executed queries.  Amazon ElastiCache offers fully managed Redis and Memcached. Seamlessly deploy, operate, and scale popular open source compatible in-memory data stores. Build data-intensive apps or improve the performance of your existing apps by retrieving data from high throughput and low latency in-memory data stores. Amazon ElastiCache is a popular choice for Gaming, Ad-Tech, Financial Services, Healthcare, and IoT apps.", 
            "title": "Elasticache"
        }, 
        {
            "location": "/aws/services/database/#redshift", 
            "text": "For data warehousing and business intelligence.", 
            "title": "Redshift"
        }, 
        {
            "location": "/aws/services/migration/", 
            "text": "Migration\n\n\nAWS migration hub\n\n\nAllows you to track you applications as you move your application to AWS and will integrate with other services of the migration framework.\n\n\nFor visualizing the progress of your migrations.\n\n\nApplication discovery  service\n\n\nDetects the applications you have along with their depencies. For example if you have a sonarqube server it may have a dependency on a sql server or a domain controller. Thus its a way of tracking dependencies for your application.\n\n\nDatabase Migration service\n\n\nVery easy way to migrate on premise databases to AWS.\n\n\nServer migration service\n\n\nHelps you to migrate your virtual or physical server to the AWS cloud.\n\n\nSnowball\n\n\nHelps you to phusically migrate large chunks of data into the cloud.", 
            "title": "Migration"
        }, 
        {
            "location": "/aws/services/migration/#migration", 
            "text": "", 
            "title": "Migration"
        }, 
        {
            "location": "/aws/services/migration/#aws-migration-hub", 
            "text": "Allows you to track you applications as you move your application to AWS and will integrate with other services of the migration framework.  For visualizing the progress of your migrations.", 
            "title": "AWS migration hub"
        }, 
        {
            "location": "/aws/services/migration/#application-discovery-service", 
            "text": "Detects the applications you have along with their depencies. For example if you have a sonarqube server it may have a dependency on a sql server or a domain controller. Thus its a way of tracking dependencies for your application.", 
            "title": "Application discovery  service"
        }, 
        {
            "location": "/aws/services/migration/#database-migration-service", 
            "text": "Very easy way to migrate on premise databases to AWS.", 
            "title": "Database Migration service"
        }, 
        {
            "location": "/aws/services/migration/#server-migration-service", 
            "text": "Helps you to migrate your virtual or physical server to the AWS cloud.", 
            "title": "Server migration service"
        }, 
        {
            "location": "/aws/services/migration/#snowball", 
            "text": "Helps you to phusically migrate large chunks of data into the cloud.", 
            "title": "Snowball"
        }, 
        {
            "location": "/aws/services/developer-tools/", 
            "text": "Developer tools\n\n\nCodeStar\n\n\nFor managing your code and colaborating with other developers in the team.\n\n\nCodeCommit\n\n\nA place to store your souce code. Git repositories on AWS.\n\n\nCodeBuild\n\n\nCompiles the code and run tests against it to producte software packages.\n\n\nCodeDeploy\n\n\nAutomates code deployment to your EC2 or on premise servers.\n\n\nCodePipeline\n\n\nContinuous delivery service to visualize and automate the steps required to release your software products.\n\n\nX-Ray\n\n\nTo debug and analyse serverless applications.\n\n\nCloud9\n\n\nThis is a IDE environment in AWS. A place where you can develop your code within your systems web browser.", 
            "title": "Developer Tools"
        }, 
        {
            "location": "/aws/services/developer-tools/#developer-tools", 
            "text": "", 
            "title": "Developer tools"
        }, 
        {
            "location": "/aws/services/developer-tools/#codestar", 
            "text": "For managing your code and colaborating with other developers in the team.", 
            "title": "CodeStar"
        }, 
        {
            "location": "/aws/services/developer-tools/#codecommit", 
            "text": "A place to store your souce code. Git repositories on AWS.", 
            "title": "CodeCommit"
        }, 
        {
            "location": "/aws/services/developer-tools/#codebuild", 
            "text": "Compiles the code and run tests against it to producte software packages.", 
            "title": "CodeBuild"
        }, 
        {
            "location": "/aws/services/developer-tools/#codedeploy", 
            "text": "Automates code deployment to your EC2 or on premise servers.", 
            "title": "CodeDeploy"
        }, 
        {
            "location": "/aws/services/developer-tools/#codepipeline", 
            "text": "Continuous delivery service to visualize and automate the steps required to release your software products.", 
            "title": "CodePipeline"
        }, 
        {
            "location": "/aws/services/developer-tools/#x-ray", 
            "text": "To debug and analyse serverless applications.", 
            "title": "X-Ray"
        }, 
        {
            "location": "/aws/services/developer-tools/#cloud9", 
            "text": "This is a IDE environment in AWS. A place where you can develop your code within your systems web browser.", 
            "title": "Cloud9"
        }, 
        {
            "location": "/aws/services/machine-learning/", 
            "text": "Machine Learning\n\n\nPolly\n\n\nTurns text into speech.\n\n\nCan choose different languages and accents.\n\n\nLex\n\n\nAlexa service.\n\n\nRekognition\n\n\nCan upload a picture and it will recognize it and let you know what elements are there in the picture.\n\n\nTranslate\n\n\nTranslate languages. Similar to google translate\n\n\nTranscribe\n\n\nAutomatic speech recognition. Turns speech into text.", 
            "title": "Machine Learning"
        }, 
        {
            "location": "/aws/services/machine-learning/#machine-learning", 
            "text": "", 
            "title": "Machine Learning"
        }, 
        {
            "location": "/aws/services/machine-learning/#polly", 
            "text": "Turns text into speech.  Can choose different languages and accents.", 
            "title": "Polly"
        }, 
        {
            "location": "/aws/services/machine-learning/#lex", 
            "text": "Alexa service.", 
            "title": "Lex"
        }, 
        {
            "location": "/aws/services/machine-learning/#rekognition", 
            "text": "Can upload a picture and it will recognize it and let you know what elements are there in the picture.", 
            "title": "Rekognition"
        }, 
        {
            "location": "/aws/services/machine-learning/#translate", 
            "text": "Translate languages. Similar to google translate", 
            "title": "Translate"
        }, 
        {
            "location": "/aws/services/machine-learning/#transcribe", 
            "text": "Automatic speech recognition. Turns speech into text.", 
            "title": "Transcribe"
        }, 
        {
            "location": "/aws/services/analytics/", 
            "text": "Analytics\n\n\nAthena\n\n\nLooks through the S3 bucket data and returns results.\n\n\nEMR : Elastic Map Reduce\n\n\nFor processing large amounts of data for big data solutions.\n\n\nCloudSearch and Elastic Search Service\n\n\nSearch services in AWS.\n\n\nKinesis\n\n\nWay of ingesting large amount of data into AWS. Social media feeds etc.\n\n\nQuickSight\n\n\nAmazons business intelligence tool.\n\n\nData pipeline\n\n\nMoving data between different AWS services.\n\n\nGlue\n\n\nUsed for ETL [Extract transform and load]", 
            "title": "Analytics"
        }, 
        {
            "location": "/aws/services/analytics/#analytics", 
            "text": "", 
            "title": "Analytics"
        }, 
        {
            "location": "/aws/services/analytics/#athena", 
            "text": "Looks through the S3 bucket data and returns results.", 
            "title": "Athena"
        }, 
        {
            "location": "/aws/services/analytics/#emr-elastic-map-reduce", 
            "text": "For processing large amounts of data for big data solutions.", 
            "title": "EMR : Elastic Map Reduce"
        }, 
        {
            "location": "/aws/services/analytics/#cloudsearch-and-elastic-search-service", 
            "text": "Search services in AWS.", 
            "title": "CloudSearch and Elastic Search Service"
        }, 
        {
            "location": "/aws/services/analytics/#kinesis", 
            "text": "Way of ingesting large amount of data into AWS. Social media feeds etc.", 
            "title": "Kinesis"
        }, 
        {
            "location": "/aws/services/analytics/#quicksight", 
            "text": "Amazons business intelligence tool.", 
            "title": "QuickSight"
        }, 
        {
            "location": "/aws/services/analytics/#data-pipeline", 
            "text": "Moving data between different AWS services.", 
            "title": "Data pipeline"
        }, 
        {
            "location": "/aws/services/analytics/#glue", 
            "text": "Used for ETL [Extract transform and load]", 
            "title": "Glue"
        }, 
        {
            "location": "/aws/services/mobile-services/", 
            "text": "Mobile services\n\n\nMobile hub\n\n\nManagement console for mobile applications.\n\n\nPinpoint\n\n\nFor using targetted push notifications.\n\n\nAppSync\n\n\nUpdates web and mobile data. Updates data from the cloud when the phone comes back online after being offline from sometime.\n\n\nDevice Farm\n\n\nTest mobile application on real android and ios devices.\n\n\nMobile Analytics\n\n\nAnalytics for your mobile application", 
            "title": "Mobile Services"
        }, 
        {
            "location": "/aws/services/mobile-services/#mobile-services", 
            "text": "", 
            "title": "Mobile services"
        }, 
        {
            "location": "/aws/services/mobile-services/#mobile-hub", 
            "text": "Management console for mobile applications.", 
            "title": "Mobile hub"
        }, 
        {
            "location": "/aws/services/mobile-services/#pinpoint", 
            "text": "For using targetted push notifications.", 
            "title": "Pinpoint"
        }, 
        {
            "location": "/aws/services/mobile-services/#appsync", 
            "text": "Updates web and mobile data. Updates data from the cloud when the phone comes back online after being offline from sometime.", 
            "title": "AppSync"
        }, 
        {
            "location": "/aws/services/mobile-services/#device-farm", 
            "text": "Test mobile application on real android and ios devices.", 
            "title": "Device Farm"
        }, 
        {
            "location": "/aws/services/mobile-services/#mobile-analytics", 
            "text": "Analytics for your mobile application", 
            "title": "Mobile Analytics"
        }, 
        {
            "location": "/aws/services/application-integration/", 
            "text": "Application Integration\n\n\nStep functions\n\n\nManaging your lamda functions and manage the steps to go through it.\n\n\nAmazon MQ\n\n\nFor message queues. similar to Rabbit MQ.\n\n\nSNS\n\n\nA notification service. Will be setting a billing alarm as part of the course.\n\n\nSQS\n\n\nOne of the oldest services.\n\n\nA way of decoupling your infrastructure.\n\n\nSWF : Simple Workflow Service\n\n\nCustomer engagement\n\n\nConnect\n\n\nA contact center as a service. Similar to having a call center on the cloud.\n\n\nSimple email service\n\n\nA great way of sending large amount of emails.", 
            "title": "Application Integration"
        }, 
        {
            "location": "/aws/services/application-integration/#application-integration", 
            "text": "", 
            "title": "Application Integration"
        }, 
        {
            "location": "/aws/services/application-integration/#step-functions", 
            "text": "Managing your lamda functions and manage the steps to go through it.", 
            "title": "Step functions"
        }, 
        {
            "location": "/aws/services/application-integration/#amazon-mq", 
            "text": "For message queues. similar to Rabbit MQ.", 
            "title": "Amazon MQ"
        }, 
        {
            "location": "/aws/services/application-integration/#sns", 
            "text": "A notification service. Will be setting a billing alarm as part of the course.", 
            "title": "SNS"
        }, 
        {
            "location": "/aws/services/application-integration/#sqs", 
            "text": "One of the oldest services.  A way of decoupling your infrastructure.", 
            "title": "SQS"
        }, 
        {
            "location": "/aws/services/application-integration/#swf-simple-workflow-service", 
            "text": "", 
            "title": "SWF : Simple Workflow Service"
        }, 
        {
            "location": "/aws/services/application-integration/#customer-engagement", 
            "text": "", 
            "title": "Customer engagement"
        }, 
        {
            "location": "/aws/services/application-integration/#connect", 
            "text": "A contact center as a service. Similar to having a call center on the cloud.", 
            "title": "Connect"
        }, 
        {
            "location": "/aws/services/application-integration/#simple-email-service", 
            "text": "A great way of sending large amount of emails.", 
            "title": "Simple email service"
        }, 
        {
            "location": "/aws/services/customer-engagement/", 
            "text": "Customer engagement\n\n\nConnect\n\n\nA contact center as a service. Similar to having a call center on the cloud.\n\n\nSimple email service\n\n\nA great way of sending large amount of emails.", 
            "title": "Customer Engagement"
        }, 
        {
            "location": "/aws/services/customer-engagement/#customer-engagement", 
            "text": "", 
            "title": "Customer engagement"
        }, 
        {
            "location": "/aws/services/customer-engagement/#connect", 
            "text": "A contact center as a service. Similar to having a call center on the cloud.", 
            "title": "Connect"
        }, 
        {
            "location": "/aws/services/customer-engagement/#simple-email-service", 
            "text": "A great way of sending large amount of emails.", 
            "title": "Simple email service"
        }, 
        {
            "location": "/aws/services/business-productivity/", 
            "text": "Business Productivity\n\n\nAlexa for Business\n\n\nUse it to dial into a meeting room or log a request for printer problems.\n\n\nChime\n\n\nLike google hangout. Record meetings.\n\n\nWork Docs\n\n\nLike a dropbox in AWS.\n\n\nWorkMail\n\n\nLike office 365 services in AWS.", 
            "title": "Business Productivity"
        }, 
        {
            "location": "/aws/services/business-productivity/#business-productivity", 
            "text": "", 
            "title": "Business Productivity"
        }, 
        {
            "location": "/aws/services/business-productivity/#alexa-for-business", 
            "text": "Use it to dial into a meeting room or log a request for printer problems.", 
            "title": "Alexa for Business"
        }, 
        {
            "location": "/aws/services/business-productivity/#chime", 
            "text": "Like google hangout. Record meetings.", 
            "title": "Chime"
        }, 
        {
            "location": "/aws/services/business-productivity/#work-docs", 
            "text": "Like a dropbox in AWS.", 
            "title": "Work Docs"
        }, 
        {
            "location": "/aws/services/business-productivity/#workmail", 
            "text": "Like office 365 services in AWS.", 
            "title": "WorkMail"
        }, 
        {
            "location": "/aws/services/desktop-and-app-streaming/", 
            "text": "Desktop and App Streaming\n\n\nWorkspaces\n\n\nIs a VDI short for Virtual Desktop Infrastructure solution. Accessing a desktop on the cloud on your device.\n\n\nAppStream 2.0\n\n\nWay of live streaming the applications. Like Citrix.", 
            "title": "Desktop and App Streaming"
        }, 
        {
            "location": "/aws/services/desktop-and-app-streaming/#desktop-and-app-streaming", 
            "text": "", 
            "title": "Desktop and App Streaming"
        }, 
        {
            "location": "/aws/services/desktop-and-app-streaming/#workspaces", 
            "text": "Is a VDI short for Virtual Desktop Infrastructure solution. Accessing a desktop on the cloud on your device.", 
            "title": "Workspaces"
        }, 
        {
            "location": "/aws/services/desktop-and-app-streaming/#appstream-20", 
            "text": "Way of live streaming the applications. Like Citrix.", 
            "title": "AppStream 2.0"
        }, 
        {
            "location": "/aws/services/internet-of-things/", 
            "text": "IOT Internet Of Things\n\n\niOT\n\n\niOT Device Management\n\n\nFreeRTOS\n\n\nGreengrass", 
            "title": "Internet Of Things"
        }, 
        {
            "location": "/aws/services/internet-of-things/#iot-internet-of-things", 
            "text": "", 
            "title": "IOT Internet Of Things"
        }, 
        {
            "location": "/aws/services/internet-of-things/#iot", 
            "text": "", 
            "title": "iOT"
        }, 
        {
            "location": "/aws/services/internet-of-things/#iot-device-management", 
            "text": "", 
            "title": "iOT Device Management"
        }, 
        {
            "location": "/aws/services/internet-of-things/#freertos", 
            "text": "", 
            "title": "FreeRTOS"
        }, 
        {
            "location": "/aws/services/internet-of-things/#greengrass", 
            "text": "", 
            "title": "Greengrass"
        }, 
        {
            "location": "/aws/services/game-development/", 
            "text": "Game Development\n\n\nGameLift\n\n\nService to help you develop games.", 
            "title": "Game Development"
        }, 
        {
            "location": "/aws/services/game-development/#game-development", 
            "text": "", 
            "title": "Game Development"
        }, 
        {
            "location": "/aws/services/game-development/#gamelift", 
            "text": "Service to help you develop games.", 
            "title": "GameLift"
        }, 
        {
            "location": "/aws/awscli/", 
            "text": "Installing AWS CLI on Mac\n\n\nTo install awscli on mac you need Python 2.6.5 or higher.\n\n\nInstall using pip\n\n\n1\npip install awscli\n\n\n\n\n\n\n\n\nYou might get the below error during the installation\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\nInstalling collected packages: six, python-dateutil, docutils, botocore, colorama, pyasn1, rsa, futures, s3transfer,    awscli\n  Found existing installation: six 1.4.1\n    DEPRECATION: Uninstalling a distutils installed project (six) has been deprecated and will be removed in a    future version. This is due to the fact that uninstalling a distutils project will only partially uninstall the   project.\n    Uninstalling six-1.4.1:\nException:\nTraceback (most recent call last):\n  File \n/Library/Python/2.7/site-packages/pip/basecommand.py\n, line 215, in main\n    status = self.run(options, args)\n  File \n/Library/Python/2.7/site-packages/pip/commands/install.py\n, line 342, in run\n    prefix=options.prefix_path,\n  File \n/Library/Python/2.7/site-packages/pip/req/req_set.py\n, line 778, in install\n    requirement.uninstall(auto_confirm=True)\n  File \n/Library/Python/2.7/site-packages/pip/req/req_install.py\n, line 754, in uninstall\n    paths_to_remove.remove(auto_confirm)\n  File \n/Library/Python/2.7/site-packages/pip/req/req_uninstall.py\n, line 115, in remove\n    renames(path, new_path)\n  File \n/Library/Python/2.7/site-packages/pip/utils/__init__.py\n, line 267, in renames\n    shutil.move(old, new)\n  File \n/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/shutil.py\n, line 302, in move\n    copy2(src, real_dst)\n  File \n/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/shutil.py\n, line 131, in copy2\n    copystat(src, dst)\n  File \n/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/shutil.py\n, line 103, in copystat\n    os.chflags(dst, st.st_flags)\nOSError: [Errno 1] Operation not permitted:     \n/var/folders/p5/qvcpp48j55z1zqsggck_v1280000gn/T/pip-zghy0c-uninstall/System/Library/Frameworks/Python.    framework/Versions/2.7/Extras/lib/python/six-1.4.1-py2.7.egg-info\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n1\nsudo -H pip install awscli --upgrade --ignore-installed six", 
            "title": "AWS CLI on Mac"
        }, 
        {
            "location": "/aws/awscli/#installing-aws-cli-on-mac", 
            "text": "To install awscli on mac you need Python 2.6.5 or higher.  Install using pip  1 pip install awscli    You might get the below error during the installation   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25 Installing collected packages: six, python-dateutil, docutils, botocore, colorama, pyasn1, rsa, futures, s3transfer,    awscli\n  Found existing installation: six 1.4.1\n    DEPRECATION: Uninstalling a distutils installed project (six) has been deprecated and will be removed in a    future version. This is due to the fact that uninstalling a distutils project will only partially uninstall the   project.\n    Uninstalling six-1.4.1:\nException:\nTraceback (most recent call last):\n  File  /Library/Python/2.7/site-packages/pip/basecommand.py , line 215, in main\n    status = self.run(options, args)\n  File  /Library/Python/2.7/site-packages/pip/commands/install.py , line 342, in run\n    prefix=options.prefix_path,\n  File  /Library/Python/2.7/site-packages/pip/req/req_set.py , line 778, in install\n    requirement.uninstall(auto_confirm=True)\n  File  /Library/Python/2.7/site-packages/pip/req/req_install.py , line 754, in uninstall\n    paths_to_remove.remove(auto_confirm)\n  File  /Library/Python/2.7/site-packages/pip/req/req_uninstall.py , line 115, in remove\n    renames(path, new_path)\n  File  /Library/Python/2.7/site-packages/pip/utils/__init__.py , line 267, in renames\n    shutil.move(old, new)\n  File  /System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/shutil.py , line 302, in move\n    copy2(src, real_dst)\n  File  /System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/shutil.py , line 131, in copy2\n    copystat(src, dst)\n  File  /System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/shutil.py , line 103, in copystat\n    os.chflags(dst, st.st_flags)\nOSError: [Errno 1] Operation not permitted:      /var/folders/p5/qvcpp48j55z1zqsggck_v1280000gn/T/pip-zghy0c-uninstall/System/Library/Frameworks/Python.    framework/Versions/2.7/Extras/lib/python/six-1.4.1-py2.7.egg-info      Solution  1 sudo -H pip install awscli --upgrade --ignore-installed six", 
            "title": "Installing AWS CLI on Mac"
        }
    ]
}